# 母题 Q15：数据库拆分与一致性哈希（详细版）

## 0. 面试开场（30 秒版本）
- 这是一个“容量增长 + 热点治理 + 在线迁移 + 一致性兜底”的综合题。
- 核心思路：先确定分片键和一致性边界，再给出可回滚迁移方案，最后用监控与演练证明可落地。
- 关键原则：单分片强一致，跨分片最终一致；控制面和数据面解耦；扩容是常态，不是事故。

---

## 1. 面试官真正考察什么
1. 你是否能把“分库分表”从概念讲到工程落地。
2. 你是否能说清“为什么这个分片键，不是另一个”。
3. 你是否能处理扩容迁移时的业务连续性与回滚。
4. 你是否能定义跨分片查询的能力边界，而不是直接说“不支持”。
5. 你是否能给出故障演练和观测指标，证明方案可操作。

---

## 2. 需求澄清（Functional / Non-Functional / Out of Scope）
### 2.1 Functional Requirements
- 支持主写入链路：创建、更新、状态流转。
- 支持主查询链路：按主键查、按租户+时间查、有限聚合查。
- 支持在线扩容：新增分片、迁移数据、切换路由、回滚。
- 支持运维能力：路由版本管理、迁移任务管理、审计。

### 2.2 Non-Functional Requirements
- 可用性：`>= 99.95%`。
- 写入 P99：`< 80ms`。
- 读取 P99：`< 120ms`。
- 扩容迁移窗口内：
  - 错误率不超过基线 `2x`。
  - P99 不超过基线 `1.5x`。
- 数据正确性：关键业务字段零静默丢失。

### 2.3 Out of Scope（明确不做）
- 不支持默认跨分片强一致事务（2PC/XA）作为主链路。
- 不支持无限维度 ad-hoc 实时分析（下沉到数仓/OLAP）。
- 不支持无过滤条件的大表扫描。

### 2.4 关键业务假设
- 业务具备天然“租户/用户”边界。
- 热点主要集中在少数租户或活动时间段。
- 读流量远高于写流量，存在明显缓存收益。

---

## 3. SLO/SLA 与错误预算
### 3.1 SLO 定义
- `Write Availability >= 99.95%`
- `Read Availability >= 99.95%`
- `Write P99 < 80ms`
- `Read P99 < 120ms`
- `Migration Cutover Failure Rate < 0.5%`

### 3.2 错误预算
- 月度可用性 99.95% 允许不可用时间约 `21.6 分钟`。
- 将错误预算拆成：
  - 基础设施故障：40%
  - 应用发布/配置：30%
  - 迁移切流：30%

### 3.3 SLO 治理动作
- 若 `write p99` 连续 10 分钟超阈：暂停迁移与灰度，进入稳定性保护模式。
- 若 `cutover error rate` 突破 1%：触发自动回滚流程。

---

## 4. 容量估算（详细）
### 4.1 输入参数（示例）
- DAU：`10,000,000`
- 人均写请求：`10/天`
- 人均读请求：`60/天`
- 峰值系数：写 `8x`，读 `6x`
- 记录平均原始体积：`1.5KB`
- 索引与元数据放大系数：`1.6`

### 4.2 QPS 估算
- 日写请求：`100,000,000`
- 平均写 QPS：`100,000,000 / 86400 ~= 1157`
- 峰值写 QPS：`1157 * 8 ~= 9256`

- 日读请求：`600,000,000`
- 平均读 QPS：`600,000,000 / 86400 ~= 6944`
- 峰值读 QPS：`6944 * 6 ~= 41664`

### 4.3 存储估算
- 单条记录落盘后：`1.5KB * 1.6 = 2.4KB`
- 每日新增数据：`100,000,000 * 2.4KB ~= 240GB`
- 年新增：`~87TB`（未含副本）
- 若主从 1:2 副本，原始存储预算需按 `~174TB/年` 评估。

### 4.4 分片规模估算
- 目标：单物理分片容量不超过 `2TB`，单分片峰值写不超过 `1500 QPS`。
- 初始方案：
  - 逻辑分片：`64`
  - 物理分片：`16`
  - 每物理分片承载 4 个逻辑分片
- 扩容策略：16 -> 24 -> 32 逐级扩，优先按热点分片拆迁。

---

## 5. 分片键设计（关键面试点）
### 5.1 候选分片键对比
| 候选键 | 优点 | 风险 | 结论 |
|---|---|---|---|
| `tenant_id` | 业务边界清晰、审计隔离好 | 大租户热点明显 | 主推荐 |
| `user_id` | 分布较均匀 | 跨租户治理复杂 | 次选 |
| `order_id`（随机） | 分布好 | 按租户查询代价高 | 不推荐 |
| `create_time` | 便于归档 | 天然热点（时间集中） | 不推荐 |

### 5.2 最终策略
- 一级分片键：`tenant_id`
- 二级分表键：`hash(user_id)` 或 `hash(order_id)`
- 查询约束：所有在线查询必须带可路由条件（tenant 或主键）

### 5.3 热点治理
- 热租户单独映射到独立物理分片（逻辑分片重绑定）。
- 对超热 key 使用“子分片”（salt）拆散写入。
- 对热点查询启用只读缓存和异步聚合视图。

---

## 6. 一致性哈希设计
### 6.1 为什么不是简单 `mod N`
- `mod N` 扩容时几乎全量重映射，迁移成本高。
- 一致性哈希通过哈希环和虚拟节点减少迁移数据量。

### 6.2 哈希环核心点
- 每个物理分片映射多个虚拟节点（VN），改善分布均匀性。
- 路由规则：
  - 计算 key 的 hash
  - 顺时针找到第一个节点即目标分片
- 扩容时仅迁移受新节点影响的部分数据范围。

### 6.3 虚拟节点数建议
- 小集群（<20 节点）：每节点 128~256 VN。
- 中集群（20~100 节点）：每节点 64~128 VN。
- VN 太少：负载不均；太多：路由表和重建开销上升。

### 6.4 路由版本
- 每次拓扑变更都会产生 `route_version`。
- 写请求带入路由版本，便于排查“旧路由写错分片”。
- 发布采用灰度：先 1% 路由客户端，再 10%、50%、100%。

---

## 7. 系统架构（数据面 + 控制面）
```text
Data Plane:
Client -> API Gateway -> Router SDK/Service -> DB Shards (Primary/Replica)
                                          \-> Cache
                                          \-> Query Aggregator

Control Plane:
Shard Manager -> Route Metadata Store -> Route Distributor
Migration Controller -> CDC/Backfill Worker -> Checksum/Repair

Observability:
Metrics + Logs + Tracing + Alert + Audit
```

### 7.1 数据面职责
- `Gateway`：鉴权、限流、trace 注入。
- `Router`：根据分片键和 route_version 找分片。
- `DB Shard`：单分片本地事务。
- `Query Aggregator`：受控跨分片查询。

### 7.2 控制面职责
- `Shard Manager`：分片生命周期管理。
- `Migration Controller`：迁移任务状态机与切流。
- `Route Distributor`：路由版本分发与一致性校验。
- `Repair Worker`：数据差异自动修复。

---

## 8. API 设计（带请求/响应示例）
### 8.1 创建记录
`POST /v1/entities`

Request:
```json
{
  "tenantId": 10001,
  "bizKey": "ORD-20260224-001",
  "payload": {"amount": 19900, "currency": "CNY"}
}
```

Headers:
- `Idempotency-Key: req-9f1b...`
- `X-Trace-Id: trace-...`

Response:
```json
{
  "id": 987654321,
  "routeVersion": 1204,
  "status": "CREATED"
}
```

### 8.2 主键查询
`GET /v1/entities/{id}?readPreference=replica`

### 8.3 受限聚合
`POST /v1/entities/aggregate`
```json
{
  "tenantId": 10001,
  "from": "2026-02-01T00:00:00Z",
  "to": "2026-02-24T00:00:00Z",
  "groupBy": ["status"],
  "maxFanout": 16,
  "timeoutMs": 80
}
```

### 8.4 迁移任务
- `POST /v1/shards/migrations`
- `POST /v1/shards/migrations/{taskId}/backfill`
- `POST /v1/shards/migrations/{taskId}/cutover`
- `POST /v1/shards/migrations/{taskId}/rollback`

### 8.5 典型错误码
- `IDEMPOTENCY_CONFLICT`
- `ROUTE_NOT_FOUND`
- `ROUTE_VERSION_STALE`
- `SHARD_OVERLOADED`
- `CROSS_SHARD_PARTIAL_TIMEOUT`
- `MIGRATION_GUARD_BLOCKED`

---

## 9. 数据模型（更完整）
### 9.1 路由元数据
```sql
CREATE TABLE shard_route (
  route_key VARCHAR(128) PRIMARY KEY,
  logical_shard INT NOT NULL,
  physical_shard INT NOT NULL,
  route_version BIGINT NOT NULL,
  status VARCHAR(32) NOT NULL,
  updated_at TIMESTAMP NOT NULL
);
```

### 9.2 主业务表（每个物理分片内）
```sql
CREATE TABLE entity_record (
  id BIGINT PRIMARY KEY,
  tenant_id BIGINT NOT NULL,
  biz_key VARCHAR(128) NOT NULL,
  status VARCHAR(32) NOT NULL,
  amount BIGINT NOT NULL,
  payload JSON NOT NULL,
  created_at TIMESTAMP NOT NULL,
  updated_at TIMESTAMP NOT NULL,
  UNIQUE KEY uk_tenant_biz (tenant_id, biz_key),
  KEY idx_tenant_time (tenant_id, created_at),
  KEY idx_tenant_status_time (tenant_id, status, created_at)
);
```

### 9.3 幂等表
```sql
CREATE TABLE idem_request (
  idem_key VARCHAR(256) PRIMARY KEY,
  tenant_id BIGINT NOT NULL,
  request_hash VARCHAR(64) NOT NULL,
  state VARCHAR(32) NOT NULL,
  response_payload JSON,
  expire_at TIMESTAMP NOT NULL,
  updated_at TIMESTAMP NOT NULL,
  KEY idx_expire (expire_at)
);
```

### 9.4 迁移任务表
```sql
CREATE TABLE migration_task (
  task_id BIGINT PRIMARY KEY,
  src_shard INT NOT NULL,
  dst_shard INT NOT NULL,
  range_start VARCHAR(128) NOT NULL,
  range_end VARCHAR(128) NOT NULL,
  state VARCHAR(32) NOT NULL,
  route_version BIGINT NOT NULL,
  backfill_lag BIGINT NOT NULL DEFAULT 0,
  checksum_pass BOOLEAN NOT NULL DEFAULT FALSE,
  rollback_reason VARCHAR(256),
  updated_at TIMESTAMP NOT NULL
);
```

### 9.5 Outbox 表
```sql
CREATE TABLE outbox_event (
  event_id BIGINT PRIMARY KEY,
  tenant_id BIGINT NOT NULL,
  biz_id BIGINT NOT NULL,
  event_type VARCHAR(64) NOT NULL,
  payload JSON NOT NULL,
  state VARCHAR(32) NOT NULL,
  retry_count INT NOT NULL DEFAULT 0,
  next_retry_at TIMESTAMP,
  created_at TIMESTAMP NOT NULL,
  updated_at TIMESTAMP NOT NULL,
  KEY idx_state_retry (state, next_retry_at)
);
```

---

## 10. 核心流程（时序级）
### 10.1 正常写入
1. 网关校验身份和流控，检查 `Idempotency-Key`。
2. Router 获取 route_version，定位 physical_shard。
3. 分片内事务：
   - 写 `entity_record`
   - 写 `outbox_event`
   - 更新 `idem_request=SUCCESS`
4. 返回成功响应（附 route_version）。

### 10.2 高峰流量
1. 热租户峰值暴涨。
2. Gateway 租户级限流 + 关键接口优先队列。
3. Router 从本地缓存命中，减少控制面压力。
4. 读请求优先走副本 + 缓存，关键查回主库。

### 10.3 迁移回填
1. 创建迁移任务，冻结目标范围路由变更。
2. 启动 backfill worker（批量拉取+写入目标分片）。
3. CDC 持续追平增量。
4. Checksum 比对通过后进入可切流状态。

### 10.4 切流与回滚
1. 灰度切流 1% -> 10% -> 50% -> 100%。
2. 任意阶段错误率、延迟超阈触发自动回滚。
3. 回滚后执行差异修复并产出复盘报告。

---

## 11. 一致性与事务边界
### 11.1 一致性模型
- 单分片：ACID 强一致。
- 跨分片：最终一致 + 可重放。

### 11.2 事务策略
- 主链路只使用本地事务，避免跨分片分布式事务放大失败面。
- 跨分片副作用（通知、索引、统计）通过 outbox 异步传播。

### 11.3 幂等策略
- 幂等键：`tenant_id + client_req_id`
- 幂等冲突处理：
  - 请求 hash 相同：返回历史结果
  - 请求 hash 不同：返回冲突错误

### 11.4 补偿策略
- 消费失败 -> 指数退避重试 -> 死信队列 -> 人工/自动补偿。
- 对账任务定期比对主表与下游状态。

---

## 12. 可用性与容错
### 12.1 常见故障与策略
- 路由服务不可用：本地路由快照 + 版本校验只读模式。
- 分片主库故障：自动主从切换 + 连接池熔断。
- 查询聚合慢分片：超时剔除 + partial 返回。
- 迁移异常：自动回滚到上个稳定 route_version。

### 12.2 灾备与恢复
- 跨可用区部署，控制面和数据面都要多 AZ。
- Binlog/CDC 保证 RPO。
- 演练目标：
  - RTO `< 10 min`
  - RPO `< 1 min`

---

## 13. 可观测性体系
### 13.1 指标
- 路由：`route_hit_rate`, `route_version_skew`
- 分片：`shard_write_qps`, `shard_read_qps`, `shard_p99`, `replica_lag`
- 迁移：`migration_backfill_lag`, `migration_checksum_pass_ratio`, `cutover_rollback_count`
- 聚合：`cross_shard_timeout_rate`, `partial_response_rate`

### 13.2 日志字段
- 必须有：`trace_id, tenant_id, route_version, logical_shard, physical_shard, idem_key`

### 13.3 告警规则（示例）
- `route_hit_rate < 95%` 持续 5 分钟 -> P1
- `replica_lag > 3s` 持续 3 分钟 -> P1
- `cutover rollback` 任意一次 -> 立即通知

### 13.4 仪表盘建议
- Dashboard 1：分片负载热力图
- Dashboard 2：迁移状态机与滞后
- Dashboard 3：错误率与回滚趋势

---

## 14. 安全与合规
- 列级加密：手机号、证件号、敏感业务字段。
- 权限隔离：迁移接口仅平台角色可调用。
- 审计不可篡改：操作日志写入独立审计存储。
- 数据最小化：跨分片查询默认只返回必要字段。

---

## 15. 成本与取舍
### 15.1 取舍清单
- 一致性哈希：
  - 优点：扩容迁移成本低
  - 代价：路由复杂、排障复杂
- 逻辑分片层：
  - 优点：长期弹性好
  - 代价：控制面治理成本
- 跨分片查询在线化：
  - 优点：实时性好
  - 代价：资源成本高，必须强限制

### 15.2 降本手段
- 冷热分层（在线库保热数据，冷数据转对象存储/OLAP）。
- 读流量优先副本与缓存。
- 热租户单独隔离，避免全局过配。

---

## 16. Java 关键代码（4 段，覆盖不同难点）
### 16.1 一致性哈希路由
```java
public final class ConsistentHashRouter {
  private final NavigableMap<Long, Integer> ring = new TreeMap<>();
  private final int virtualNodes;

  public ConsistentHashRouter(int virtualNodes) {
    this.virtualNodes = virtualNodes;
  }

  public void addShard(int shardId) {
    for (int i = 0; i < virtualNodes; i++) {
      ring.put(hash(shardId + "#" + i), shardId);
    }
  }

  public int route(long tenantId) {
    if (ring.isEmpty()) throw new IllegalStateException("empty ring");
    long key = hash(Long.toString(tenantId));
    Map.Entry<Long, Integer> e = ring.ceilingEntry(key);
    return e != null ? e.getValue() : ring.firstEntry().getValue();
  }

  private long hash(String raw) {
    return Hashing.murmur3_128().hashString(raw, StandardCharsets.UTF_8).asLong();
  }
}
```

### 16.2 幂等写入 + 本地事务 + Outbox
```java
public class EntityWriteService {
  public WriteResult create(CreateEntityCmd cmd) {
    String idemKey = cmd.tenantId() + ":" + cmd.requestId();
    IdemState idem = idemRepo.tryStart(idemKey, cmd.payloadHash());
    if (idem.isReplay()) return idem.toResult();
    if (idem.isConflict()) throw new BizException("IDEMPOTENCY_CONFLICT");

    Transaction tx = txManager.begin();
    try {
      entityRepo.insert(cmd.toEntity());
      outboxRepo.insert(OutboxEvent.created(cmd.tenantId(), cmd.bizId(), "ENTITY_CREATED", cmd.payload()));
      tx.commit();

      WriteResult result = WriteResult.ok(cmd.bizId());
      idemRepo.finishSuccess(idemKey, result);
      return result;
    } catch (Exception ex) {
      tx.rollback();
      idemRepo.finishFailed(idemKey, ex.getMessage());
      throw ex;
    }
  }
}
```

### 16.3 迁移状态机（守卫 + 切流 + 回滚）
```java
public class MigrationService {
  public MigrationResult cutover(long taskId) {
    MigrationTask task = taskRepo.lock(taskId);
    guard(task);

    long nextVersion = routeRepo.publishNewVersion(task.range(), task.dstShard());
    waitObserveWindow(Duration.ofMinutes(2));

    if (isSloBroken(task.dstShard())) {
      routeRepo.rollbackVersion(task.range(), task.srcShard());
      taskRepo.markRollback(taskId, "SLO_BROKEN");
      return MigrationResult.rollback(taskId);
    }

    taskRepo.markDone(taskId, nextVersion);
    return MigrationResult.success(taskId, nextVersion);
  }

  private void guard(MigrationTask task) {
    if (!task.checksumPass()) throw new BizException("MIGRATION_GUARD_BLOCKED");
    if (task.backfillLag() > 1000) throw new BizException("BACKFILL_LAG_TOO_HIGH");
  }
}
```

### 16.4 跨分片聚合（并发受控 + partial 返回）
```java
public class AggregateService {
  public AggregateResponse query(AggregateRequest req) {
    List<Integer> shards = router.targetShards(req.tenantId(), req.timeRange());
    int parallelism = Math.min(req.maxFanout(), 16);
    ExecutorService pool = Executors.newFixedThreadPool(parallelism);

    List<CompletableFuture<ShardPartial>> futures = shards.stream()
      .map(s -> CompletableFuture
        .supplyAsync(() -> queryOneShard(s, req), pool)
        .orTimeout(req.timeoutMs(), TimeUnit.MILLISECONDS)
        .exceptionally(ex -> ShardPartial.timeout(s)))
      .toList();

    List<ShardPartial> partials = futures.stream().map(CompletableFuture::join).toList();
    boolean partial = partials.stream().anyMatch(ShardPartial::isTimeout);
    return AggregateResponse.merge(partials, partial);
  }
}
```

---

## 17. 故障演练 Runbook（详细）
### 演练 1：路由版本错配
- 触发：模拟部分实例未更新 route_version。
- 观察：`route_version_skew`、`write_error_rate`。
- 止血：下线旧版本实例 + 强制刷新路由缓存。
- 验收：错配恢复 < 5 分钟，错误率回归基线。

### 演练 2：迁移切流后目标分片抖动
- 触发：目标分片 CPU 拉高 + IO 抖动。
- 观察：`target shard p99`、`cutover error rate`。
- 止血：自动回滚 route_version。
- 验收：回滚完成 < 3 分钟，业务不中断。

### 演练 3：跨分片查询拖垮线程池
- 触发：单分片故障导致 fan-out 全阻塞。
- 观察：`aggregate timeout rate`、`thread pool saturation`。
- 止血：并发上限 + 超时熔断 + partial 返回。
- 验收：核心写链路不受影响。

### 演练 4：CDC 堵塞
- 触发：下游消费停滞。
- 观察：`backfill lag`、`outbox retry queue depth`。
- 止血：扩容消费者 + 分区重平衡 + 降低非核心事件优先级。

---

## 18. 面试追问与答题模板
### 高频追问
1. 分片键选错怎么办？
2. 为什么不用分布式事务？
3. 扩容如何不停机？
4. 跨分片分页如何稳定？
5. 如何证明迁移安全？
6. 一致性哈希如何解决热点？

### 推荐答题结构
1. 明确边界（什么场景强一致、什么场景最终一致）。
2. 讲主路径（写、读、迁移）。
3. 讲风控（回滚守卫、告警阈值、演练）。
4. 讲成本（为什么不是更复杂方案）。

---

## 19. 迁移学习（和其他母题的连接）
- 与 Q34（Redis）：
  - 共性：哈希分布、扩缩容、热点治理
  - 差异：Q15重点是持久化数据库和迁移状态机
- 与 Q84（支付）：
  - 共性：幂等、补偿、审计
  - 差异：Q84重状态机与账务一致，Q15重路由与分片治理
- 与 Q62（容器编排）：
  - 共性：控制面/数据面解耦
  - 差异：Q62重资源调度，Q15重数据路由

### 还需补齐知识点
- 在线 DDL 策略（gh-ost / pt-online-schema-change）
- 数据校验（全量 checksum + 增量 CDC 对账）
- 分片再均衡自动化工具链

---

## 20. 上场前 Checklist
- [ ] 我能解释为什么选 `tenant_id` 作为一级分片键。
- [ ] 我能讲清一致性哈希与 `mod N` 的扩容差异。
- [ ] 我能画出“迁移状态机”并说出回滚条件。
- [ ] 我能给出跨分片查询的能力边界和降级策略。
- [ ] 我能说清至少 3 个关键监控指标与告警阈值。
- [ ] 我能讲出 2 个成本取舍，不是“全都要”。

---

## 21. 30 秒总结
- 这题不是“分库分表怎么做”，而是“怎样长期扩容且随时能回滚”。
- 高分答案必须包含：分片键选择依据、一致性边界、迁移状态机、故障演练、观测指标。
- 只讲概念不讲守卫条件与回滚机制，面试基本拿不到高分。
