# Q3 Web Crawler - 网络爬虫（超高频）

## 1. 三句话题目本质
1. 这题本质是“在有限资源下，持续、合法、低重复地抓更多高价值网页”。  
2. 技术核心不是单机抓取，而是分布式调度、URL 去重、礼貌抓取和增量更新。  
3. 面试官真正想听的是：你如何在高并发抓取时，避免被封、避免重复、还能快速恢复故障。  

## 2. 一个真实场景故事
你在做一个“垂直招聘搜索”产品。每天要抓 2000 万个职位页。上线第一周，系统把同一批 URL 抓了 4 次，带宽炸了；还因为无视 `robots.txt` 被几个大站封禁，导致数据断流。你重构后做了三件事：  
1. 统一 URL 规范化 + Bloom Filter 去重。  
2. Host 维度礼貌抓取（限速、重试退避、黑白名单）。  
3. 增量策略（Last-Modified/ETag + 内容指纹）。  
之后重复抓取率从 38% 降到 4%，封禁率下降 90% 以上。

## 3. 术语白话表（新手可懂）
1. Seed URL：最开始那批“种子网址”。  
2. Frontier：待抓取 URL 队列。  
3. URL 规范化：把“看起来不同但其实同一页”的 URL 变成同一个标准串。  
4. Bloom Filter：省内存的“可能存在”去重结构，有误判但几乎不漏判。  
5. robots.txt：站点告诉爬虫哪些路径能抓、哪些别抓。  
6. Politeness Delay：同一站点两次请求之间必须间隔的时间。  
7. ETag/Last-Modified：服务端告诉你“页面是否有变化”的标记。  
8. Canonical URL：页面声明的标准地址，帮助去重。  
9. Crawl Budget：你每天能抓多少页的“预算”。  
10. Parser：把 HTML 变成结构化字段的模块。  
11. Retry with Backoff：失败后重试，但每次等待更久，避免打爆对方。  
12. Dead Letter Queue：多次失败后放弃实时重试，进入人工或离线处理队列。  

## 4. 需求澄清（功能/非功能/不做范围/SLO）
### 4.1 功能需求
1. 从 Seed 开始发现新 URL 并持续抓取。  
2. 去重、解析、抽取正文和外链。  
3. 支持增量抓取，优先更新高价值页面。  
4. 遵守 `robots.txt`、域名限速、封禁策略。  
5. 提供任务看板：抓取成功率、延迟、重复率、封禁率。  

### 4.2 非功能需求
1. 高吞吐：峰值 20 万 URL/min。  
2. 高可用：调度层和队列层要支持故障切换。  
3. 低重复：重复抓取率 < 5%。  
4. 可恢复：单机故障不丢任务，可重放。  

### 4.3 不做范围
1. 不做搜索排序（这是 Q32 母题重点）。  
2. 不做复杂 NLP 语义理解。  
3. 不做反爬攻防对抗平台。  

### 4.4 SLO/SLA
1. 抓取任务提交到首次尝试：P95 < 60s。  
2. 抓取成功率（可抓域）：>= 98%。  
3. 调度服务可用性：>= 99.95%。  

## 5. 容量估算（数字推导）
假设目标 2 亿 URL，日更新率 5%，平均页面 300KB：  
1. 每天需抓取量：2 亿 * 5% = 1000 万页。  
2. 日流量：1000 万 * 300KB = 3,000,000,000KB ≈ 2.79TB/天。  
3. 平均 QPS：1000 万 / 86400 ≈ 116 req/s。  
4. 峰值按 8 倍：约 930 req/s。  
5. 队列保留 3 天重试任务：约 3000 万消息，按 250B 元数据约 7GB。  
6. 原始 HTML 保留 7 天：2.79TB * 7 ≈ 19.5TB（可压缩分层存储）。  

## 6. 架构设计（简版+完整版）
### 6.1 简版
`Seed输入 -> URL规范化 -> 去重 -> 调度队列 -> Crawler Worker -> Parser -> 存储 -> 新URL回流`

### 6.2 完整版
1. URL Ingest Service：接收 Seed 和外部补种。  
2. Normalizer：统一协议、参数排序、去锚点、Canonical 处理。  
3. Dedup Layer：Bloom Filter + 精确 KV（双层去重）。  
4. Frontier Scheduler：按 host、优先级、抓取预算调度。  
5. Fetcher Pool：并发抓取，连接池、超时、重试、代理策略。  
6. Politeness Guard：robots 缓存、host 级令牌桶、封禁回避。  
7. Parser & Extractor：正文提取、结构化字段、链接抽取。  
8. Storage：对象存储（原文）+ OLTP（元数据）+ OLAP（统计）。  
9. Observability：指标、日志、Trace、告警、Runbook。  

## 7. API 设计（请求/响应/错误码/幂等）
1. `POST /v1/crawl/jobs`：提交抓取任务（支持幂等键）。  
2. `GET /v1/crawl/jobs/{jobId}`：查看任务状态。  
3. `POST /v1/crawl/seeds/import`：批量导入种子。  
4. `GET /v1/crawl/hosts/{host}/policy`：查看 host 礼貌策略。  

请求示例：
```json
{
  "source": "partner_feed",
  "urls": ["https://example.com/jobs/123?a=1&b=2"],
  "priority": "HIGH",
  "idempotencyKey": "seed-20260224-001"
}
```

响应示例：
```json
{
  "jobId": "job_8f2a",
  "accepted": 1,
  "duplicated": 0,
  "status": "QUEUED"
}
```

错误码：`429_TOO_MANY_REQUESTS`、`409_DUPLICATED_JOB`、`422_ROBOTS_DENIED`、`503_FETCHER_OVERLOADED`。  

## 8. 数据模型（实体、索引、分片分区）
1. `crawl_url`：`url_hash(pk)`、`normalized_url`、`host`、`status`、`next_fetch_at`。  
2. `crawl_result`：`result_id`、`url_hash`、`http_code`、`etag`、`content_hash`、`fetched_at`。  
3. `host_policy`：`host(pk)`、`qps_limit`、`min_interval_ms`、`robots_version`。  
4. `retry_task`：`task_id`、`url_hash`、`retry_count`、`next_retry_at`、`reason`。  
5. 索引：`idx_host_next_fetch(host,next_fetch_at)`、`idx_status_fetch(status,next_fetch_at)`。  
6. 分片策略：按 `host_hash` 分片，避免单站热点打爆单分区。  

## 9. 核心流程（正常/高峰/故障恢复）
1. 正常链路：URL 入队 -> 规范化 -> 去重 -> 按 host 调度 -> 抓取 -> 解析 -> 新 URL 回流。  
2. 高峰链路：队列积压升高 -> 启动低优先级限流 -> 保留高价值域名预算 -> 弹性扩容 Fetcher。  
3. 故障恢复：Fetcher 节点故障 -> 任务超时回收 -> 重投到同 host 其他 worker -> 失败超过阈值进 DLQ。  

## 10. 一致性与事务边界
1. URL 去重采用“Bloom 快速判重 + KV 精准判重”避免误丢。  
2. 入队与状态更新使用 Outbox，保证“写库成功但消息丢失”可补发。  
3. 结果写入采用幂等键（`url_hash + fetched_at_bucket`）防重试重复写。  
4. 不追求强一致；抓取系统以最终一致为主，核心是可回放和可补偿。  

## 11. 可用性与容错
1. 调度器主从切换，Frontier 存储多副本。  
2. Host 级限流，防止单域异常拖垮全局。  
3. 熔断：某 host 错误率 > 60% 持续 5 分钟，暂停抓取 10 分钟。  
4. 降级：解析服务过载时先只存原文，延迟结构化抽取。  
5. RTO 目标 15 分钟，RPO 目标 1 分钟。  

## 12. 可观测性（指标+阈值+处置）
核心指标：  
1. `crawl_success_rate`（成功率）  
2. `duplicate_ratio`（重复率）  
3. `queue_lag_seconds`（队列滞后）  
4. `robots_denied_ratio`（robots 拒绝率）  
5. `host_error_rate`（站点错误率）  

告警阈值：  
1. 成功率 < 95% 持续 10 分钟 -> P1。  
2. 队列滞后 > 1800s 持续 10 分钟 -> P1。  
3. 重复率 > 8% 持续 30 分钟 -> P2。  
4. robots 拒绝率 > 15% 持续 15 分钟 -> P2。  

处置流程：告警 -> 值班确认 -> 限流/熔断 -> 回放补抓 -> 复盘。  

## 13. 安全与合规
1. 严格遵守 robots 和站点条款，支持域名黑名单。  
2. User-Agent 透明标识并提供联系邮箱。  
3. 不抓取登录态隐私页，不绕过认证墙。  
4. 数据存储做加密与访问审计。  
5. 对合作源设置授权白名单和审计追踪。  

## 14. 成本与取舍
1. Bloom Filter 节省内存但有误判，需二级精确校验。  
2. 更激进并发能提升吞吐，但封禁风险上升。  
3. 全量重抓简单但贵，增量抓取复杂但长期成本低。  
4. 解析实时化体验好但算力贵，可按价值分层处理。  

## 15. Java 关键代码（贴题难点，充分细节）
### 15.1 URL 规范化与哈希
```java
public final class UrlNormalizer {
    public NormalizedUrl normalize(String raw) {
        URI uri = URI.create(raw.trim());
        String scheme = (uri.getScheme() == null ? "https" : uri.getScheme().toLowerCase());
        String host = uri.getHost() == null ? "" : uri.getHost().toLowerCase();
        String path = (uri.getPath() == null || uri.getPath().isEmpty()) ? "/" : uri.getPath();
        // 参数排序 + 去追踪参数
        Map<String, String> query = QueryParser.parse(uri.getRawQuery());
        query.remove("utm_source");
        query.remove("utm_medium");
        String canonical = scheme + "://" + host + path + QueryParser.buildSorted(query);
        String hash = Hashing.sha256(canonical);
        return new NormalizedUrl(canonical, host, hash);
    }
}
```

### 15.2 双层去重（Bloom + KV）
```java
public class DedupService {
    private final BloomFilter<CharSequence> bloom;
    private final UrlSeenRepository repo;

    public DedupResult checkAndMark(String urlHash) {
        if (!bloom.mightContain(urlHash)) {
            bloom.put(urlHash);
            repo.insertSeen(urlHash); // 精确层写入
            return DedupResult.NEW_URL;
        }
        if (repo.exists(urlHash)) {
            return DedupResult.DUPLICATED;
        }
        // Bloom误判：KV不存在，仍判新URL
        repo.insertSeen(urlHash);
        return DedupResult.NEW_URL;
    }
}
```

### 15.3 Host 维度礼貌抓取调度
```java
public class HostAwareScheduler {
    private final Map<String, RateLimiter> hostLimiters = new ConcurrentHashMap<>();

    public Optional<CrawlTask> nextTask(List<CrawlTask> candidates) {
        for (CrawlTask task : candidates) {
            RateLimiter limiter = hostLimiters.computeIfAbsent(
                task.host(),
                h -> RateLimiter.create(task.hostPolicy().qpsLimit())
            );
            if (limiter.tryAcquire()) {
                return Optional.of(task);
            }
        }
        return Optional.empty();
    }
}
```

### 15.4 失败重试与指数退避
```java
public class RetryPlanner {
    public Instant nextRetryAt(int retryCount, Instant now) {
        long baseMs = 500L;
        long maxMs = 60_000L;
        long delay = Math.min(maxMs, baseMs * (1L << Math.min(retryCount, 10)));
        long jitter = ThreadLocalRandom.current().nextLong(100, 900);
        return now.plusMillis(delay + jitter);
    }

    public boolean shouldDeadLetter(int retryCount, int statusCode) {
        return retryCount >= 8 || statusCode == 403 || statusCode == 451;
    }
}
```

### 15.5 增量抓取决策（ETag + 内容指纹）
```java
public class IncrementalFetchPolicy {
    public FetchDecision decide(UrlMeta meta, HttpHeaders headers) {
        String etag = headers.firstValue("ETag").orElse("");
        String lastModified = headers.firstValue("Last-Modified").orElse("");
        if (etag.equals(meta.lastEtag()) && !etag.isEmpty()) {
            return FetchDecision.NOT_MODIFIED;
        }
        if (lastModified.equals(meta.lastModified()) && !lastModified.isEmpty()) {
            return FetchDecision.NOT_MODIFIED;
        }
        return FetchDecision.FETCH_FULL;
    }
}
```

## 16. 前端功能代码（贴题控制台/运营页）
### 16.1 抓取任务看板页（React + TS）
```tsx
type Job = { id: string; host: string; status: string; lagSec: number; duplicateRatio: number };

export function CrawlJobDashboard() {
  const [jobs, setJobs] = useState<Job[]>([]);
  useEffect(() => {
    fetch("/api/crawl/jobs?status=RUNNING")
      .then(r => r.json())
      .then(setJobs);
  }, []);

  return (
    <table>
      <thead><tr><th>Job</th><th>Host</th><th>Status</th><th>Lag(s)</th><th>重复率</th></tr></thead>
      <tbody>
        {jobs.map(j => (
          <tr key={j.id}>
            <td>{j.id}</td><td>{j.host}</td><td>{j.status}</td>
            <td>{j.lagSec}</td><td>{(j.duplicateRatio * 100).toFixed(2)}%</td>
          </tr>
        ))}
      </tbody>
    </table>
  );
}
```

### 16.2 Host 策略管理页（React + TS）
```tsx
type HostPolicy = { host: string; qpsLimit: number; minIntervalMs: number; enabled: boolean };

export function HostPolicyPanel() {
  const [form, setForm] = useState<HostPolicy>({ host: "", qpsLimit: 2, minIntervalMs: 500, enabled: true });

  async function save() {
    await fetch("/api/crawl/host-policy", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify(form)
    });
    alert("策略已保存");
  }

  return (
    <div>
      <input value={form.host} onChange={e => setForm({ ...form, host: e.target.value })} placeholder="example.com" />
      <input type="number" value={form.qpsLimit} onChange={e => setForm({ ...form, qpsLimit: Number(e.target.value) })} />
      <input type="number" value={form.minIntervalMs} onChange={e => setForm({ ...form, minIntervalMs: Number(e.target.value) })} />
      <button onClick={save}>保存策略</button>
    </div>
  );
}
```

## 17. 测试策略
1. 单测：URL 规范化、去重误判处理、重试退避算法。  
2. 集成测试：调度 -> 抓取 -> 解析 -> 存储全链路。  
3. 压测：模拟高峰 host 热点和慢站点拖垮场景。  
4. 故障演练：队列节点宕机、Fetcher 大面积超时、robots 策略误配。  
5. 回归：每次策略上线前回放历史样本验证重复率和封禁率。  

## 18. 丰富例子（面试可复述）
1. 新增 10 万 Seed，如何防止同域爆发抓取导致封禁。  
2. 某站点把 `?from=app` 参数改了，如何避免 URL 重复翻倍。  
3. 某域 403 激增，如何自动熔断并通知人工确认。  
4. 页面频繁变动但价值低，如何降低抓取频率节省成本。  
5. 队列积压 2 小时，如何优先保障高价值域恢复。  
6. Parser 新版本导致字段为空，如何快速回滚并重跑。  
7. Bloom 误判升高，如何通过 KV 精确层兜底。  
8. 跨机房网络抖动，如何避免任务重复执行。  
9. robots 文件更新，如何秒级生效到调度层。  
10. 某合作方投诉抓取过频，如何回溯日志并给证据。  
11. 冷门站点长期抓不到，如何调高探索权重。  
12. 预算砍半后，如何通过增量策略保持核心覆盖率。  

## 19. 面试追问+回答模板
1. 问：为什么不用仅数据库去重？  
答：全靠数据库会有高 IO 成本。Bloom 先过滤 90%+ 重复，再用 KV 做精确校验，性能和准确性平衡最好。  
2. 问：如何证明系统“礼貌抓取”？  
答：按 host 维度限流、缓存 robots、设置最小抓取间隔，并保留审计日志可回溯。  
3. 问：高峰积压时先保什么？  
答：先保高价值域与新鲜度指标，再降级低优先级任务，并对积压任务做分级回放。  

## 20. 新手学习路线
1. 第一步：先理解 URL 规范化和去重。  
2. 第二步：理解 host 级限流与 robots 合规。  
3. 第三步：掌握增量抓取和失败重试。  
4. 第四步：用一个小爬虫项目做队列化重构。  
5. 第五步：补齐可观测性和故障演练思维。  

## 21. 上场前Checklist
1. 能讲清“去重、礼貌抓取、增量更新”三大主线。  
2. 能写出 URL 规范化规则和误判处理。  
3. 能给出至少 3 个告警阈值和动作。  
4. 能解释为什么最终一致适合爬虫。  
5. 能说出与搜索母题（Q32）的边界和差异。  

## 22. 与母题差异（共性/差异/新增知识/话术）
### 22.1 对应母题
- 母题：`Q32 Design Google Search Engine`。  

### 22.2 共性能力
1. 都有分布式抓取链路。  
2. 都需要 URL 去重与规范化。  
3. 都强调可观测性和故障恢复。  
4. 都依赖队列削峰和弹性扩展。  

### 22.3 关键差异
1. Q3 重点是“抓到并合规”，Q32 重点是“索引+检索+排序”。  
2. Q3 主要难点在 host 礼貌策略和抓取预算，Q32 在索引分片和检索延迟。  
3. Q3 结果多为原文和抽取字段，Q32 要服务查询 API。  
4. Q3 更关注封禁率和重复率，Q32 更关注召回率和相关性。  
5. Q3 可弱化排序算法，Q32 必讲排序链路。  

### 22.4 本题新增必补知识
1. `robots.txt` 和合规边界。  
2. Host 维度限流策略。  
3. URL canonical 规范化细节。  
4. 增量抓取策略（ETag/Last-Modified）。  
5. 抓取预算分配和高价值域优先级。  

### 22.5 面试差异话术
1. “如果 Q32 是搜索全链路，Q3 就是前半段抓取引擎，重点在合法、高效、不重复。”  
2. “Q3 的第一 KPI 是抓取成功率和重复率，不是搜索相关性。”  
3. “Q3 不需要展开排序模型，但必须展开礼貌抓取、增量更新和故障回放。”  
