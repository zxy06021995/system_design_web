# Q90：Design Peer-to-Peer Network（P2P 网络）- 95分面试版

## 1. 三句话题目本质
1. P2P 系统的核心不是“点对点传文件”，而是“在无中心或弱中心条件下，让节点持续找到彼此并可靠交换数据”。  
2. 难点在于节点频繁上下线（churn）、网络不稳定、NAT 限制、恶意节点干扰。  
3. 面试高分关键：讲清节点发现、DHT 路由、NAT 穿透、副本修复、反攻击与可观测治理。  

## 2. 一个真实场景故事
你在做一个去中心化文件分发系统：
- 白天有 100 万节点在线，晚上只剩 30 万。
- 用户分布在家宽、校园网、公司内网、移动网络，很多节点都在 NAT 后面。
- 某个热门文件突然爆火，查询和下载流量暴涨。

如果系统设计不当会出现：
- 节点找不到资源（查找失败率飙升）。
- 下载中途大量断连（体验崩）。
- 恶意节点投毒假数据（数据不可信）。

P2P 题的本质就是在这种不稳定网络环境里，仍然保证“可查、可下、可恢复”。

## 3. 术语白话表（至少 10 项）
| 术语 | 白话解释 | 面试可复述 |
|---|---|---|
| Peer | 节点 | “网络里每个参与者” |
| Bootstrap Node | 引导节点 | “新节点初始联系的已知入口” |
| Node ID | 节点身份 ID | “通常是公钥哈希，用于路由和鉴别” |
| DHT | 分布式哈希表 | “把 key 映射到负责该 key 的节点” |
| Kademlia | 常见 DHT 协议 | “按 XOR 距离找最近节点，查询跳数低” |
| K-bucket | 路由桶 | “按距离范围分组存节点信息” |
| XOR Distance | 异或距离 | “节点之间‘逻辑距离’定义方式” |
| Replication | 副本复制 | “同一 key 存多个节点，防丢失” |
| Churn | 节点波动 | “节点频繁上线下线” |
| NAT Traversal | NAT 穿透 | “让内网节点也能建立连接” |
| STUN/TURN | 穿透辅助协议 | “STUN探测公网映射，TURN中继兜底” |
| Gossip | 八卦传播 | “节点间扩散健康与拓扑信息” |
| Sybil Attack | 女巫攻击 | “恶意构造大量伪节点控制路由” |
| Eclipse Attack | 日蚀攻击 | “恶意节点包围目标节点干扰路由” |
| Quorum | 多副本确认 | “读写至少多少副本算成功” |

## 4. 需求澄清（功能/非功能/不做范围）
### 4.1 功能需求
- 节点加入/离开网络，能快速完成发现与路由表构建。
- 支持 `PUT/GET`（或 `STORE/FIND_VALUE`）键值定位。
- 支持文件分块索引与多源下载。
- 支持副本修复与数据完整性校验。
- 支持 NAT 穿透和失败时中继降级。

### 4.2 非功能需求（SLO 示例）
- 查询成功率：`>= 99%`（节点波动正常范围下）。
- 平均查询跳数：`<= 8 hops`。
- 查询延迟：`P95 <= 800ms`（跨公网场景）。
- 网络可用性：`>= 99.9%`。
- 数据完整性校验失败率：`< 0.01%`。

### 4.3 Out of Scope（首版不做）
- 不做完全匿名通信网络（如 onion routing）。
- 不做复杂链上结算激励机制。
- 不做跨协议网关（先单协议栈）。

## 5. 容量估算（含数字推导）
### 5.1 假设
- 在线节点数：`1,200,000`
- 每节点平均共享数据：`4GB`
- 逻辑 key 数：`8,000,000,000`
- 查询峰值：`90,000 qps`
- 每查询平均消息 hop：`6`

### 5.2 消息量估算
- 查询消息 QPS：`90,000 * 6 ≈ 540,000 rpc/s`
- 若每 RPC 平均 500B，消息流量约 `270MB/s`（不含响应和重试）

### 5.3 存储估算
- 逻辑数据总量约 `4.8PB`
- 若副本因子 3，物理占用约 `14.4PB`

### 5.4 关键结论
- 复制因子与可用性是硬取舍：副本多更稳但成本高。
- 查询性能核心在“路由表质量”和“节点活性管理”。
- NAT 穿透失败率若高，会显著拉低可达性和查询成功率。

## 6. 架构（简版 + 完整版）
### 6.1 简版架构
```text
Bootstrap -> Peer Discovery -> DHT Routing -> Data Store/Fetch -> Replica Repair
```

### 6.2 完整版架构
```text
[Bootstrap Cluster]
  -> Peer Join/Auth
  -> Initial Peer List

[P2P Node]
  -> Identity + Crypto
  -> Routing Table (K-buckets)
  -> RPC Engine (PING/FIND_NODE/FIND_VALUE/STORE)
  -> Local Storage (chunks + metadata)
  -> Replication Manager
  -> NAT Traversal Module (STUN/TURN/Relay)
  -> Anti-abuse Guard (rate limit/reputation)

[Background Services]
  -> Gossip Health Sync
  -> Replica Repair Scheduler
  -> Blacklist/Trust Feed
  -> Metrics Collector
```

### 6.3 组件职责
- Bootstrap：只负责“让新节点找到网络入口”，不参与全局路由决策。
- DHT Router：负责 key 路由、邻居维护和查询收敛。
- Replication Manager：保证副本数量和健康度。
- NAT Module：提高实际可连接率。

## 7. API/协议设计（含消息样例）
### 7.1 PING
Request:
```json
{
  "type": "PING",
  "fromNodeId": "0xabc123",
  "ts": 1771935000,
  "sig": "..."
}
```
Response:
```json
{
  "type": "PONG",
  "fromNodeId": "0xdef888",
  "ts": 1771935001
}
```

### 7.2 FIND_NODE
```json
{
  "type": "FIND_NODE",
  "targetNodeId": "0x7788",
  "k": 16
}
```

### 7.3 FIND_VALUE
```json
{
  "type": "FIND_VALUE",
  "key": "sha256:xxxx",
  "wantReplicas": 8
}
```

### 7.4 STORE
```json
{
  "type": "STORE",
  "key": "sha256:xxxx",
  "valueRef": "chunk://nodeA/chunk-001",
  "ttlSec": 86400,
  "version": 12
}
```

### 7.5 错误码语义
- `408_PEER_TIMEOUT`：节点无响应。
- `409_VERSION_CONFLICT`：写入版本冲突。
- `429_PEER_RATE_LIMIT`：节点限流触发。
- `451_BLOCKED_PEER`：节点被信誉系统拉黑。

## 8. 数据模型（核心表/索引）
### 8.1 节点状态表
```sql
CREATE TABLE peer_node (
  peer_id VARCHAR(64) PRIMARY KEY,
  pubkey VARCHAR(256) NOT NULL,
  ip VARCHAR(64),
  port INT,
  nat_type VARCHAR(32),               -- FULL_CONE/RESTRICTED/SYMMETRIC/UNKNOWN
  last_seen TIMESTAMP NOT NULL,
  score DOUBLE NOT NULL DEFAULT 0.0,  -- 信誉分
  status VARCHAR(16) NOT NULL         -- ACTIVE/SUSPECT/BLACKLISTED
);
CREATE INDEX idx_peer_status_seen ON peer_node(status, last_seen DESC);
```

### 8.2 DHT 元数据
```sql
CREATE TABLE dht_record (
  dht_key VARCHAR(128) PRIMARY KEY,
  value_ref VARCHAR(512) NOT NULL,
  version BIGINT NOT NULL,
  checksum VARCHAR(128) NOT NULL,
  updated_at TIMESTAMP NOT NULL
);
```

### 8.3 副本映射表
```sql
CREATE TABLE replica_mapping (
  dht_key VARCHAR(128) NOT NULL,
  peer_id VARCHAR(64) NOT NULL,
  state VARCHAR(16) NOT NULL,         -- HEALTHY/STALE/MISSING
  last_verify_at TIMESTAMP NOT NULL,
  PRIMARY KEY(dht_key, peer_id)
);
CREATE INDEX idx_replica_key_state ON replica_mapping(dht_key, state);
```

### 8.4 查询日志
```sql
CREATE TABLE lookup_log (
  lookup_id BIGINT PRIMARY KEY,
  key_hash VARCHAR(128) NOT NULL,
  hops INT NOT NULL,
  success BOOLEAN NOT NULL,
  latency_ms INT NOT NULL,
  created_at TIMESTAMP NOT NULL
);
```

## 9. 核心流程（至少 3 条）
### 9.1 节点加入流程
1. 新节点连接 bootstrap，提交 nodeId/pubkey。
2. bootstrap 返回一批可联通邻居节点。
3. 新节点发 PING/FIND_NODE 建立初始 k-buckets。
4. 完成首轮健康探测后进入 ACTIVE 状态。

### 9.2 查询流程（Kademlia 迭代查找）
1. 对 key 取哈希，计算目标 ID。
2. 从本地路由表取最近的 `alpha` 个节点并并发查询。
3. 每轮用返回的更近节点替换候选集合。
4. 命中 value 则返回；否则直到收敛到最近节点集。

### 9.3 高峰流程（热门资源）
1. 某 key 查询 QPS 暴涨。
2. 系统动态提高该 key 副本因子（如 3 -> 8）。
3. 多源并发下载，客户端按块拼接校验。
4. 热度回落后逐步回收冗余副本。

### 9.4 故障恢复流程（节点大规模下线）
1. Gossip 检测某区域节点大量离线。
2. Replica Manager 扫描受影响 key，补齐副本。
3. 查询路由自动绕开离线节点，更新 k-buckets。
4. 恢复后做拓扑重平衡。

## 10. 一致性与事务边界
### 10.1 一致性策略
- P2P 通常采用最终一致，不做全局强一致。
- 写入冲突可用版本号（version vector 或 LWW）解决。

### 10.2 写入策略
- `W` 个副本确认后返回成功（如 W=2）。
- 读取时从 `R` 个副本取最新（如 R=2）。
- 满足 `R + W > N` 可提高读写一致性概率。

### 10.3 边界说明
- 节点离线和网络分区下会出现短时旧数据读取。
- 通过读修复（read-repair）和反熵同步最终收敛。

### 10.4 面试可复述
“P2P 不追求强一致，核心是高可用下的最终收敛和可验证完整性。”

## 11. 可用性与容错
### 11.1 常见故障
- 高 churn 导致路由表老化。
- NAT 穿透失败导致节点不可达。
- 恶意节点返回假路由或假数据。
- 大量 key 副本不足导致查询失败。

### 11.2 容错策略
- 周期性 PING 刷新路由桶，淘汰失效节点。
- NAT 穿透失败时转 Relay/TURN 兜底。
- 数据块哈希校验，不通过直接丢弃。
- 副本巡检与自动修复任务持续运行。

### 11.3 RTO / RPO
- 查询可用性恢复 RTO：`<= 10 分钟`（大规模波动场景）。
- 副本恢复 RPO：分钟到小时级（视副本策略）。

## 12. 可观测性（指标 + 告警阈值）
### 12.1 网络健康指标
- `active_peers`
- `peer_churn_rate`
- `nat_traversal_success_rate`

阈值示例：
- `peer_churn_rate > 25%/10min` -> P1
- `nat_traversal_success_rate < 70%` 持续 15 分钟 -> P1

### 12.2 查询指标
- `lookup_success_rate`
- `lookup_hops_avg`
- `lookup_latency_p95`

阈值示例：
- `lookup_success_rate < 95%` 持续 10 分钟 -> P1
- `lookup_hops_avg > 10` 持续 30 分钟 -> P2

### 12.3 副本指标
- `replica_health_ratio`
- `under_replicated_key_count`
- `repair_queue_backlog`

阈值示例：
- `replica_health_ratio < 98%` -> P1
- `under_replicated_key_count > baseline*2` -> P1

## 13. 安全与防攻击
- 节点身份绑定公私钥，消息签名校验。
- 限制同网段/同指纹短时间内的大量新节点注册（抗 Sybil）。
- 路由表多样性约束，防止被单一来源污染（抗 Eclipse）。
- 数据完整性校验（chunk hash + merkle root）。
- 黑名单和信誉分系统，动态剔除恶意节点。

## 14. 成本与取舍
### 14.1 成本构成
- 副本占用的存储成本。
- 节点间流量成本（查询 + 同步 + 修复）。
- 中继服务成本（NAT 兜底时会增加）。

### 14.2 关键取舍
- 副本因子越高，可用性越好但成本越高。
- 路由表越大，查询更快但维护开销更高。
- 穿透失败全走 relay 会稳但带宽成本高。

### 14.3 降本策略
- 热 key 动态升副本，冷 key 降副本。
- 查询近邻优先，减少跨区流量。
- relay 仅兜底，优先直连。

## 15. 关键代码（Java 更细 + 前端功能代码）
### 15.1 Java：XOR 距离与最近节点选择
```java
public class DistanceUtil {
    public BigInteger xorDistance(String hexA, String hexB) {
        return new BigInteger(hexA, 16).xor(new BigInteger(hexB, 16));
    }

    public List<Peer> closestPeers(String targetId, Collection<Peer> peers, int k) {
        return peers.stream()
            .sorted(Comparator.comparing(p -> xorDistance(p.getNodeId(), targetId)))
            .limit(k)
            .toList();
    }
}
```

### 15.2 Java：K-bucket 路由表维护（刷新+淘汰）
```java
public class RoutingTable {
    private final List<KBucket> buckets;
    private final String selfId;

    public RoutingTable(String selfId, int bucketCount) {
        this.selfId = selfId;
        this.buckets = IntStream.range(0, bucketCount).mapToObj(i -> new KBucket(20)).toList();
    }

    public void onPeerSeen(Peer peer) {
        int idx = bucketIndex(peer.getNodeId());
        buckets.get(idx).addOrRefresh(peer);
    }

    public void evictStalePeers() {
        for (KBucket b : buckets) {
            b.removeIf(p -> Duration.between(p.getLastSeen(), Instant.now()).toMinutes() > 15);
        }
    }

    private int bucketIndex(String otherId) {
        BigInteger d = new BigInteger(selfId, 16).xor(new BigInteger(otherId, 16));
        return Math.max(0, d.bitLength() - 1);
    }
}
```

### 15.3 Java：Kademlia 迭代查找（alpha 并发）
```java
public class IterativeLookupService {
    private static final int ALPHA = 3;
    private static final int K = 16;

    public Optional<ValueRef> findValue(String keyHash) {
        Set<String> queried = new HashSet<>();
        PriorityQueue<PeerDistance> frontier = new PriorityQueue<>(Comparator.comparing(pd -> pd.distance));
        routing.closestPeers(keyHash, K).forEach(p -> frontier.offer(new PeerDistance(p, distance(p, keyHash))));

        while (!frontier.isEmpty()) {
            List<PeerDistance> batch = new ArrayList<>();
            while (!frontier.isEmpty() && batch.size() < ALPHA) {
                PeerDistance pd = frontier.poll();
                if (queried.add(pd.peer.getNodeId())) batch.add(pd);
            }
            if (batch.isEmpty()) break;

            for (PeerDistance pd : batch) {
                FindValueResp resp = rpc.findValue(pd.peer, keyHash);
                if (resp.getValue().isPresent()) return resp.getValue();
                for (Peer n : resp.getCloserPeers()) {
                    if (!queried.contains(n.getNodeId())) {
                        frontier.offer(new PeerDistance(n, distance(n, keyHash)));
                    }
                }
            }
        }
        return Optional.empty();
    }

    private BigInteger distance(Peer p, String keyHash) {
        return new BigInteger(p.getNodeId(), 16).xor(new BigInteger(keyHash, 16));
    }

    private static class PeerDistance {
        Peer peer;
        BigInteger distance;
        PeerDistance(Peer p, BigInteger d) { this.peer = p; this.distance = d; }
    }
}
```

### 15.4 Java：副本修复任务（不足副本自动补齐）
```java
public class ReplicaRepairService {
    private final int targetReplica = 3;

    public void repairKey(String keyHash) {
        List<Peer> current = replicaRepo.healthyReplicas(keyHash);
        if (current.size() >= targetReplica) return;

        int need = targetReplica - current.size();
        List<Peer> candidates = routing.closestPeers(keyHash, 20).stream()
            .filter(p -> !current.contains(p))
            .limit(need)
            .toList();

        ValueRef ref = valueRepo.getValueRef(keyHash);
        for (Peer p : candidates) {
            boolean ok = rpc.store(p, keyHash, ref);
            if (ok) replicaRepo.markHealthy(keyHash, p.getNodeId());
        }
    }
}
```

### 15.5 Java：NAT 穿透与中继降级
```java
public class ConnectivityService {
    public ConnectResult connect(Peer target) {
        NatInfo selfNat = stunClient.detect();
        NatInfo targetNat = rpc.queryNat(target);

        if (canHolePunch(selfNat, targetNat)) {
            boolean ok = holePunchClient.tryPunch(target);
            if (ok) return ConnectResult.direct();
        }
        // 打洞失败走中继
        RelayEndpoint relay = relayPool.pick();
        return relayClient.connectViaRelay(target, relay) ? ConnectResult.relay(relay) : ConnectResult.failed();
    }

    private boolean canHolePunch(NatInfo a, NatInfo b) {
        return a.isPunchFriendly() && b.isPunchFriendly();
    }
}
```

### 15.6 前端（React + TypeScript）：节点健康与拓扑面板
```tsx
import { useEffect, useState } from "react";

type PeerStat = {
  peerId: string;
  status: "ACTIVE" | "SUSPECT" | "BLACKLISTED";
  natType: string;
  lastSeen: string;
};

export function PeerHealthPanel() {
  const [peers, setPeers] = useState<PeerStat[]>([]);
  const [summary, setSummary] = useState({ active: 0, suspect: 0, blacklisted: 0 });

  useEffect(() => {
    let timer: number;
    const load = async () => {
      const resp = await fetch("/api/v1/p2p/peers/health");
      if (resp.ok) {
        const data = await resp.json();
        setPeers(data.items || []);
        setSummary(data.summary || { active: 0, suspect: 0, blacklisted: 0 });
      }
      timer = window.setTimeout(load, 5000);
    };
    load();
    return () => window.clearTimeout(timer);
  }, []);

  return (
    <div>
      <h3>P2P 节点健康</h3>
      <p>active={summary.active} suspect={summary.suspect} blacklisted={summary.blacklisted}</p>
      <ul>
        {peers.slice(0, 30).map((p) => (
          <li key={p.peerId}>
            {p.peerId} | {p.status} | {p.natType} | lastSeen={p.lastSeen}
          </li>
        ))}
      </ul>
    </div>
  );
}
```

### 15.7 前端（React + TypeScript）：查询路径可视化（调试工具）
```tsx
import { useState } from "react";

type LookupStep = { hop: number; peerId: string; latencyMs: number; result: string };

export function LookupTracePanel() {
  const [keyHash, setKeyHash] = useState("");
  const [steps, setSteps] = useState<LookupStep[]>([]);
  const [msg, setMsg] = useState("");

  const trace = async () => {
    const resp = await fetch("/api/v1/p2p/lookup/trace", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ keyHash })
    });
    if (!resp.ok) {
      setMsg(`trace失败: ${resp.status}`);
      return;
    }
    const data = await resp.json();
    setSteps(data.steps || []);
    setMsg(`success=${data.success}, hops=${data.hops}, latency=${data.latencyMs}ms`);
  };

  return (
    <div>
      <h3>Lookup Trace</h3>
      <input value={keyHash} onChange={(e) => setKeyHash(e.target.value)} placeholder="输入 key hash" />
      <button onClick={trace}>追踪查询路径</button>
      {msg && <p>{msg}</p>}
      <pre>{JSON.stringify(steps, null, 2)}</pre>
    </div>
  );
}
```

## 16. 测试策略
### 16.1 单元测试
- XOR 距离排序正确性。
- k-bucket 插入、淘汰、刷新逻辑。
- 状态冲突（版本）处理。

### 16.2 集成测试
- 节点加入 -> 路由建立 -> 查找命中全链路。
- NAT 穿透失败自动 relay 路径。
- 副本修复任务对 under-replicated key 的恢复能力。

### 16.3 压测
- 90k 查询 QPS 压测。
- 30% churn 场景下查询成功率压测。
- 热 key 高并发场景副本扩展效果压测。

### 16.4 故障注入
- 大量节点下线。
- 网络分区和高丢包。
- 恶意节点注入假路由和假 value。

## 17. 丰富例子（至少 10 个）
1. 新节点上线后通过 bootstrap 获取初始邻居。  
2. 节点 15 分钟未心跳，被路由表淘汰。  
3. 查询 key 时首轮失败，第二轮找到更近节点命中。  
4. NAT 对称型节点无法打洞，自动走 relay。  
5. 热门文件临时提升副本因子，提高下载成功率。  
6. 某副本节点下线，修复任务补齐新副本。  
7. 恶意节点返回假 value，hash 校验失败后拒绝。  
8. 路由表被单一网段污染，反作弊规则触发清理。  
9. 网络分区恢复后通过 gossip 重新收敛拓扑。  
10. 查询 hops 从 11 降到 6，延迟显著下降。  
11. 高 churn 夜间时段自动提高探测频率。  
12. 下载中途节点离线，客户端自动切换到其他副本继续下载。  

## 18. 面试追问 + 可复述回答
### Q1：为什么常用 Kademlia？
可复述：  
“因为 XOR 距离定义简单、路由可收敛，查询复杂度接近 O(logN)，在大规模节点下表现稳定。”

### Q2：NAT 穿透失败怎么办？
可复述：  
“优先 STUN 打洞，失败后走 TURN/Relay 兜底，保证可达性优先。”

### Q3：如何保证数据不被恶意节点篡改？
可复述：  
“每个块做内容哈希校验，元数据签名，校验不过直接丢弃并下调节点信誉。”

### Q4：节点频繁上下线会不会导致查询失败？
可复述：  
“会，所以要持续刷新路由桶、提高冗余副本、并行查询多个近邻节点。”

### Q5：副本一致性如何实现？
可复述：  
“采用最终一致策略，写入版本控制 + 周期反熵 + 读修复。”

### Q6：如何防止 Sybil 攻击？
可复述：  
“准入限速、身份成本、信誉系统和拓扑多样性约束联合治理。”

## 19. 新手学习路线
### 第 1 周：DHT 基础
- 实现 Node ID、XOR 距离、k-bucket。
- 跑通 FIND_NODE/FIND_VALUE。

### 第 2 周：可靠性
- 加副本策略和修复任务。
- 加节点心跳和失效淘汰。

### 第 3 周：网络现实问题
- 做 NAT 探测与打洞。
- 增加 relay 兜底路径。

### 第 4 周：安全与运维
- 增加签名校验和信誉系统。
- 做前端健康看板和 lookup trace 工具。

## 20. 上场前 Checklist
- [ ] 我能解释 P2P 与中心化架构的关键差异。  
- [ ] 我能讲清 Kademlia 的查询流程和复杂度。  
- [ ] 我能说出节点 churn 如何影响可用性及应对策略。  
- [ ] 我能说明 NAT 穿透失败时的降级路径。  
- [ ] 我能给出至少 3 个可执行告警阈值。  
- [ ] 我能解释副本修复与最终一致机制。  
- [ ] 我能讲出 Sybil/Eclipse 基本防护思路。  
- [ ] 我能展示前端面板如何排查查询失败路径。  

## 21. 30 秒总结
P2P 网络高分答案是：  
“通过 DHT 实现去中心路由，通过副本和修复保证可用，通过 NAT 穿透和中继保证可达，通过签名与信誉体系保证可信，再用观测系统持续治理 churn 和攻击风险。”  
把这句话落到具体协议、数据模型、阈值和代码实现，你的面试答案就会非常扎实。  
