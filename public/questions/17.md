# Q17 GFS 分布式文件系统（高频）

## 1. 三句话题目本质
1. 这题核心是“海量文件如何在多机器上可靠存储并高吞吐读写”。  
2. 难点在于分块、副本一致性、元数据管理和故障恢复。  
3. 面试高分要讲清：控制面与数据面分离、故障场景怎么不丢数据。  

## 2. 一个真实场景故事
你要为日志平台存储每天 3PB 新数据。单机存储很快到瓶颈，且磁盘故障频繁。改造成 GFS 风格后：  
1. 大文件按 Chunk 分块多副本。  
2. Master 管理元数据，ChunkServer 管数据块。  
3. 自动重复制修复坏副本。  
故障下仍能持续写入，数据可用性显著提升。  

## 3. 术语白话表（新手可懂）
1. Chunk：文件切成的大块（如 64MB）。  
2. ChunkServer：存块的机器。  
3. Master：元数据管理节点。  
4. Replica：副本。  
5. Lease：主副本写入租约。  
6. Append：追加写（日志场景常见）。  
7. Checksum：校验和，检测坏块。  
8. Heartbeat：心跳，Master 感知节点存活。  
9. Re-replication：副本不足时自动补副本。  
10. Snapshot：快照。  
11. Garbage Collection：垃圾回收，清理旧块。  
12. Rack Awareness：跨机架放副本抗故障。  

## 4. 需求澄清（功能/非功能/不做范围/SLO）
### 4.1 功能需求
1. 支持 TB 级大文件读写。  
2. 支持 append 主导的写入模式。  
3. 支持多副本容灾。  
4. 支持坏块检测与自动修复。  
5. 支持目录和文件元数据管理。  

### 4.2 非功能需求
1. 高吞吐优先于低延迟。  
2. 高可用，单机故障不影响整体。  
3. 可扩展，节点可水平扩展。  
4. 可运维，故障可定位可修复。  

### 4.3 不做范围
1. 不做 POSIX 完整语义。  
2. 不做小文件极致优化。  
3. 不做跨云统一文件系统。  

### 4.4 SLO/SLA
1. 数据可用性 >= 99.99%。  
2. 写入吞吐满足 3PB/天摄入。  
3. 坏块修复延迟 P95 < 10 分钟。  

## 5. 容量估算（数字推导）
假设日新增 3PB，Chunk 64MB，副本数 3：  
1. 单日 Chunk 数约 `3PB / 64MB ≈ 50,331,648`。  
2. 含 3 副本总物理写入约 9PB/天。  
3. 若单 ChunkServer 每天可靠承载 30TB，需约 300 台以上。  
4. Master 元数据每块按 200B，单日新增元数据约 10GB。  
5. 元数据需内存驻留与日志持久化双保障。  

## 6. 架构设计（简版+完整版）
### 6.1 简版
`Client -> Master(元数据) -> ChunkServer(数据块副本)`

### 6.2 完整版
1. Master：命名空间、块映射、租约管理。  
2. ChunkServer：块存储、校验、复制。  
3. Client Library：先问元数据再直连数据节点。  
4. Replication Manager：副本策略与重复制。  
5. Health Monitor：心跳、坏盘、网络隔离检测。  
6. Checksum Validator：读写校验。  
7. Snapshot Manager：元数据快照和回滚。  
8. GC Service：失效块清理。  

## 7. API 设计（请求/响应/错误码/幂等）
1. `POST /v1/fs/create`  
2. `POST /v1/fs/append`  
3. `GET /v1/fs/read?offset=&length=`  
4. `POST /v1/fs/snapshot`  
5. `GET /v1/fs/chunks/{chunkId}/replicas`  

append 请求示例：
```json
{
  "path": "/logs/2026/02/24/app.log",
  "dataRef": "obj://upload-buf/blk_88",
  "idempotencyKey": "append-20260224-88"
}
```

错误码：`503_MASTER_LEADER_UNAVAILABLE`、`409_LEASE_CONFLICT`、`507_CHUNK_NO_SPACE`、`500_CHECKSUM_MISMATCH`。  

## 8. 数据模型（实体、索引、分片分区）
1. `inode_meta`：路径、inode、权限、版本。  
2. `chunk_meta`：chunk_id、file_id、index、primary_replica。  
3. `replica_meta`：chunk_id、server_id、rack_id、status。  
4. `lease_meta`：chunk_id、holder、expire_at。  
5. `checksum_meta`：chunk_id、segment_no、crc32。  
6. 索引：`idx_file_chunk(file_id,index)`。  
7. 分片：元数据按目录前缀分区。  

## 9. 核心流程（正常/高峰/故障恢复）
1. 正常写：Client 向 Master 申请租约 -> Primary 副本排序写入 -> Secondary 跟随确认。  
2. 高峰写：批量 append 合并、小包聚合、写管线并发。  
3. 故障恢复：ChunkServer 心跳丢失 -> 标记失效 -> 触发重复制 -> 恢复副本数。  

## 10. 一致性与事务边界
1. 文件系统采用弱一致 append 语义，保证最终可见。  
2. 同一 Chunk 的写入通过 primary lease 序列化。  
3. Master 元数据变更写 WAL，崩溃后重放恢复。  
4. 数据校验失败触发副本替换，不直接返回脏数据。  

## 11. 可用性与容错
1. Master 主备切换 + 元数据日志复制。  
2. 副本跨机架放置，避免机架级故障。  
3. 心跳超时快速摘除失效节点。  
4. 读路径可从任一健康副本读取。  
5. RTO 15 分钟，RPO 1 分钟。  

## 12. 可观测性（指标+阈值+处置）
关键指标：  
1. `chunkserver_live_count`  
2. `under_replicated_chunk_count`  
3. `append_throughput_mb_s`  
4. `checksum_error_rate`  
5. `master_rpc_p95_ms`  

告警阈值：  
1. 副本不足块数 > 10 万（10分钟）-> P1。  
2. checksum 错误率 > 0.01%（10分钟）-> P1。  
3. Master RPC P95 > 150ms（10分钟）-> P2。  
4. 存活 ChunkServer 下降 > 10%（5分钟）-> P1。  

处置：扩容副本 -> 隔离故障节点 -> 流量重路由 -> 校验修复。  

## 13. 安全与合规
1. Client-Server 双向认证。  
2. 传输加密与磁盘加密。  
3. 访问控制按目录和租户隔离。  
4. 审计访问日志可追溯。  
5. 数据保留和删除策略可配置。  

## 14. 成本与取舍
1. 副本数越高越安全但成本越高。  
2. 大 Chunk 提高吞吐但小随机读体验差。  
3. 强一致会降低吞吐，日志场景可接受弱一致。  
4. 主备 Master 提高可靠性但增加控制面复杂度。  

## 15. Java 关键代码（贴题难点，充分细节）
### 15.1 租约校验
```java
public class LeaseService {
    public boolean valid(Lease lease) {
        return lease != null && lease.expireAt().isAfter(Instant.now());
    }
}
```

### 15.2 Append 写序列化
```java
public class AppendCoordinator {
    public synchronized long append(Chunk chunk, byte[] data) {
        long offset = chunk.nextOffset();
        chunk.writeAt(offset, data);
        chunk.setNextOffset(offset + data.length);
        return offset;
    }
}
```

### 15.3 校验和验证
```java
public class ChecksumVerifier {
    public void verify(byte[] data, long expectedCrc) {
        CRC32 crc = new CRC32();
        crc.update(data);
        if (crc.getValue() != expectedCrc) {
            throw new IllegalStateException("CHECKSUM_MISMATCH");
        }
    }
}
```

### 15.4 副本修复任务
```java
public class ReReplicationJob {
    public void run(ChunkMeta chunk, List<ServerNode> candidates, ReplicaManager rm) {
        if (chunk.replicaCount() >= 3) return;
        ServerNode target = candidates.stream().findFirst().orElseThrow();
        rm.copyReplica(chunk.chunkId(), target.id());
    }
}
```

### 15.5 心跳处理
```java
public class HeartbeatHandler {
    public void onHeartbeat(String serverId, Instant ts, ServerRegistry registry) {
        registry.markAlive(serverId, ts);
    }
}
```

## 16. 前端功能代码（贴题控制台/运营页）
### 16.1 集群健康看板（React + TS）
```tsx
type FsMetrics = { liveNodes: number; underReplicated: number; checksumErrRate: number };

export function FsClusterDashboard() {
  const [m, setM] = useState<FsMetrics | null>(null);
  useEffect(() => { fetch("/api/fs/metrics").then(r => r.json()).then(setM); }, []);
  if (!m) return <div>loading...</div>;
  return <div>节点:{m.liveNodes} 副本不足:{m.underReplicated} 校验错误率:{m.checksumErrRate}%</div>;
}
```

### 16.2 副本修复操作页（React + TS）
```tsx
export function ReplicaRepairPanel() {
  const [chunkId, setChunkId] = useState("");
  async function repair() {
    await fetch("/api/fs/repair", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ chunkId })
    });
    alert("修复任务已提交");
  }
  return (
    <div>
      <input value={chunkId} onChange={e => setChunkId(e.target.value)} placeholder="chunk id" />
      <button onClick={repair}>修复副本</button>
    </div>
  );
}
```

## 17. 测试策略
1. 单测：租约有效性、checksum 校验、append 序列化。  
2. 集成：写入-复制-读取-恢复全链路。  
3. 压测：大文件 append 吞吐与读写混合。  
4. 故障演练：ChunkServer 宕机、Master 切换、网络分区。  
5. 数据正确性：随机抽样校验副本一致性。  

## 18. 丰富例子（面试可复述）
1. Master 挂了如何恢复。  
2. 某块副本损坏如何自动修复。  
3. 副本不足时先修哪些块。  
4. 机架故障如何保证数据可读。  
5. append 重试如何避免重复写。  
6. 小文件太多怎么处理。  
7. 元数据过大怎么拆分。  
8. 校验错误率上升如何定位。  
9. 读热点集中在少数块如何扩展。  
10. 快照和回滚如何做。  
11. 删除文件后何时真正回收空间。  
12. 跨地域容灾如何扩展。  

## 19. 面试追问+回答模板
1. 问：为什么控制面和数据面分离？  
答：Master 管元数据轻量高效，数据面直连 ChunkServer 可获得高吞吐。  
2. 问：一致性如何保证？  
答：同 Chunk 用 lease 串行写，元数据 WAL 持久化，副本通过校验与重复制收敛。  
3. 问：最常见故障是什么？  
答：节点故障和副本不足，核心是快速检测、隔离和自动修复。  

## 20. 新手学习路线
1. 先理解块存储和副本模型。  
2. 再学 Master/ChunkServer 职责分工。  
3. 学 lease、checksum、重复制。  
4. 学容灾与故障恢复。  
5. 学运维指标与告警。  

## 21. 上场前Checklist
1. 能画出控制面/数据面架构。  
2. 能讲清写入租约流程。  
3. 能给出副本修复阈值。  
4. 能解释 RTO/RPO 目标。  
5. 能讲出与母题差异。  

## 22. 与母题差异（共性/差异/新增知识/话术）
### 22.1 对应母题
- 母题：`Q31 Design Dropbox - 文件同步系统`。  

### 22.2 共性能力
1. 都是文件相关存储系统。  
2. 都有分块和副本概念。  
3. 都强调可靠性和故障恢复。  
4. 都需要版本与元数据管理。  

### 22.3 关键差异
1. Q31 偏“用户文件同步产品”；Q17 偏“底层分布式文件存储引擎”。  
2. Q17 更强调 Chunk、副本、Lease 与控制面设计。  
3. Q31 更强调客户端同步冲突与跨端体验。  
4. Q17 目标是吞吐和可靠性，Q31 目标是同步一致与产品能力。  
5. Q17 不展开客户端同步协议细节。  

### 22.4 本题新增必补知识
1. Master 元数据管理。  
2. Chunk 副本放置策略。  
3. Lease 写入一致性。  
4. Checksum 和坏块修复。  
5. 控制面高可用设计。  

### 22.5 面试差异话术
1. “Q31 是上层文件同步产品，Q17 是底层存储引擎。”  
2. “答 Q17 要重点讲块、副本、租约和修复，不是讲用户冲突解决。”  
3. “Q17 是基础设施题，性能和可靠性优先级更高。”  
