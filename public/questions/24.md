# Q24 MapReduce - 分布式计算框架（高频）

## 1. 三句话题目本质
1. MapReduce 解决的是“大规模离线数据如何并行计算”。  
2. 核心阶段是 Map、Shuffle、Reduce，难点在调度、倾斜和容错。  
3. 面试要讲清：任务如何拆、数据如何搬、失败如何重跑。  

## 2. 一个真实场景故事
你要每天统计 200TB 行为日志，单机跑要 20 小时。改成 MapReduce 后：  
1. Map 并行清洗与抽取。  
2. Shuffle 按 key 分发到 Reduce。  
3. Reduce 汇总产出报表。  
作业时长降到 50 分钟左右，但一度被“热点 key”拖慢，后续通过分桶和二次聚合解决。  

## 3. 术语白话表（新手可懂）
1. Map：把输入转成键值对。  
2. Shuffle：按 key 重分布数据。  
3. Reduce：对同 key 的值做聚合。  
4. Partition：把 key 分配到不同 reducer。  
5. Combiner：本地预聚合，减少网络传输。  
6. Data Locality：任务尽量在数据所在节点运行。  
7. Speculative Execution：推测执行，慢任务开副本。  
8. Straggler：拖后腿的慢任务。  
9. Skew：数据倾斜，某些 key 太热。  
10. Checkpoint：中间结果或状态保存点。  
11. Retry：失败重试。  
12. DAG：任务依赖图。  

## 4. 需求澄清（功能/非功能/不做范围/SLO）
### 4.1 功能需求
1. 支持批量数据处理。  
2. 支持 Map/Reduce 自定义逻辑。  
3. 支持作业失败自动重试。  
4. 支持任务监控和日志追踪。  
5. 支持资源配额与优先级队列。  

### 4.2 非功能需求
1. 高吞吐，能处理 TB/PB 数据。  
2. 容错，节点故障可恢复。  
3. 可扩展，节点数增加后可线性提升。  
4. 成本可控（资源利用率高）。  

### 4.3 不做范围
1. 不做实时流计算引擎。  
2. 不做交互式低延迟查询。  
3. 不做在线事务处理。  

### 4.4 SLO/SLA
1. 关键批处理作业按日准时完成（如 T+1 前）。  
2. 失败重试成功率 >= 99%。  
3. 集群调度服务可用性 >= 99.9%。  

## 5. 容量估算（数字推导）
假设日处理 200TB，Mapper 平均处理 200MB/s：  
1. 单 mapper 一小时处理约 720GB。  
2. 若要 1 小时完成，理论需约 278 个 mapper 并行。  
3. Shuffle 传输按输入 60% 估计约 120TB 网络流量。  
4. 若网卡总吞吐不足，会成为瓶颈。  
5. 热 key 若占 15% 数据，会拖慢某些 reducer。  

## 6. 架构设计（简版+完整版）
### 6.1 简版
`Job提交 -> 调度器 -> Map任务 -> Shuffle -> Reduce任务 -> 输出存储`

### 6.2 完整版
1. Job Manager：接收作业、解析 DAG、分配资源。  
2. Resource Manager：队列、配额、优先级。  
3. Worker Node：执行 map/reduce。  
4. Distributed Storage：输入与输出数据存储。  
5. Shuffle Service：中间数据拉取和合并。  
6. Retry & Speculative：失败重试与慢任务推测执行。  
7. Metrics & Logs：进度、失败原因、倾斜检测。  
8. Catalog：作业元数据与版本管理。  

## 7. API 设计（请求/响应/错误码/幂等）
1. `POST /v1/jobs`：提交作业。  
2. `GET /v1/jobs/{jobId}`：查看状态。  
3. `POST /v1/jobs/{jobId}/cancel`：取消作业。  
4. `GET /v1/jobs/{jobId}/metrics`：查看指标。  

提交示例：
```json
{
  "name": "daily_user_uv",
  "input": "s3://logs/2026-02-24/",
  "output": "s3://dw/uv/2026-02-24/",
  "mapper": "com.demo.UvMapper",
  "reducer": "com.demo.UvReducer",
  "idempotencyKey": "job-uv-20260224"
}
```

错误码：`409_DUPLICATE_JOB`、`422_INVALID_DAG`、`503_NO_RESOURCE`。  

## 8. 数据模型（实体、索引、分片分区）
1. `job_meta`：job_id、owner、priority、status。  
2. `task_meta`：task_id、job_id、type、attempt、state。  
3. `shuffle_block`：map_task_id、partition_id、size。  
4. `task_log_index`：task_id、log_uri。  
5. `resource_quota`：队列、CPU、内存上限。  
6. 分区：按 `job_id`/`task_id` 分片存储元数据。  

## 9. 核心流程（正常/高峰/故障恢复）
1. 正常：切分输入 -> map -> shuffle -> reduce -> 写输出。  
2. 高峰：资源紧张时按队列优先级调度，低优先级作业排队。  
3. 故障恢复：worker 宕机 -> 任务重试到其他节点 -> 失败超阈值报警。  

## 10. 一致性与事务边界
1. 任务输出采用临时目录 + commit，避免半成品被读取。  
2. task retry 要幂等，重复执行不污染结果。  
3. 最终结果在 job 成功 commit 后可见。  
4. 中间 shuffle 数据允许重建，不做强持久化事务。  

## 11. 可用性与容错
1. 调度器 HA（主备或多副本）。  
2. Worker 无状态，失败可重调度。  
3. 慢任务推测执行防拖尾。  
4. 数据倾斜自动检测与告警。  
5. RTO 20 分钟，RPO 0（批作业可重跑）。  

## 12. 可观测性（指标+阈值+处置）
关键指标：  
1. `job_success_rate`  
2. `avg_task_retry_count`  
3. `shuffle_wait_time`  
4. `straggler_ratio`  
5. `queue_wait_p95_sec`  

告警阈值：  
1. 作业成功率 < 95%（日）-> P1。  
2. 拖尾任务占比 > 10%（30分钟）-> P2。  
3. shuffle 等待 > 300s（10分钟）-> P1。  
4. 队列等待 P95 > 600s（10分钟）-> P2。  

处置：扩容 worker -> 调整分区 -> 启用推测执行 -> 降低低优先级配额。  

## 13. 安全与合规
1. 作业提交鉴权与租户隔离。  
2. 输入输出路径 ACL。  
3. 敏感字段脱敏处理。  
4. 作业日志审计。  
5. 数据生命周期管理。  

## 14. 成本与取舍
1. 更高并行度更快但资源成本更高。  
2. 推测执行能降尾延迟但会增加重复计算。  
3. 更多 reducer 降低单点压力但增加调度开销。  
4. Combiner 能降网络开销，但不适用于所有聚合函数。  

## 15. Java 关键代码（贴题难点，充分细节）
### 15.1 Mapper 示例
```java
public class UvMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
    private static final IntWritable ONE = new IntWritable(1);
    @Override
    protected void map(LongWritable key, Text value, Context ctx) throws IOException, InterruptedException {
        String userId = parseUserId(value.toString());
        if (userId != null) ctx.write(new Text(userId), ONE);
    }
}
```

### 15.2 Reducer 示例
```java
public class UvReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context ctx) throws IOException, InterruptedException {
        int sum = 0;
        for (IntWritable v : values) sum += v.get();
        ctx.write(key, new IntWritable(sum));
    }
}
```

### 15.3 自定义分区器（防倾斜）
```java
public class SkewAwarePartitioner extends Partitioner<Text, IntWritable> {
    @Override
    public int getPartition(Text key, IntWritable value, int numPartitions) {
        if (isHotKey(key.toString())) {
            return Math.abs((key.toString() + ThreadLocalRandom.current().nextInt(4)).hashCode()) % numPartitions;
        }
        return Math.abs(key.hashCode()) % numPartitions;
    }
}
```

### 15.4 任务重试策略
```java
public class RetryPolicy {
    public boolean shouldRetry(int attempt, Exception ex) {
        return attempt < 4 && !(ex instanceof IllegalArgumentException);
    }
}
```

### 15.5 输出原子提交
```java
public class OutputCommitter {
    public void commit(Path temp, Path finalPath, FileSystem fs) throws IOException {
        if (fs.exists(finalPath)) fs.delete(finalPath, true);
        fs.rename(temp, finalPath);
    }
}
```

## 16. 前端功能代码（贴题控制台/运营页）
### 16.1 作业监控页（React + TS）
```tsx
type JobStat = { jobId: string; status: string; progress: number; queueWaitSec: number };

export function JobDashboard() {
  const [jobs, setJobs] = useState<JobStat[]>([]);
  useEffect(() => { fetch("/api/jobs").then(r => r.json()).then(setJobs); }, []);
  return <ul>{jobs.map(j => <li key={j.jobId}>{j.jobId} {j.status} {j.progress}% 等待{j.queueWaitSec}s</li>)}</ul>;
}
```

### 16.2 作业提交页（React + TS）
```tsx
export function JobSubmitPage() {
  const [input, setInput] = useState("s3://logs/2026-02-24/");
  async function submit() {
    await fetch("/api/jobs", {
      method: "POST",
      headers: {"Content-Type":"application/json"},
      body: JSON.stringify({ name: "daily_uv", input, output: "s3://dw/uv/latest/" })
    });
    alert("作业已提交");
  }
  return <div><input value={input} onChange={e => setInput(e.target.value)} /><button onClick={submit}>提交</button></div>;
}
```

## 17. 测试策略
1. 单测：Mapper/Reducer 正确性。  
2. 集成：小数据集端到端结果校验。  
3. 压测：大规模 shuffle 下网络与磁盘瓶颈。  
4. 故障演练：worker 宕机、慢任务、重试风暴。  
5. 回归：结果一致性对比基线。  

## 18. 丰富例子（面试可复述）
1. 热 key 导致 reducer 慢如何处理。  
2. 为什么要 combiner。  
3. 推测执行什么时候开。  
4. 作业重复提交如何幂等。  
5. 半成品输出如何避免被消费。  
6. 任务一直重试怎么止损。  
7. 输入数据脏行怎么处理。  
8. 资源争抢时如何保障核心报表。  
9. shuffle 过大如何优化。  
10. 队列等待太久如何分级。  
11. map 端本地失败如何恢复。  
12. 日终 SLA 快超时如何应急。  

## 19. 面试追问+回答模板
1. 问：MapReduce 为何适合离线？  
答：吞吐优先、容错强、可批量并行，适合大规模离线统计。  
2. 问：最常见性能问题是什么？  
答：数据倾斜和 shuffle 瓶颈。  
3. 问：如何保证结果正确？  
答：任务幂等、输出原子提交、失败重跑与结果校验。  

## 20. 新手学习路线
1. 先写简单 word count。  
2. 学 Shuffle/Partition 机制。  
3. 学倾斜治理和 combiner。  
4. 学调度和重试策略。  
5. 学监控与 SLA 运维。  

## 21. 上场前Checklist
1. 能完整讲 Map->Shuffle->Reduce。  
2. 能解释数据倾斜治理。  
3. 能给出告警阈值和处置。  
4. 能讲清重试与幂等。  
5. 能讲出与母题差异。  

## 22. 与母题差异（共性/差异/新增知识/话术）
### 22.1 对应母题
- 母题：`Q41 Data Pipeline`。  

### 22.2 共性能力
1. 都是数据处理链路。  
2. 都有任务调度和容错。  
3. 都关注可观测性和质量。  
4. 都涉及吞吐与成本平衡。  

### 22.3 关键差异
1. Q41 是端到端管道系统；Q24 是批计算执行引擎。  
2. Q24 更深讲 Map/Shuffle/Reduce 算法框架。  
3. Q41 更关注多源接入、治理和下游分发。  
4. Q24 对倾斜和拖尾治理更细。  
5. Q24 强调作业级并行执行机制。  

### 22.4 本题新增必补知识
1. 分区器与 shuffle 机制。  
2. combiner 适用边界。  
3. 推测执行策略。  
4. 输出原子提交。  
5. 任务重试与拖尾治理。  

### 22.5 面试差异话术
1. “Q41 讲数据系统全链路，Q24 讲其中的批处理计算内核。”  
2. “Q24 答题重点在执行模型和性能瓶颈，而不是业务指标口径。”  
3. “把 shuffle、倾斜、重试讲清，就抓住了 Q24 的核心。”  
