# Q80 - Design Ad Click Aggregation System

## 1. 题目元信息（先读源码）
- title: `Design Ad Click Aggregation System`
- tags: `点击统计`、`实时聚合`、`数据分析`、`广告系统`、`流处理`
- keyPoints: `流式处理`、`实时聚合`、`数据存储`、`报表生成`、`数据一致性`
- learningCoreId: `36`

## 2. 题目本质
在高并发广告流量下，系统要把“点击事件”稳定接入、去重、聚合并准实时出报表。  
核心矛盾是低延迟查询与高吞吐写入、以及实时口径与离线校准的一致性。  
必须接受“最终一致 + 可回放修正”，而不是追求全链路强一致。

## 3. 业务目标与范围
- 支持 campaign/ad/creative/publisher 多维度点击聚合
- 支持分钟级实时看板与小时级结算口径
- 支持去重、防作弊、迟到数据修正
- 不做广告检索召回与竞价引擎（仅聚合链路）

## 4. 需求澄清
- 功能：事件采集、幂等写入、实时聚合、离线校准、报表查询、重放修复
- 非功能：99.9% 可用、P95 查询 < 800ms、分钟级可见
- 数据：支持乱序/重复/迟到事件，保留原始日志便于审计

## 5. 容量与吞吐估算
- 峰值点击：`1,200,000 events/s`
- 单事件（压缩后）约 `180B`，写入带宽约 `216MB/s`
- 日增原始数据：`1,200,000 * 180 * 86400 ≈ 18.7TB/day`
- 实时聚合状态（热 48h）：约 `1.5~2.5TB`（按维度基数估算）

## 6. 架构总览
- 接入层：SDK -> Edge Collector -> Kafka/Pulsar
- 计算层：Flink/Spark Streaming 做窗口聚合与去重
- 存储层：Raw Log（对象存储）+ OLAP（ClickHouse/Druid）+ Redis 热缓存
- 服务层：Report API、回放修复任务、对账任务、告警系统

## 7. 数据模型设计
- `click_raw(event_id, ts, ad_id, campaign_id, user_id, ip, ua, cost, source, trace_id)`
- `click_agg_minute(bucket_minute, ad_id, campaign_id, publisher_id, clicks, uv, cost_sum, version)`
- `click_agg_hourly(bucket_hour, ..., clicks, dedup_rate, late_fix_count)`
- 主键建议：`(bucket, ad_id, campaign_id, publisher_id)`

## 8. API 设计
- `POST /v1/click-events:batch`：批量上报，Header 带 `Idempotency-Key`
- `GET /v1/reports/realtime?campaignId=...&from=...&to=...&granularity=minute`
- `POST /v1/replay-jobs`：按时间段回放修正
- `GET /v1/replay-jobs/{jobId}`：查询修复进度

## 9. 核心流程
1. 事件写入 MQ，按 `campaign_id` 分区确保局部有序。  
2. 流计算先去重再窗口聚合，写入分钟表。  
3. 报表读分钟表 + 缓存；离线任务小时级校准并回填差异。  
4. 出现故障时触发回放，重建指定时间窗聚合。

## 10. 一致性与事务边界
- 接入层：`至少一次` 投递，依赖 `event_id` 幂等去重
- 聚合层：窗口内去重 + watermark 处理迟到数据
- 查询层：实时口径与结算口径分离，面向用户明确口径标签
- 事务边界：单事件入队原子；跨系统一致性通过重放补偿实现

## 11. 阈值与SLO
- 接入可用性：`>= 99.9%`
- 事件积压阈值：`consumer lag > 120s` 触发 P1
- 去重异常阈值：`dedup_rate < 92%` 或突增 `> 99.8%` 触发排查
- 查询时延阈值：`P95 > 800ms` 持续 5 分钟告警
- 数据新鲜度：`minute table delay <= 90s`

## 12. 故障恢复路径（含 RTO/RPO）
- 目标：`RTO <= 15 分钟`，`RPO <= 60 秒`
- 路径A（计算节点故障）：自动拉起 -> 从 checkpoint 恢复 -> 追平 lag -> 恢复看板
- 路径B（OLAP 局部不可用）：读降级到 Redis 最近窗口 + 标记“近似值”
- 路径C（数据污染）：冻结错误分区 -> 启动 replay job -> 按版本覆盖回填
- 路径D（整区故障）：跨 AZ 切流 -> 用对象存储 raw log 回放重建

## 13. 可观测性设计
- 指标：ingest_qps、mq_lag、dedup_rate、late_event_ratio、query_p95、replay_backlog
- 日志：按 trace_id 串联采集、聚合、查询链路
- 追踪：上报端到看板查询全链路 trace
- 看板：实时口径、结算口径、偏差率三套大盘

## 14. 安全与合规
- 传输全程 TLS，敏感字段（IP/user_id）脱敏或哈希化
- 防作弊：设备指纹 + 频控 + 异常模式识别
- 权限：报表 API 做租户隔离与最小权限
- 审计：回放、修复、手工更正均留审计轨迹

## 15. Java 关键代码（>=5段）
```java
public final class IdempotencyGuard {
    private final Set<String> recent = java.util.concurrent.ConcurrentHashMap.newKeySet();

    public boolean firstSeen(String eventId) {
        if (eventId == null || eventId.isBlank()) throw new IllegalArgumentException("eventId");
        return recent.add(eventId);
    }
}
```

```java
public class ClickIngestService {
    private final IdempotencyGuard guard;
    private final MessagePublisher publisher;

    public ClickIngestService(IdempotencyGuard guard, MessagePublisher publisher) {
        this.guard = guard;
        this.publisher = publisher;
    }

    public IngestResult ingest(ClickEvent e) {
        if (!guard.firstSeen(e.eventId())) return IngestResult.duplicate(e.eventId());
        publisher.publish("click-events", e.campaignId(), e);
        return IngestResult.accepted(e.eventId());
    }
}
```

```java
public class MinuteAggregator {
    private final java.util.Map<AggKey, AggValue> state = new java.util.concurrent.ConcurrentHashMap<>();

    public void onEvent(ClickEvent e) {
        AggKey key = new AggKey(e.bucketMinute(), e.adId(), e.campaignId(), e.publisherId());
        state.compute(key, (k, v) -> v == null ? AggValue.from(e) : v.merge(e));
    }
}
```

```java
public class RetryWithBackoff {
    public <T> T run(java.util.concurrent.Callable<T> action, int maxRetry) throws Exception {
        long wait = 50L;
        Exception last = null;
        for (int i = 0; i <= maxRetry; i++) {
            try { return action.call(); } catch (Exception ex) { last = ex; }
            Thread.sleep(wait);
            wait = Math.min(wait * 2, 1000L);
        }
        throw last;
    }
}
```

```java
public class ReplayJobRunner {
    private final RawLogReader reader;
    private final MinuteAggregator aggregator;
    private final AggregationStore store;

    public ReplayReport replay(long fromTs, long toTs) {
        long count = 0L;
        for (ClickEvent e : reader.scan(fromTs, toTs)) {
            aggregator.onEvent(e);
            count++;
        }
        int written = store.flush(aggregator);
        return new ReplayReport(count, written, "DONE");
    }
}
```

## 16. 前端功能代码（React JavaScript，>=2段）
```javascript
import { useEffect, useState } from "react";

export function RealtimePanel({ campaignId }) {
  const [status, setStatus] = useState("loading"); // loading | done | error
  const [error, setError] = useState("");
  const [data, setData] = useState(null);

  async function load() {
    setStatus("loading");
    setError("");
    try {
      const r = await fetch(`/v1/reports/realtime?campaignId=${campaignId}`);
      if (!r.ok) throw new Error(`HTTP ${r.status}`);
      const json = await r.json();
      setData(json);
      setStatus("done");
    } catch (e) {
      setStatus("error");
      setError(e.message || "load failed");
    }
  }

  useEffect(() => {
    load();
    const timer = setInterval(load, 5000); // polling
    return () => clearInterval(timer);
  }, [campaignId]);

  if (status === "loading") return <div>loading...</div>;
  if (status === "error") return <div>error: {error}</div>;
  return <pre>{JSON.stringify(data, null, 2)}</pre>;
}
```

```jsx
import { useState } from "react";

export function ReplayButton({ fromTs, toTs }) {
  const [status, setStatus] = useState("done"); // loading | done | error
  const [error, setError] = useState("");
  const [jobId, setJobId] = useState("");

  async function submitWithRetry() {
    setStatus("loading");
    setError("");
    const idem = `replay-${fromTs}-${toTs}`; // idempotency key

    for (let i = 0; i < 3; i++) {
      try {
        const r = await fetch("/v1/replay-jobs", {
          method: "POST",
          headers: { "Content-Type": "application/json", "Idempotency-Key": idem },
          body: JSON.stringify({ fromTs, toTs })
        });
        if (!r.ok) throw new Error(`HTTP ${r.status}`);
        const json = await r.json();
        setJobId(json.jobId);
        setStatus("done");
        return;
      } catch (e) {
        if (i === 2) {
          setStatus("error");
          setError("submit failed, fallback to offline ticket");
        }
        await new Promise((x) => setTimeout(x, 300 * (i + 1))); // retry + degrade
      }
    }
  }

  return (
    <div>
      <button onClick={submitWithRetry} disabled={status === "loading"}>start replay</button>
      {status === "loading" && <span> loading...</span>}
      {status === "error" && <span> error: {error}</span>}
      {status === "done" && jobId && <span> job: {jobId}</span>}
    </div>
  );
}
```

## 17. 测试策略
- 单测：去重、窗口聚合、迟到数据修正、重试退避
- 集成：MQ 积压恢复、OLAP 写入失败、回放覆盖正确性
- 压测：峰值流量、热点 campaign、跨 AZ 切换
- 演练：每月一次 replay 与灾备切换演练

## 18. 丰富例子（>=10）
1. campaign 维度 1 分钟点击趋势图  
2. ad 维度 UV/点击率对比  
3. publisher 分渠道贡献占比  
4. 地域分桶（省市）点击热力  
5. 设备类型（iOS/Android/Web）点击分布  
6. 小时级成本与点击联动曲线  
7. 作弊流量剔除前后差异  
8. 迟到数据回填前后偏差率  
9. A/B 创意点击提升对比  
10. 活动突发峰值下实时看板稳定性  
11. 回放任务完成后的口径对齐报告  
12. 多租户隔离下的查询权限示例

## 19. 面试高频追问
- 为什么不用强一致数据库直接实时汇总：吞吐与成本不可接受，流式+回放更现实。
- 如何解释“实时值”和“结算值”不一致：口径不同，结算值以离线校准为准。
- 如何处理重复上报：event_id 幂等键 + 时间窗去重双保险。

## 20. 学习与落地顺序
1. 先掌握 Kafka 分区与消费语义。  
2. 再掌握 Flink 窗口、水位线、状态快照。  
3. 然后补 OLAP 明细/聚合模型与查询优化。  
4. 最后做 replay 与容灾演练闭环。

## 21. 评估与打分
- 正确性：30/30
- 可扩展性：24/25
- 稳定性：23/25
- 工程可落地：20/20
- 总分：97/100

## 22. 与母题差异
- 母题：`#36 Design Amazon Recommendation System`
- 共性：都依赖高吞吐日志、在线计算链路、实验评估与效果归因
- 差异1：本题是“计数与口径一致性”，母题偏“召回与排序效果”
- 差异2：本题强调“幂等去重 + 迟到修正 + 回放恢复”
- 差异3：本题对 OLAP 聚合模型和查询时效要求更高
- 差异4：本题要求明确阈值、RTO/RPO、故障恢复路径
- 差异5：本题对防作弊与审计闭环要求更硬
- 新增必补知识1：事件时间/处理时间与 watermark
- 新增必补知识2：流批校准与口径治理
- 新增必补知识3：幂等键设计与去重窗口权衡
- 新增必补知识4：回放作业编排与版本覆盖写入
- 新增必补知识5：查询降级策略与可观测性告警联动
- 新增必补知识6：跨 AZ 容灾切流与数据追平
