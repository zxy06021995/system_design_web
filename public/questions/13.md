# Design Distributed Counter - 分布式计数器（母题）

> 对应题：Q13 `Design Distributed Counter - 分布式计数器`  
> 迁移价值：这题学透后，可覆盖点赞计数、浏览计数、库存余量、限额统计、配额控制、实时指标累计等高频场景。

---

## 1. 先定题目边界

### 1.1 P0 能力
1. 高并发 `increment/decrement/add`。
2. 读取当前计数值。
3. 多节点部署下计数不乱。
4. 支持幂等请求，防重放重复计数。
5. 支持按 key 隔离（如 `post:like:123`）。

### 1.2 P1 能力
1. 批量增量与批量读取。
2. 多维计数（按天、按租户、按业务）。
3. 回补修正（reconcile）。
4. 多机房容灾（跨 Region）。
5. 强一致模式与最终一致模式可切换。

### 1.3 非功能目标（必须量化）
1. 写入峰值：`>= 500,000 ops/s`（集群）。
2. 读延迟：P95 `< 10ms`，P99 `< 30ms`。
3. 可用性：`>= 99.99%`。
4. 数据正确性：
   1. 最终一致模式：秒级收敛。
   2. 强一致模式：线性一致（牺牲吞吐）。

---

## 2. 容量估算

### 2.1 假设
1. 日增量请求：`10,000,000,000`（100 亿）。
2. 平均 payload：`~120B`（key + delta + requestId）。
3. 读写比：`1:3`（写重）。
4. 热 key 比例：前 1% key 承担 60% 流量。

### 2.2 计算
1. 写平均 QPS：`10^10 / 86400 ≈ 115,740`。
2. 写峰值 QPS（x5）：`≈ 578,700`。
3. 若 24 台写节点平均分担，单机峰值约 `24,100` ops/s。
4. 增量日志日写入：`10^10 * 120B ≈ 1.2TB/天`（压缩前）。

### 2.3 结论
1. 单点计数器一定会成为瓶颈。
2. 必须做分片（striped counter）以摊平写热点。
3. 读侧要接受“聚合开销”或“快照延迟”二选一。

---

## 3. 一致性模型（先讲清最关键）

### 3.1 最终一致模式（默认）
1. 写：路由到分片原子累加。
2. 读：聚合各分片值 + 可选快照缓存。
3. 优点：吞吐高、成本低。
4. 缺点：读到的是“近实时”而非严格实时。

### 3.2 强一致模式（小流量关键场景）
1. 用单分区强一致存储（如 Raft group / 单行事务）串行写。
2. 读写都走 leader。
3. 优点：线性一致。
4. 缺点：吞吐低，扩展难。

### 3.3 面试中的标准答法
1. 点赞数、浏览数：最终一致。
2. 库存余量、额度扣减：强一致或“强一致扣减 + 最终一致展示”。

---

## 4. 高层架构

```text
Client/App
   |
   v
API Gateway (auth/rate-limit)
   |
   v
Counter API Service
   |---------------------------|
   v                           v
Redis Counter Cluster      Idempotency Store (Redis/DB)
   |                           |
   v                           v
Counter Delta Stream (Kafka / Redis Stream)
   |
   v
Aggregator / Flusher
   |
   v
MySQL Counter Snapshot Store
   |
   v
Query Cache (optional)
```

设计要点：
1. 写路径完全走内存原子命令（INCRBY）保障吞吐。
2. 通过幂等去重避免重试导致重复累计。
3. 异步落盘形成可恢复快照，降低重启恢复成本。

---

## 5. 数据模型

### 5.1 计数器元数据表

```sql
CREATE TABLE counter_meta (
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  counter_key VARCHAR(256) NOT NULL,
  biz_type VARCHAR(64) NOT NULL,
  consistency_mode TINYINT NOT NULL DEFAULT 0, -- 0=eventual,1=strong
  shard_count INT NOT NULL DEFAULT 32,
  status TINYINT NOT NULL DEFAULT 1,
  created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
  updated_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  UNIQUE KEY uk_counter_key (counter_key)
) ENGINE=InnoDB;
```

### 5.2 分片快照表

```sql
CREATE TABLE counter_shard_snapshot (
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  counter_key VARCHAR(256) NOT NULL,
  shard_id INT NOT NULL,
  value BIGINT NOT NULL,
  version BIGINT NOT NULL DEFAULT 0,
  updated_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  UNIQUE KEY uk_counter_shard (counter_key, shard_id),
  KEY idx_counter_key (counter_key)
) ENGINE=InnoDB;
```

### 5.3 聚合快照表

```sql
CREATE TABLE counter_total_snapshot (
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  counter_key VARCHAR(256) NOT NULL,
  total_value BIGINT NOT NULL,
  watermark_event_id BIGINT NOT NULL,
  updated_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  UNIQUE KEY uk_counter_total (counter_key)
) ENGINE=InnoDB;
```

### 5.4 幂等请求表

```sql
CREATE TABLE counter_idempotency (
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  counter_key VARCHAR(256) NOT NULL,
  request_id VARCHAR(64) NOT NULL,
  delta BIGINT NOT NULL,
  created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
  UNIQUE KEY uk_counter_req (counter_key, request_id),
  KEY idx_created_at (created_at)
) ENGINE=InnoDB;
```

### 5.5 增量日志表（可选，如果不走 MQ）

```sql
CREATE TABLE counter_delta_log (
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  counter_key VARCHAR(256) NOT NULL,
  shard_id INT NOT NULL,
  delta BIGINT NOT NULL,
  request_id VARCHAR(64) NOT NULL,
  event_time DATETIME NOT NULL,
  created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
  KEY idx_counter_time (counter_key, event_time),
  KEY idx_shard_time (counter_key, shard_id, event_time)
) ENGINE=InnoDB;
```

---

## 6. Key 设计与分片策略

### 6.1 Key 规范
推荐格式：

```text
counter:{biz}:{entityId}:{metric}:{date}
```

示例：
1. `counter:post:987:like:2026-02-23`
2. `counter:merchant:1001:quota:2026-02-23`

### 6.2 分片键
1. 物理 key：`{counterKey}:shard:{i}`。
2. 写入路由：
   1. 随机分片（均匀但读聚合重）。
   2. requestId 哈希分片（更稳定）。

### 6.3 分片数怎么定
1. 按热 key 峰值写入倒推。
2. 单分片目标写入不超过 Redis 单 key 安全上限的 60~70%。
3. 常用起步值：`16/32/64`。

---

## 7. API 设计

### 7.1 增量写入

```http
POST /api/v1/counters/{counterKey}/increment
Headers:
  X-Request-Id: req_20260223_13001
Body:
{
  "delta": 1
}
```

返回：

```json
{
  "accepted": true,
  "counterKey": "counter:post:987:like:2026-02-23",
  "mode": "eventual"
}
```

### 7.2 读取计数

```http
GET /api/v1/counters/{counterKey}
```

返回：

```json
{
  "counterKey": "counter:post:987:like:2026-02-23",
  "value": 1234567,
  "stalenessMs": 800
}
```

### 7.3 批量读取

```http
POST /api/v1/counters/batch-get
Body:
{
  "keys": ["counter:post:1:like:2026-02-23","counter:post:2:like:2026-02-23"]
}
```

### 7.4 调账（仅运营/系统内部）

```http
POST /api/v1/counters/{counterKey}/adjust
Body:
{
  "delta": -120,
  "reason": "reconcile-fix"
}
```

---

## 8. 核心链路一：写入（最终一致模式）

1. 网关鉴权、限流、trace 注入。
2. 检查 `requestId` 幂等：
   1. 若已处理，直接返回 accepted。
   2. 若未处理，继续。
3. 根据 `requestId`/随机选择 shard。
4. 对分片 key 执行 `INCRBY delta`（原子）。
5. 记录幂等标记（TTL 或持久化）。
6. 异步投递 delta 事件到流系统用于落盘和校验。

关键点：
1. 幂等检查与 `INCRBY` 最好放到同一个 Lua 脚本里，避免竞态。
2. `delta` 可正可负时要做下限保护（例如不允许小于 0）。

---

## 9. 核心链路二：读取（聚合 + 快照）

### 9.1 直接聚合
1. 并发读取所有 shard 值。
2. 本地求和返回。
3. 适合低频查询，简单直接。

### 9.2 快照优先
1. 先读 `counter_total_snapshot`。
2. 需要时叠加最近增量（watermark 后）。
3. 适合高频查询，延迟更稳定。

### 9.3 读写取舍
1. 写快：分片多，读慢（聚合成本高）。
2. 读快：预聚合多，写链路更复杂。

---

## 10. 核心链路三：异步落盘与恢复

1. 增量事件进入 Kafka（或 Redis Stream）。
2. Flusher 消费并更新 `counter_shard_snapshot`。
3. 周期性汇总到 `counter_total_snapshot`。
4. 服务重启时：
   1. 从快照加载。
   2. 从 watermark 后增量重放。

目标：
1. 保证“可恢复”。
2. 保证“可审计”。
3. 为跨系统对账提供基础数据。

---

## 11. Java 关键代码（可直接讲）

### 11.1 分片路由器

```java
public class CounterShardRouter {
    public int chooseShard(String counterKey, String requestId, int shardCount) {
        if (shardCount <= 0) {
            throw new IllegalArgumentException("invalid shardCount");
        }
        int h = (counterKey + "#" + requestId).hashCode();
        return (h & 0x7fffffff) % shardCount;
    }
}
```

### 11.2 Lua 原子幂等增量

```java
@Service
public class CounterWriteService {
    private final StringRedisTemplate redisTemplate;
    private final CounterShardRouter shardRouter = new CounterShardRouter();

    // KEYS[1] = idemKey
    // KEYS[2] = shardCounterKey
    // ARGV[1] = idemTtlSec
    // ARGV[2] = delta
    // 返回 1 表示本次生效，0 表示重复请求
    private static final String LUA = """
        if redis.call('SETNX', KEYS[1], '1') == 1 then
          redis.call('EXPIRE', KEYS[1], ARGV[1])
          redis.call('INCRBY', KEYS[2], ARGV[2])
          return 1
        end
        return 0
        """;

    public CounterWriteService(StringRedisTemplate redisTemplate) {
        this.redisTemplate = redisTemplate;
    }

    public boolean increment(String counterKey, String requestId, long delta, int shardCount) {
        int shard = shardRouter.chooseShard(counterKey, requestId, shardCount);
        String idemKey = "idem:counter:" + counterKey + ":" + requestId;
        String shardKey = "counter:" + counterKey + ":shard:" + shard;

        DefaultRedisScript<Long> script = new DefaultRedisScript<>();
        script.setScriptText(LUA);
        script.setResultType(Long.class);

        Long applied = redisTemplate.execute(
                script,
                Arrays.asList(idemKey, shardKey),
                "86400",
                String.valueOf(delta)
        );
        return applied != null && applied == 1L;
    }
}
```

### 11.3 分片聚合读取（Pipeline）

```java
@Service
public class CounterReadService {
    private final StringRedisTemplate redisTemplate;

    public CounterReadService(StringRedisTemplate redisTemplate) {
        this.redisTemplate = redisTemplate;
    }

    public long getCounterValue(String counterKey, int shardCount) {
        List<Object> values = redisTemplate.executePipelined((RedisCallback<Object>) connection -> {
            for (int i = 0; i < shardCount; i++) {
                String key = "counter:" + counterKey + ":shard:" + i;
                connection.stringCommands().get(key.getBytes(StandardCharsets.UTF_8));
            }
            return null;
        });

        long total = 0L;
        for (Object v : values) {
            if (v != null) {
                total += Long.parseLong(v.toString());
            }
        }
        return total;
    }
}
```

### 11.4 快照落盘消费者

```java
@Service
public class CounterSnapshotFlusher {
    private final CounterSnapshotRepository snapshotRepository;
    private final CounterTotalRepository totalRepository;

    public CounterSnapshotFlusher(CounterSnapshotRepository snapshotRepository,
                                  CounterTotalRepository totalRepository) {
        this.snapshotRepository = snapshotRepository;
        this.totalRepository = totalRepository;
    }

    @KafkaListener(topics = "counter-delta", groupId = "counter-flusher")
    @Transactional
    public void onDelta(CounterDeltaEvent event) {
        snapshotRepository.upsertShardValue(
                event.counterKey(),
                event.shardId(),
                event.newShardValue()
        );
        totalRepository.markNeedRebuild(event.counterKey(), event.eventId());
    }
}
```

### 11.5 定时聚合任务

```java
@Component
public class CounterAggregateJob {
    private final CounterSnapshotRepository snapshotRepository;
    private final CounterTotalRepository totalRepository;

    public CounterAggregateJob(CounterSnapshotRepository snapshotRepository,
                               CounterTotalRepository totalRepository) {
        this.snapshotRepository = snapshotRepository;
        this.totalRepository = totalRepository;
    }

    @Scheduled(fixedDelay = 2000)
    @Transactional
    public void aggregateDirtyCounters() {
        List<DirtyCounter> dirty = totalRepository.pickDirtyCounters(500);
        for (DirtyCounter dc : dirty) {
            long sum = snapshotRepository.sumShards(dc.counterKey());
            totalRepository.upsertTotal(dc.counterKey(), sum, dc.maxEventId());
        }
    }
}
```

### 11.6 强一致计数器（低吞吐关键场景示例）

```java
@Service
public class StrongCounterService {
    private final JdbcTemplate jdbcTemplate;

    public StrongCounterService(JdbcTemplate jdbcTemplate) {
        this.jdbcTemplate = jdbcTemplate;
    }

    @Transactional
    public long incrementStrong(String counterKey, long delta) {
        // 依赖数据库行锁，保证线性化顺序
        Long value = jdbcTemplate.queryForObject(
                "SELECT total_value FROM counter_total_snapshot WHERE counter_key=? FOR UPDATE",
                Long.class,
                counterKey
        );
        long next = (value == null ? 0L : value) + delta;
        if (next < 0) {
            throw new IllegalStateException("counter below zero");
        }
        jdbcTemplate.update(
                "INSERT INTO counter_total_snapshot(counter_key,total_value,watermark_event_id) VALUES(?,?,0) " +
                        "ON DUPLICATE KEY UPDATE total_value=VALUES(total_value)",
                counterKey,
                next
        );
        return next;
    }
}
```

---

## 12. CRDT 计数器（跨机房最终一致）

### 12.1 G-Counter
1. 每节点只增不减。
2. merge 时取每节点最大值。
3. 总值为所有节点分量求和。

### 12.2 PN-Counter
1. 两个 G-Counter：P（增）与 N（减）。
2. 值 = `sum(P) - sum(N)`。
3. 适合跨 Region 双活场景。

### 12.3 取舍
1. 优点：无中心、天然可 merge。
2. 缺点：状态膨胀、强实时一致性弱。

---

## 13. 热点 key 治理

1. 分片扩容：
   1. 从 16 扩到 64 分片。
   2. 通过版本化 key 平滑切换（`v1/v2`）。
2. 写侧打散：
   1. requestId 哈希路由。
   2. 客户端批量合并再上报。
3. 读侧缓存：
   1. 短 TTL（如 100~500ms）。
   2. 热 key 主动刷新。
4. 限流与降级：
   1. 超热点 key 可返回近似值或快照值。
   2. 保护集群稳定优先。

---

## 14. 负值与配额边界

### 14.1 不允许小于 0 的计数器
1. 库存、额度场景必须做下限保护。
2. 单纯 `INCRBY -1` 不够，需要原子检查+扣减。

Lua 示例思路：
1. 先取当前值。
2. 校验 `current + delta >= 0`。
3. 通过才写入，否则返回失败。

### 14.2 软配额与硬配额
1. 软配额：超限可告警但不拦截。
2. 硬配额：超限直接拒绝。
3. 配额计数通常要求更强一致。

---

## 15. 可观测性与告警

### 15.1 核心指标
1. increment QPS、read QPS。
2. 写入成功率、幂等命中率。
3. 热 key TopN 及倾斜度（skew）。
4. snapshot 延迟、watermark 延迟。
5. 快照值与在线聚合值差异率（对账指标）。
6. Redis CPU、内存、慢查询。

### 15.2 告警阈值示例
1. 写失败率 > 0.5% 持续 5 分钟告警。
2. snapshot 延迟 > 10 秒告警。
3. 对账差异 > 0.1% 告警。
4. 单 key 占比 > 20% 告警（热点风险）。

---

## 16. 故障演练

### 演练1：Redis 节点故障
1. 现象：部分分片不可写。
2. 处理：自动故障转移 + 熔断降级 + 重试预算控制。
3. 验证：可用性与数据差异窗口是否可接受。

### 演练2：消费者积压
1. 现象：delta 日志堆积，快照延迟变大。
2. 处理：扩容 flusher + 提升批处理大小 + 降低非关键聚合频率。
3. 验证：watermark 延迟恢复目标。

### 演练3：幂等存储失效
1. 现象：客户端重试导致重复计数。
2. 处理：快速切换备用幂等存储，开启对账修正任务。
3. 验证：重复增量比例回落。

### 演练4：超热点 key 打爆单分片
1. 现象：某 shard 延迟飙升。
2. 处理：扩容分片并切换路由版本，短期读走快照。
3. 验证：P99 恢复与错读窗口可控。

---

## 17. 与相近题的边界

1. 与 `Rate Limiter`：
   1. 限流需要窗口语义与实时判断。
   2. 计数器是基础能力，不等于完整限流系统。

2. 与 `Distributed Lock`：
   1. 锁强调互斥与所有权。
   2. 计数器强调累加正确性与吞吐。

3. 与 `Metrics Monitoring`：
   1. 指标系统会再加维度标签、聚合函数、查询语义。
   2. 计数器是指标系统里的一个 primitive。

4. 与 `Message Queue`：
   1. MQ 解决异步传输与回放。
   2. 计数器题更关注状态累计与一致性边界。

5. 与 `库存扣减`：
   1. 库存扣减常要求强一致和不为负。
   2. 通用计数器可采用最终一致。

---

## 18. 面试高频追问

1. 为什么要分片计数？
   1. 单 key 热点会成为瓶颈。
   2. 分片把写压力均摊到多个 key。

2. 分片后读取会变慢，怎么平衡？
   1. 高频读走聚合快照。
   2. 低频读可直接聚合分片值。

3. 如何保证重试不重复加？
   1. requestId 幂等。
   2. 幂等检查与增量写入放同一原子操作。

4. 什么时候必须强一致？
   1. 库存、额度等不可超卖场景。
   2. 展示型计数通常不必强一致。

5. 跨机房怎么做？
   1. CRDT（G/PN counter）最终一致。
   2. 或单 Region 主写 + 只读副本。

6. 如何做修正？
   1. 保留增量日志。
   2. 周期性对账并写 adjust 事件回补。

---

## 19. 上场前 Checklist

1. 是否说明一致性级别与业务场景匹配。
2. 是否说明分片策略和读写权衡。
3. 是否给出幂等与防重设计。
4. 是否覆盖了快照与恢复流程。
5. 是否给出热点 key 治理方案。
6. 是否定义了关键监控指标与告警阈值。
7. 是否准备了 Redis 故障、积压、重复计数的演练方案。

---

## 20. 30秒总结

分布式计数器的核心不是“加一”，而是“在高并发下可扩展地加一且结果可解释”。  
主线答案要讲清三点：  
1. 写快：分片 + 原子增量。  
2. 不乱：幂等 + 状态边界。  
3. 可恢复：增量日志 + 快照 + 对账修正。  

