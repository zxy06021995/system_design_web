# Q103：Design Message Queue System

## 1. 题目元信息（按源文件读取）
- title：Design Message Queue System
- tags：消息队列、Kafka、分区、消费者组、持久化
- keyPoints：Topic分区、消费者组、Offset管理、消息持久化、高可用设计
- learningCoreId：9（母题：Design Message Queue - 消息队列）
- 总分：97/100

## 2. 题目目标与边界
- 目标：设计一个高吞吐、可扩展、可恢复的消息队列系统。
- 范围：发布、消费、重试、死信、回放、监控告警、权限审计。
- 非目标：跨地域强一致事务；超长事务编排引擎。

## 3. 需求澄清
- 功能：Topic/Partition 管理、Producer 发布、Consumer Group 消费、Offset 提交、重试与 DLQ。
- 非功能：可用性 >= 99.95%，P99 发布延迟 < 30ms，顺序性按分区保证。
- 运维：支持灰度、限流、扩容、故障演练、回放修复。

## 4. 容量估算
- 峰值写入：200 万 msg/s，平均消息 1KB，入口带宽约 2GB/s。
- 保留 3 天：200万 * 1024 * 86400 * 3 ≈ 531TB（未压缩）。
- 压缩率按 0.4 估算，落盘约 212TB；副本 3 份约 636TB。
- 消费组 200 个，滞后监控粒度到 partition。

## 5. 核心架构
- 接入层：Gateway + Auth + Quota。
- 存储层：Broker 集群（分区副本、ISR、顺序追加写）。
- 协调层：元数据服务（Topic、Partition、Broker、Leader）。
- 消费层：Consumer Group、Rebalance、Offset Store。
- 治理层：重试队列、DLQ、回放任务、审计与告警。

## 6. 数据模型
- topic(topic_id, name, partitions, replication_factor, retention_hours)
- partition(topic_id, partition_id, leader_broker, isr_brokers)
- message(topic_id, partition_id, offset, key, payload, ts)
- consumer_offset(group_id, topic_id, partition_id, committed_offset, update_ts)
- dlq(topic_id, partition_id, origin_offset, reason, payload, retry_count)

## 7. API 设计
- `POST /api/topics`：创建 Topic。
- `POST /api/messages:publish`：批量发布消息（支持幂等键）。
- `POST /api/offsets:commit`：提交消费位点。
- `POST /api/messages:replay`：按 topic/partition/offset 范围回放。
- `GET /api/groups/{groupId}/lag`：查询消费滞后。

## 8. 写入流程
1. Producer 带 `idempotencyKey` 发送到 Gateway。
2. Gateway 做鉴权、配额、限流并路由到 Leader Broker。
3. Leader 追加写日志并复制到 ISR。
4. 满足 `acks=all` 后返回成功；失败按可重试错误码返回。
5. 指标上报：写入吞吐、延迟、失败率、复制延迟。

## 9. 消费流程
1. Consumer Group 分配分区。
2. Consumer 拉取消息并执行业务处理。
3. 处理成功后提交 offset；失败进入重试策略。
4. 超过阈值写入 DLQ，避免阻塞主消费。
5. 回放工具按区间重放并审计记录。

## 10. 一致性与语义
- 默认 At-least-once：依赖消费端幂等。
- 可选 Exactly-once（高成本）：事务写 + 事务提交 offset。
- 顺序语义：仅保证同一 partition 内有序。
- 幂等实现：`idempotencyKey + producerId + sequence`。

## 11. 阈值体系
- 发布失败率阈值：`> 0.5%` 持续 5 分钟触发 P1。
- 复制延迟阈值：`> 3s` 持续 3 分钟触发 P1。
- Consumer Lag 阈值：单分区 `> 500000` 触发 P1。
- Rebalance 频率阈值：`> 6 次/10 分钟` 触发 P2。
- DLQ 增速阈值：`> 2000 条/分钟` 触发 P1。

## 12. 故障恢复路径（含 RTO/RPO）
- 场景 A：单 Broker 故障
  - 路径：Leader 迁移 -> ISR 收敛 -> 客户端元数据刷新 -> 恢复写入。
  - 目标：RTO <= 2 分钟，RPO = 0（acks=all + minISR 满足时）。
- 场景 B：机房级故障（同城双活）
  - 路径：流量切换 -> 只读降级 -> 异步追平 -> 恢复双写。
  - 目标：RTO <= 15 分钟，RPO <= 30 秒（跨机房异步复制）。
- 场景 C：消费端程序 Bug
  - 路径：暂停消费组 -> 回滚版本 -> 依据审计区间回放。
  - 目标：RTO <= 10 分钟，RPO <= 1 分钟（依赖 offset 快照频率）。

## 13. 高可用与容灾设计
- 多副本 + ISR + 自动选主。
- 元数据服务多节点仲裁，防止单点。
- 读写隔离与分区重分配，平衡热点。
- 同城双活，跨城灾备，定期演练切换。

## 14. 安全与合规
- ACL：按 topic/group 维度做最小权限。
- 传输加密：TLS；存储加密：磁盘或列级密钥。
- 审计：Topic 变更、权限变更、回放操作全留痕。
- 数据保留与删除策略符合合规要求。

## 15. Java 关键代码（>=5）
```java
public final class IdempotencyStore {
    private final ConcurrentHashMap<String, Long> processed = new ConcurrentHashMap<>();

    public boolean firstSeen(String key, long offset) {
        return processed.putIfAbsent(key, offset) == null;
    }

    public long getOffset(String key) {
        Long v = processed.get(key);
        if (v == null) {
            throw new IllegalArgumentException("unknown idempotency key");
        }
        return v;
    }
}
```

```java
public class PublishService {
    public PublishResult publish(Broker leader, Message msg) {
        if (msg.getPayload().length == 0) {
            return PublishResult.fail("empty payload");
        }
        long offset = leader.append(msg);
        boolean replicated = leader.waitForAcks(offset, 2_000);
        if (!replicated) {
            return PublishResult.fail("replication timeout");
        }
        return PublishResult.ok(offset);
    }
}
```

```java
public class RetryPolicy {
    public Duration nextDelay(int attempt) {
        int capped = Math.min(attempt, 8);
        long baseMs = (long) Math.pow(2, capped) * 100L;
        long jitter = ThreadLocalRandom.current().nextLong(0, 100);
        return Duration.ofMillis(baseMs + jitter);
    }

    public boolean shouldToDlq(int attempt, String errorCode) {
        return attempt >= 8 || "NON_RETRYABLE".equals(errorCode);
    }
}
```

```java
public class OffsetCommitService {
    private final Map<String, Long> committed = new ConcurrentHashMap<>();

    public CommitResult commit(String groupPartition, long newOffset, long processedOffset) {
        if (newOffset < processedOffset) {
            return CommitResult.fail("offset rollback detected");
        }
        committed.put(groupPartition, newOffset);
        return CommitResult.ok(newOffset);
    }
}
```

```java
public class RecoveryRunbookExecutor {
    public RecoveryResult executeBrokerFailure(String brokerId) {
        boolean fenced = fenceBroker(brokerId);
        boolean migrated = migrateLeaders(brokerId);
        boolean refreshed = refreshClientMetadata();
        if (fenced && migrated && refreshed) {
            return RecoveryResult.success("broker failure recovered");
        }
        return RecoveryResult.partial("manual intervention required");
    }

    private boolean fenceBroker(String brokerId) { return brokerId != null && !brokerId.isEmpty(); }
    private boolean migrateLeaders(String brokerId) { return brokerId != null && !brokerId.isEmpty(); }
    private boolean refreshClientMetadata() { return true; }
}
```

## 16. React JavaScript 前端代码（>=2）
```jsx
import React, { useEffect, useRef, useState } from "react";

export function LagPanel() {
  const [status, setStatus] = useState("loading"); // loading | done | error
  const [data, setData] = useState(null);
  const [err, setErr] = useState("");
  const timerRef = useRef(null);

  async function fetchLag() {
    setStatus("loading");
    setErr("");
    try {
      const res = await fetch("/api/groups/core-group/lag");
      if (!res.ok) {
        setStatus("error");
        setErr("lag api failed");
        return;
      }
      const body = await res.json();
      setData(body);
      setStatus("done");
    } catch (e) {
      setStatus("error");
      setErr("network error");
    }
  }

  useEffect(() => {
    fetchLag();
    timerRef.current = setInterval(fetchLag, 5000); // polling
    return () => clearInterval(timerRef.current);
  }, []);

  return (
    <section>
      <h3>Consumer Lag</h3>
      {status === "loading" && <p>loading...</p>}
      {status === "error" && <p>error: {err}</p>}
      {status === "done" && <pre>{JSON.stringify(data, null, 2)}</pre>}
    </section>
  );
}
```

```javascript
import React, { useState } from "react";

export function ReplayButton() {
  const [status, setStatus] = useState("done"); // loading | done | error
  const [message, setMessage] = useState("");

  async function triggerReplay() {
    const idempotencyKey = "replay-20260224-topicA-p0-100-1000"; // idempotent key
    setStatus("loading");
    setMessage("");
    for (let attempt = 1; attempt <= 3; attempt += 1) { // retry
      try {
        const res = await fetch("/api/messages:replay", {
          method: "POST",
          headers: { "Content-Type": "application/json", "Idempotency-Key": idempotencyKey },
          body: JSON.stringify({ topic: "topicA", partition: 0, from: 100, to: 1000 })
        });
        if (res.ok) {
          setStatus("done");
          setMessage("replay accepted");
          return;
        }
      } catch (e) {
        // continue retry
      }
      await new Promise((r) => setTimeout(r, 500 * attempt));
    }
    setStatus("error");
    setMessage("replay failed after retries, fallback to manual runbook"); // degrade
  }

  return (
    <div>
      <button onClick={triggerReplay}>Trigger Replay</button>
      {status === "loading" && <p>loading...</p>}
      {status === "error" && <p>error: {message}</p>}
      {status === "done" && message && <p>{message}</p>}
    </div>
  );
}
```

## 17. 测试与演练
- 单元测试：分区路由、幂等、重试、DLQ、offset 边界。
- 集成测试：发布到消费端到端、rebalance、回放链路。
- 压测：峰值吞吐、抖动注入、磁盘慢盘、网络分区。
- 故障演练：Broker 宕机、元数据服务主切、误提交 offset 回滚。

## 18. 丰富例子（>=10）
1. 电商下单事件进入 Topic `order-created`，按 `userId` 分区保证同用户顺序。
2. 支付结果事件消费失败 5 次后进 DLQ，人工修复后回放。
3. 营销系统突发流量，触发 producer 限流，核心交易 Topic 不被挤压。
4. 某分区 leader 故障，2 分钟内自动迁移并恢复写入。
5. Consumer 升级导致反序列化异常，暂停消费组并快速回滚。
6. 数据分析组 lag 持续上升，扩容消费者并重平衡恢复。
7. 跨机房链路抖动，切只读降级，待复制追平后恢复双活。
8. 审计要求追踪某条消息全链路，按 `traceId` 定位发布与消费日志。
9. 重试风暴出现，指数退避与 jitter 将失败流量平滑。
10. 业务误删 offset，依据快照与回放窗口恢复消费进度。
11. 某租户超配额，网关拒绝并告警，不影响其他租户。
12. Topic 存储超预算，按保留策略缩短 retention 并归档冷数据。

## 19. 监控面板建议
- 吞吐：produce/consume QPS、字节流量、分区热点。
- 时延：发布 P95/P99、端到端消费延迟。
- 正确性：重复消费率、DLQ 比例、回放成功率。
- 稳定性：ISR 收缩次数、leader 切换次数、rebalance 次数。

## 20. 常见追问与答法
- 为什么不用全局顺序？答：吞吐和可扩展性成本过高，通常仅需分区内顺序。
- 怎么防重复消费？答：消费端幂等键 + 去重存储 + 可重放审计。
- 怎么保证恢复可控？答：阈值触发 + 标准 Runbook + RTO/RPO 目标化管理。

## 21. 落地清单
- 先交付：基础发布消费、offset、监控告警。
- 二期交付：重试/DLQ/回放、审计、自动扩缩容。
- 上线门槛：压测达标、演练通过、值班手册完备、SLO 看板就绪。

## 22. 与母题差异
- 母题（Q9）偏“通用消息队列设计框架”；本题（Q103）强调“系统化落地与运维闭环”。
- 本题更聚焦以下新增必补知识：
  1. Topic 分区热点治理与重分配策略。
  2. Consumer Group Rebalance 抖动抑制与分配器选择。
  3. Offset 管理审计、误提交回滚与快照恢复。
  4. RTO/RPO 目标化设计与按场景恢复路径。
  5. 幂等键贯穿发布、回放、重试的端到端治理。
  6. 阈值驱动告警分级与自动化 Runbook 执行。
