# Design Twitter Timeline & Search - 推特时间线与搜索（母题）

> 对应题：Q2 `Twitter Timeline - 推特时间线与搜索`  
> 迁移价值：这道题吃透后，可覆盖 News Feed、社交动态流、实时分发、排序、部分推荐与检索题型。

---

## 1. 题目边界先定死

### 1.1 P0 功能（必须）
1. 用户发帖（文本/媒体引用）。
2. 用户关注/取关。
3. 用户查看自己的 Home Timeline（关注的人发的内容）。
4. 支持分页与下拉刷新。
5. 基础互动计数（点赞数、评论数）可延迟更新。

### 1.2 P1 功能（加分）
1. 大V策略（千万粉丝不能全量 push）。
2. 简单排序（时间+互动热度混排）。
3. 搜索入口（按关键词检索公开帖子）。
4. 黑名单、屏蔽词、内容审核状态。

### 1.3 非功能目标（面试要量化）
1. 可用性：`>= 99.99%`。
2. 发帖接口：P95 `< 150ms`。
3. 拉取 Timeline：P95 `< 200ms`，P99 `< 400ms`。
4. 数据一致性：Feed 允许最终一致，帖子主数据强一致。
5. 扩展性：支持 10 倍增长，不改核心架构。

---

## 2. 容量估算（必须算）

### 2.1 假设
1. DAU：`10,000,000`。
2. 人均发帖：`2/天`。
3. 人均刷 Timeline：`20/天`。
4. 平均关注数：`300`。
5. 峰值系数：`5`。

### 2.2 估算结果
1. 日发帖量：`20,000,000`。
2. 发帖平均 QPS：`20,000,000 / 86400 ≈ 231`。
3. 发帖峰值 QPS：`≈ 1155`。
4. Timeline 读平均 QPS：`200,000,000 / 86400 ≈ 2315`。
5. Timeline 读峰值 QPS：`≈ 11,575`（按 12k 设计）。

### 2.3 关键结论
1. 读远高于写，是典型读多写少系统。
2. 真正难点是“写放大”：一个帖子可能要分发给海量粉丝。
3. 必须做 `Hybrid Fan-out`（普通用户 push，大V pull）。

---

## 3. 架构总览（主链路 + 检索链路）

```text
                 +----------------------------+
                 |        Mobile/Web          |
                 +-------------+--------------+
                               |
                               v
                    +----------+-----------+
                    | API Gateway / Auth  |
                    +----+------------+---+
                         |            |
                         v            v
                +--------+---+   +----+----------------+
                | Post Service|   | Timeline Service   |
                +-----+-------+   +----+---------------+
                      |                |
                      v                v
              +-------+------+   +-----+---------------------+
              | MySQL(PostDB)|   | Redis Timeline / Cache    |
              +-------+------+   +-----+---------------------+
                      |                |
                      v                v
                +-----+----------------+----+
                | Kafka: post-created topic |
                +-----+----------------+----+
                      |                |
                      v                v
              +-------+------+   +-----+---------------------+
              | Fanout Worker |   | Enrichment / Counter Svc |
              +-------+------+   +-----+---------------------+
                      |
                      v
              +-------+-----------------+
              | Relationship Service DB |
              +-------------------------+

Search Path:
Post Service -> Kafka -> Indexer -> OpenSearch/ES -> Search API
```

---

## 4. 核心设计决策

### 4.1 Fan-out 三种模式对比
1. Push（写扩散）：
   1. 优点：读快，拉 Feed 成本低。
   2. 缺点：发帖时写放大明显。
2. Pull（读聚合）：
   1. 优点：发帖轻量。
   2. 缺点：读时要合并大量作者，读延迟高。
3. Hybrid（推荐）：
   1. 普通用户 push 给粉丝。
   2. 大V不 push，读时按需 pull 最近帖子并合并。

### 4.2 大V阈值策略
1. 粉丝数 `< 100k`：push。
2. 粉丝数 `>= 100k`：pull。
3. 阈值不是写死，按写扩散成本动态调整（可按小时热更新配置）。

### 4.3 一致性模型
1. 发帖成功后，帖子主表已持久化。
2. Feed 分发异步进行，允许短暂不可见（最终一致）。
3. 对用户承诺“几秒内可见”，不是强实时全局一致。

---

## 5. 数据模型（最小必需）

### 5.1 帖子主表

```sql
CREATE TABLE post (
  post_id BIGINT PRIMARY KEY,
  author_id BIGINT NOT NULL,
  content VARCHAR(1000) NOT NULL,
  media_json JSON NULL,
  visibility TINYINT NOT NULL DEFAULT 1,  -- 1=public,2=follower-only,3=private
  review_status TINYINT NOT NULL DEFAULT 1, -- 1=pass,2=pending,3=blocked
  created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
  updated_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  INDEX idx_author_time (author_id, created_at),
  INDEX idx_created_time (created_at)
) ENGINE=InnoDB;
```

### 5.2 关注关系表

```sql
CREATE TABLE user_follow (
  user_id BIGINT NOT NULL,
  followee_id BIGINT NOT NULL,
  created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
  PRIMARY KEY (user_id, followee_id),
  INDEX idx_followee (followee_id)
) ENGINE=InnoDB;
```

### 5.3 粉丝计数表（可缓存）

```sql
CREATE TABLE user_counter (
  user_id BIGINT PRIMARY KEY,
  follower_count BIGINT NOT NULL DEFAULT 0,
  following_count BIGINT NOT NULL DEFAULT 0,
  post_count BIGINT NOT NULL DEFAULT 0,
  updated_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
) ENGINE=InnoDB;
```

### 5.4 Timeline 缓存结构（Redis）

```text
Key: timeline:{userId}
Type: ZSET
Score: publishTimeMillis
Value: postId
Retention: 保留最近 1000~2000 条
```

### 5.5 幂等记录表（发帖防重）

```sql
CREATE TABLE request_idempotency (
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  user_id BIGINT NOT NULL,
  request_id VARCHAR(64) NOT NULL,
  response_json JSON NOT NULL,
  created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
  UNIQUE KEY uk_user_req (user_id, request_id)
) ENGINE=InnoDB;
```

### 5.6 Outbox 表（主数据与消息一致）

```sql
CREATE TABLE outbox_event (
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  aggregate_type VARCHAR(32) NOT NULL,
  aggregate_id BIGINT NOT NULL,
  event_type VARCHAR(64) NOT NULL,
  payload JSON NOT NULL,
  status TINYINT NOT NULL DEFAULT 0, -- 0=new,1=sent,2=failed
  retry_count INT NOT NULL DEFAULT 0,
  created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
  updated_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  INDEX idx_status_time (status, created_at)
) ENGINE=InnoDB;
```

---

## 6. API 设计（面试可直接照讲）

### 6.1 发帖

```http
POST /api/v1/posts
Headers:
  Authorization: Bearer xxx
  X-Request-Id: req_20260223_2001
Body:
{
  "content": "hello timeline",
  "mediaUrls": ["https://cdn.example.com/a.jpg"],
  "visibility": "PUBLIC"
}
```

返回：

```json
{
  "postId": "9452389472394",
  "authorId": "10001",
  "createdAt": "2026-02-23T10:00:00Z"
}
```

### 6.2 拉 Timeline

```http
GET /api/v1/timeline?cursor=1740245010000_9452389472394&limit=20
```

返回：

```json
{
  "items": [
    {
      "postId": "9452389472394",
      "authorId": "10001",
      "content": "hello timeline",
      "likeCount": 12,
      "commentCount": 3,
      "createdAt": "2026-02-23T10:00:00Z"
    }
  ],
  "nextCursor": "1740244000000_9452389400000",
  "hasMore": true
}
```

### 6.3 关注/取关

```http
POST /api/v1/users/{followeeId}/follow
DELETE /api/v1/users/{followeeId}/follow
```

### 6.4 搜索帖子

```http
GET /api/v1/search/posts?q=java+system+design&from=2026-02-01&to=2026-02-23&page=1&pageSize=20
```

---

## 7. 核心流程一：发帖（写链路）

1. 网关完成鉴权、限流、trace 注入。
2. Post Service 检查 `userId + requestId` 幂等。
3. 事务写入 `post` 主表。
4. 同事务写 `outbox_event(post_created)`。
5. 事务提交后由 Outbox Relay 投递到 Kafka。
6. Fanout Worker 消费事件，按策略分发到粉丝 Timeline。
7. 搜索 Indexer 同步消费并更新检索索引。

为什么要 Outbox：
1. 避免“帖子写成功但消息没发出去”。
2. 通过重试与死信队列可补偿。

---

## 8. 核心流程二：拉 Timeline（读链路）

1. 读取 `timeline:{userId}` 的 ZSET，按游标分页取 N 条。
2. 批量回源帖子详情缓存（`post:{postId}`）。
3. 对用户关注的大V，拉取其最近 M 条并合并。
4. 去重后做轻量排序（时间优先 + 热度微调）。
5. 过滤不可见内容（隐私、审核、拉黑）。
6. 返回结果和 `nextCursor`。

性能关键点：
1. 所有查询必须批量化，避免 N+1。
2. 合并大V帖子时限制每个作者取样条数，防止爆炸。
3. 游标分页优先于 offset 分页。

---

## 9. 排序策略（先能跑，再迭代）

### 9.1 第一阶段（规则排序）
1. 主排序：`createdAt` 降序。
2. 次排序：互动分（点赞、评论、转发）加权。
3. 降权：低质内容、重复内容。

示例分数（简化）：

```text
score = 0.75 * recencyScore + 0.25 * engagementScore - penalty
```

### 9.2 第二阶段（模型排序）
1. 加入用户画像和行为特征。
2. 采用二阶段：粗排（召回1000）+ 精排（前100）。
3. 线上 A/B 验证 CTR、停留时长、负反馈率。

---

## 10. 搜索子系统（题目带 Search，需要覆盖）

### 10.1 写入链路
1. Post 写入后发 `post_created` 事件。
2. Indexer 消费事件，抽取字段（author、content、hashtags、语言）。
3. 写 OpenSearch/ES 倒排索引。

### 10.2 查询链路
1. Search API 收到 query。
2. 执行召回（关键词/标签/作者）。
3. 过滤不可见与违规帖子。
4. 按时间与相关性融合排序返回。

### 10.3 一致性说明
1. 搜索是近实时，不要求与主库强一致。
2. 一般秒级延迟可接受。

---

## 11. Java 关键代码（可讲可写）

### 11.1 发帖服务（幂等 + Outbox）

```java
@Service
public class PostService {
    private final PostRepository postRepository;
    private final IdempotencyRepository idempotencyRepository;
    private final OutboxRepository outboxRepository;
    private final IdGenerator idGenerator;

    public PostService(PostRepository postRepository,
                       IdempotencyRepository idempotencyRepository,
                       OutboxRepository outboxRepository,
                       IdGenerator idGenerator) {
        this.postRepository = postRepository;
        this.idempotencyRepository = idempotencyRepository;
        this.outboxRepository = outboxRepository;
        this.idGenerator = idGenerator;
    }

    @Transactional
    public CreatePostResponse createPost(long userId, String requestId, CreatePostRequest req) {
        CreatePostResponse replay = idempotencyRepository.findResponse(userId, requestId);
        if (replay != null) {
            return replay;
        }

        long postId = idGenerator.nextId();
        PostEntity post = new PostEntity(
                postId,
                userId,
                req.content(),
                req.mediaUrlsJson(),
                Visibility.PUBLIC,
                ReviewStatus.PASS
        );
        postRepository.insert(post);

        OutboxEvent event = OutboxEvent.postCreated(postId, userId, post.getCreatedAt());
        outboxRepository.insert(event);

        CreatePostResponse response = new CreatePostResponse(postId, userId, post.getCreatedAt());
        idempotencyRepository.save(userId, requestId, response);
        return response;
    }
}
```

### 11.2 Outbox Relay（可靠投递）

```java
@Component
public class OutboxRelayJob {
    private final OutboxRepository outboxRepository;
    private final KafkaTemplate<String, String> kafkaTemplate;
    private final ObjectMapper objectMapper;

    public OutboxRelayJob(OutboxRepository outboxRepository,
                          KafkaTemplate<String, String> kafkaTemplate,
                          ObjectMapper objectMapper) {
        this.outboxRepository = outboxRepository;
        this.kafkaTemplate = kafkaTemplate;
        this.objectMapper = objectMapper;
    }

    @Scheduled(fixedDelay = 200)
    public void relay() {
        List<OutboxEvent> events = outboxRepository.lockBatchNewEvents(200);
        for (OutboxEvent event : events) {
            try {
                String payload = objectMapper.writeValueAsString(event.getPayload());
                kafkaTemplate.send("post-created", String.valueOf(event.getAggregateId()), payload).get();
                outboxRepository.markSent(event.getId());
            } catch (Exception ex) {
                outboxRepository.markFailedAndRetry(event.getId(), ex.getMessage());
            }
        }
    }
}
```

### 11.3 Fanout Worker（普通用户 push，大V跳过）

```java
@Component
public class FanoutConsumer {
    private static final long CELEBRITY_THRESHOLD = 100_000L;
    private static final int TIMELINE_MAX_SIZE = 1500;

    private final FollowRepository followRepository;
    private final UserCounterRepository userCounterRepository;
    private final StringRedisTemplate redisTemplate;

    public FanoutConsumer(FollowRepository followRepository,
                          UserCounterRepository userCounterRepository,
                          StringRedisTemplate redisTemplate) {
        this.followRepository = followRepository;
        this.userCounterRepository = userCounterRepository;
        this.redisTemplate = redisTemplate;
    }

    @KafkaListener(topics = "post-created", groupId = "fanout-g1")
    public void onPostCreated(PostCreatedEvent event) {
        long followerCount = userCounterRepository.getFollowerCount(event.authorId());
        if (followerCount >= CELEBRITY_THRESHOLD) {
            // 大V采用 pull，不做全量 push
            return;
        }

        List<Long> followers = followRepository.listFollowerIds(event.authorId());
        for (Long followerId : followers) {
            String key = "timeline:" + followerId;
            redisTemplate.opsForZSet().add(key, String.valueOf(event.postId()), event.createdAtMillis());
            // 控制每个 timeline 容量
            Long size = redisTemplate.opsForZSet().size(key);
            if (size != null && size > TIMELINE_MAX_SIZE) {
                redisTemplate.opsForZSet().removeRange(key, 0, size - TIMELINE_MAX_SIZE - 1);
            }
        }
    }
}
```

### 11.4 Timeline 读取与合并（游标分页）

```java
@Service
public class TimelineService {
    private final StringRedisTemplate redisTemplate;
    private final PostQueryService postQueryService;
    private final FollowRepository followRepository;
    private final UserCounterRepository userCounterRepository;
    private final RankingService rankingService;

    public TimelineService(StringRedisTemplate redisTemplate,
                           PostQueryService postQueryService,
                           FollowRepository followRepository,
                           UserCounterRepository userCounterRepository,
                           RankingService rankingService) {
        this.redisTemplate = redisTemplate;
        this.postQueryService = postQueryService;
        this.followRepository = followRepository;
        this.userCounterRepository = userCounterRepository;
        this.rankingService = rankingService;
    }

    public TimelinePage queryHomeTimeline(long userId, TimelineCursor cursor, int limit) {
        String key = "timeline:" + userId;
        double maxScore = cursor == null ? Double.MAX_VALUE : cursor.score();

        Set<ZSetOperations.TypedTuple<String>> tuples =
                redisTemplate.opsForZSet().reverseRangeByScoreWithScores(key, 0, maxScore, 0, limit * 2);

        List<Long> pushedPostIds = new ArrayList<>();
        if (tuples != null) {
            for (ZSetOperations.TypedTuple<String> t : tuples) {
                pushedPostIds.add(Long.parseLong(Objects.requireNonNull(t.getValue())));
            }
        }

        // 合并大V帖子（pull）
        List<Long> celebrityIds = followRepository.listFolloweeIds(userId).stream()
                .filter(fid -> userCounterRepository.getFollowerCount(fid) >= 100_000L)
                .limit(200)
                .toList();
        List<Long> pulledPostIds = postQueryService.listRecentPostsByAuthors(celebrityIds, 100);

        // 去重合并
        LinkedHashSet<Long> merged = new LinkedHashSet<>();
        merged.addAll(pushedPostIds);
        merged.addAll(pulledPostIds);

        List<PostView> posts = postQueryService.batchGetPostViews(new ArrayList<>(merged));
        List<PostView> ranked = rankingService.rankForUser(userId, posts);
        List<PostView> pageItems = ranked.stream().limit(limit).toList();

        TimelineCursor next = TimelineCursor.from(pageItems);
        return new TimelinePage(pageItems, next, ranked.size() > limit);
    }
}
```

### 11.5 轻量排序服务

```java
@Service
public class RankingService {
    public List<PostView> rankForUser(long userId, List<PostView> candidates) {
        long now = System.currentTimeMillis();
        return candidates.stream()
                .peek(p -> {
                    double recencyHours = Math.max(1.0, (now - p.createdAtMillis()) / 3600000.0);
                    double recencyScore = 1.0 / recencyHours;
                    double engagementScore = 0.6 * p.likeCount() + 1.2 * p.commentCount() + 1.5 * p.repostCount();
                    double qualityPenalty = p.isLowQuality() ? 10.0 : 0.0;
                    p.setScore(0.75 * recencyScore + 0.25 * Math.log1p(engagementScore) - qualityPenalty);
                })
                .sorted(Comparator.comparingDouble(PostView::score).reversed())
                .toList();
    }
}
```

### 11.6 搜索索引消费者

```java
@Component
public class SearchIndexerConsumer {
    private final SearchClient searchClient;

    public SearchIndexerConsumer(SearchClient searchClient) {
        this.searchClient = searchClient;
    }

    @KafkaListener(topics = "post-created", groupId = "search-indexer-g1")
    public void onPostCreated(PostCreatedEvent event) {
        SearchDocument doc = new SearchDocument(
                String.valueOf(event.postId()),
                event.authorId(),
                event.content(),
                event.createdAt()
        );
        searchClient.index("post_index_v1", doc.id(), doc);
    }
}
```

---

## 12. 缓存与数据一致性策略

### 12.1 Timeline 缓存策略
1. 读：优先读 Redis timeline。
2. 写：由 fanout worker 异步写入。
3. 关注变更后可做“懒更新”，不需要立刻重建全部 timeline。

### 12.2 Post 详情缓存
1. Key：`post:{postId}`。
2. 更新策略：写数据库后删缓存。
3. TTL：可配 12h + 抖动。

### 12.3 反模式提醒
1. 不要把所有统计写回主帖表做强一致实时更新。
2. 不要在拉 feed 时逐条查作者信息（N+1 灾难）。

---

## 13. 故障场景与应对

### 13.1 大V发帖引发队列积压
1. 现象：`post-created` topic lag 快速增长。
2. 处理：
   1. 动态把该作者切到 pull 模式。
   2. fanout worker 扩容。
   3. 限制单用户每分钟发帖数。

### 13.2 Redis 抖动导致 timeline 读延迟升高
1. 现象：P99 从 300ms 飙到 1s。
2. 处理：
   1. 启用本地短缓存（1~2 秒）。
   2. 降级为“只返回最近缓存页”。
   3. 逐步恢复，避免回源风暴。

### 13.3 Search 集群故障
1. 主链路影响：发帖和 timeline 不应受影响。
2. 降级：搜索接口返回“暂不可用”，并保留重建索引任务。

### 13.4 索引落后主库
1. 允许秒级延迟。
2. 提供“最近发布内容”快速查询兜底，不依赖索引。

---

## 14. 可观测性与 SLO

### 14.1 必监控指标
1. 发帖成功率、发帖延迟（P95/P99）。
2. timeline 读取成功率、P95/P99。
3. Redis 命中率、key 数量、内存水位。
4. Kafka lag、消费速率、失败重试率。
5. fanout 写放大倍数（每帖分发条数）。
6. 搜索索引延迟（index freshness）。

### 14.2 典型阈值
1. timeline P99 > 500ms（5 分钟）告警。
2. post-created lag > 2,000,000 告警。
3. Redis 内存 > 80% 告警，>90% 紧急。
4. 搜索索引延迟 > 60 秒告警。

---

## 15. 和同类题的差异（少题覆盖）

1. 与 `Chat System`：
   1. 聊天强调点对点实时性和顺序。
   2. Feed 强调 fanout、排序和读扩展。

2. 与 `Recommendation System`：
   1. Feed 母题可以先用规则排序。
   2. 推荐题会深挖召回/粗排/精排与在线学习。

3. 与 `Search Engine`：
   1. Feed 里的搜索是“帖子检索子能力”。
   2. 搜索引擎题会强调爬取、索引、相关性体系全链路。

4. 与 `Push Notification`：
   1. Feed 是内容消费系统。
   2. Push 是触达系统，目标与链路不同。

---

## 16. 面试高频追问（直接准备）

1. 为什么不用纯 push？
   1. 大V写放大会把系统打穿。
   2. 混合策略在性能和复杂度上更平衡。

2. 关注关系更新后要不要重算全部 timeline？
   1. 不要全量重算，代价太高。
   2. 用懒更新 + 下一次读取时合并新关系。

3. 如何避免重复帖子？
   1. push 与 pull 合并后按 `postId` 去重。
   2. 排序前统一去重，避免重复曝光。

4. 删除帖子如何处理？
   1. 主表软删。
   2. 异步广播删除事件，清理缓存 timeline 中对应 postId。

5. 如何保证发帖与分发不丢？
   1. 主表事务 + Outbox。
   2. Relay 重试 + 死信队列 + 回放工具。

6. 游标为什么不用 pageNo/pageSize？
   1. 动态流场景 offset 会跳页/重复，性能也差。
   2. 游标基于时间和 postId 更稳定。

---

## 17. 上场前 Checklist

1. 是否明确 Hybrid Fan-out 的边界和大V阈值。
2. 是否能画出发帖与拉流两条链路。
3. 是否说明最终一致与可见性延迟预期。
4. 是否给出缓存、队列、索引三类故障兜底。
5. 是否给出写放大、lag、延迟等关键指标。
6. 是否有 Java 代码能落地（而不是只有概念）。
7. 是否能讲清与推荐/搜索/聊天题的边界。

---

## 18. 30秒总结

这题本质是“高并发读场景下的内容分发系统”。  
核心不是 CRUD，而是如何在海量关注关系下稳住写扩散与读延迟：  
用 `Hybrid Fan-out + Redis Timeline + Kafka 异步分发 + Outbox 一致性 + 游标分页 + 轻量排序`，就能把系统从“能跑”做到“可面试、可上线、可扩展”。

