# Design Message Queue - 消息队列系统（母题）

> 对应题：Q9 `Design Message Queue - 消息队列`  
> 迁移价值：这题学透后，可覆盖异步解耦、削峰填谷、事件驱动、重试补偿、死信回放、Outbox、Saga 等高频系统设计题。

---

## 1. 题目边界（先讲清楚）

### 1.1 P0 能力
1. 生产者发布消息（高吞吐、低延迟）。
2. 消费者订阅并消费消息。
3. 消息持久化，不因单机故障丢失。
4. 消费位点（offset）可提交、可恢复。
5. 分区内有序。

### 1.2 P1 能力
1. 延时消息（Delay Queue）。
2. 重试队列与死信队列（DLQ）。
3. 消息回放（按时间或 offset 回放）。
4. 多租户配额与限流。
5. 跨机房复制。

### 1.3 非功能目标（面试要量化）
1. 写入吞吐：峰值 `>= 1,000,000 msg/s`（集群维度）。
2. 端到端延迟：P95 `< 20ms`，P99 `< 50ms`（不含下游业务耗时）。
3. 可用性：`>= 99.99%`。
4. 数据可靠性：`acks=all` 场景下 RPO 接近 0。
5. 运维目标：Broker 故障 1 分钟内自动恢复服务。

---

## 2. 容量估算（必须算）

### 2.1 假设
1. 每日消息量：`10,000,000,000`（100 亿）。
2. 平均消息大小：`1KB`。
3. 峰值系数：`5`。
4. 保留期：`3` 天热数据。

### 2.2 计算
1. 平均 QPS：`10^10 / 86400 ≈ 115,740 msg/s`。
2. 峰值 QPS：`≈ 578,700 msg/s`（按 60 万设计）。
3. 每日写入流量：`10^10 * 1KB ≈ 10TB/天`（压缩前）。
4. 3 天数据：`30TB`（未含副本）。
5. 副本数 3 时总存储：`≈ 90TB`（再加索引与冗余，建议 110TB 预留）。

### 2.3 结论
1. 必须顺序写磁盘，随机写成本过高。
2. 必须分区水平扩展，单机吞吐无法承接。
3. 必须副本复制 + 自动选主，保障高可用。

---

## 3. 核心语义（先定协议，后谈实现）

### 3.1 投递语义
1. At-most-once：可能丢，不重复。
2. At-least-once：不丢，可能重复。
3. Exactly-once（工程定义）：在“生产端幂等 + 事务 + 消费端幂等”组合下实现业务上恰好一次。

### 3.2 顺序语义
1. 只保证“分区内顺序”。
2. 全局顺序代价高，通常不做。
3. 同业务键（如 `orderId`）固定进同分区以获得业务顺序。

### 3.3 消费语义
1. 拉模型（poll）更易做流控与批处理。
2. 提交 offset 表示“该位点之前都已处理成功”。
3. 先处理再提交，避免消息丢失。

---

## 4. 高层架构

```text
Producer SDK
    |
    v
+-------------------+      +----------------------+
| Access Gateway    |----->| Topic Metadata Svc   |
| auth / quota      |      | route / leader map   |
+---------+---------+      +-----------+----------+
          |                            |
          v                            v
 +--------+----------------------------+-------------------+
 |                    Broker Cluster                        |
 |  +-----------+   +-----------+   +-----------+          |
 |  | Broker A  |   | Broker B  |   | Broker C  |          |
 |  | P0 leader |   | P1 leader |   | P2 leader |          |
 |  | ISR repl  |   | ISR repl  |   | ISR repl  |          |
 |  +-----+-----+   +-----+-----+   +-----+-----+          |
 +--------+-------------+---------------+-------------------+
          |             |               |
          +-------------+---------------+
                        v
               Log Segment Storage (SSD)

Consumer SDK <---- Broker fetch API
    |
    v
Offset Store / Group Coordinator

Side Path:
Retry Topic / DLQ Topic / Replay Job / Metrics & Alerting
```

---

## 5. 数据与元数据设计

### 5.1 Topic 元数据表

```sql
CREATE TABLE mq_topic (
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  topic_name VARCHAR(128) NOT NULL,
  partition_count INT NOT NULL,
  replica_factor INT NOT NULL,
  retention_hours INT NOT NULL,
  cleanup_policy VARCHAR(32) NOT NULL DEFAULT 'delete', -- delete/compact
  status TINYINT NOT NULL DEFAULT 1,
  created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
  updated_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  UNIQUE KEY uk_topic_name (topic_name)
) ENGINE=InnoDB;
```

### 5.2 分区状态表

```sql
CREATE TABLE mq_partition (
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  topic_name VARCHAR(128) NOT NULL,
  partition_id INT NOT NULL,
  leader_broker_id INT NOT NULL,
  isr_json JSON NOT NULL,
  high_watermark BIGINT NOT NULL DEFAULT 0,
  log_start_offset BIGINT NOT NULL DEFAULT 0,
  log_end_offset BIGINT NOT NULL DEFAULT 0,
  status TINYINT NOT NULL DEFAULT 1,
  updated_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  UNIQUE KEY uk_topic_partition (topic_name, partition_id)
) ENGINE=InnoDB;
```

### 5.3 消费组位点表

```sql
CREATE TABLE mq_consumer_offset (
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  group_id VARCHAR(128) NOT NULL,
  topic_name VARCHAR(128) NOT NULL,
  partition_id INT NOT NULL,
  committed_offset BIGINT NOT NULL,
  metadata VARCHAR(256) NULL,
  updated_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  UNIQUE KEY uk_group_topic_partition (group_id, topic_name, partition_id)
) ENGINE=InnoDB;
```

### 5.4 Broker 本地日志组织（重点）
1. 每个分区是 append-only log。
2. 按 segment 切分，如每 1GB 一个 segment。
3. 每个 segment 有两类索引：
   1. offset->position 索引。
   2. timestamp->offset 索引（用于按时间回放）。

---

## 6. API 设计（协议层）

### 6.1 创建 Topic

```http
POST /api/v1/topics
Body:
{
  "topicName": "order-events",
  "partitionCount": 48,
  "replicaFactor": 3,
  "retentionHours": 72
}
```

### 6.2 生产消息

```http
POST /api/v1/topics/{topic}/messages:batch
Body:
{
  "acks": "all",
  "compression": "lz4",
  "records": [
    {
      "key": "order_90001",
      "headers": {"traceId":"t-001"},
      "value": "base64...",
      "timestamp": 1740276000000
    }
  ]
}
```

### 6.3 拉取消息

```http
POST /api/v1/consumer-groups/{groupId}/fetch
Body:
{
  "topic": "order-events",
  "partition": 3,
  "offset": 102400,
  "maxBytes": 1048576,
  "maxWaitMs": 300
}
```

### 6.4 提交位点

```http
POST /api/v1/consumer-groups/{groupId}/commit
Body:
{
  "topic": "order-events",
  "partition": 3,
  "offset": 102560
}
```

### 6.5 回放

```http
POST /api/v1/replay-jobs
Body:
{
  "sourceTopic": "order-events",
  "targetTopic": "order-events-replay",
  "fromTimestamp": 1740200000000,
  "toTimestamp": 1740280000000,
  "filters": {"orderType":"refund"}
}
```

---

## 7. 核心流程一：生产写入

1. Producer 按 topic 拉取元数据，拿到分区 leader。
2. 根据 key 选择分区（哈希或自定义）。
3. 发往 leader，leader 先追加本地日志。
4. leader 复制给 ISR 副本。
5. 满足 `acks` 条件后返回成功。

`acks` 策略说明：
1. `0`：不等确认，最快但可能丢。
2. `1`：leader 写入即返回，leader 宕机窗口可能丢。
3. `all`：ISR 达标后返回，可靠性最高。

---

## 8. 核心流程二：消费与位点提交

1. Consumer 向协调器注册加入 group。
2. 协调器分配分区（rebalance）。
3. Consumer `fetch` 拉取消息并执行业务处理。
4. 处理成功后提交 committed offset。
5. Consumer 重启后从 committed offset 继续。

关键原则：
1. 处理失败不提交 offset。
2. 业务成功但 commit 失败可能重复消费，消费端必须幂等。

---

## 9. 核心流程三：重试、死信、回放

### 9.1 重试
1. 瞬时故障（网络抖动）进入重试队列。
2. 使用指数退避：1s/5s/30s/5m。
3. 超过最大次数转 DLQ。

### 9.2 死信（DLQ）
1. 保存失败消息、错误码、堆栈、重试次数。
2. 提供控制台人工修复后回放。

### 9.3 回放
1. 支持按 offset 范围回放。
2. 支持按时间窗回放。
3. 支持按 header 条件回放（如 `traceId`）。

---

## 10. Java 关键代码（面试可直接讲）

### 10.1 分区路由器

```java
public final class HashPartitioner {
    public int partition(String key, int partitionCount) {
        if (partitionCount <= 0) {
            throw new IllegalArgumentException("partitionCount must be positive");
        }
        if (key == null || key.isBlank()) {
            // 无 key 时随机或轮询，这里简化成时间扰动
            return (int) (System.nanoTime() % partitionCount);
        }
        int h = key.hashCode();
        return (h & 0x7fffffff) % partitionCount;
    }
}
```

### 10.2 Broker 追加写入（顺序日志）

```java
public class LogAppender {
    private final SegmentManager segmentManager;

    public AppendResult append(String topic, int partition, byte[] payload, Map<String, String> headers) {
        Segment segment = segmentManager.activeSegment(topic, partition);
        long offset = segment.nextOffset();
        long now = System.currentTimeMillis();

        MessageRecord record = new MessageRecord(
                offset,
                now,
                headers,
                payload
        );

        long position = segment.append(record.serialize());
        segment.appendIndex(offset, position);
        segment.appendTimeIndex(now, offset);

        return new AppendResult(offset, position, now);
    }
}
```

### 10.3 复制与 ACK 判断

```java
public class ReplicaAcker {
    public boolean canAckAll(long offset, Set<Integer> isrBrokerIds, Map<Integer, Long> followerAckOffset) {
        // acks=all: offset 必须被 ISR 中每个副本追上
        for (Integer brokerId : isrBrokerIds) {
            Long ack = followerAckOffset.get(brokerId);
            if (ack == null || ack < offset) {
                return false;
            }
        }
        return true;
    }
}
```

### 10.4 Consumer 拉取与提交

```java
public class SimpleConsumerWorker {
    private final BrokerClient brokerClient;
    private final OffsetStore offsetStore;
    private final BusinessHandler businessHandler;

    public void consumeLoop(String groupId, String topic, int partition) {
        long nextOffset = offsetStore.load(groupId, topic, partition);

        while (!Thread.currentThread().isInterrupted()) {
            FetchResponse resp = brokerClient.fetch(topic, partition, nextOffset, 1024 * 1024, 300);
            if (resp.records().isEmpty()) {
                continue;
            }

            for (MessageRecord record : resp.records()) {
                // 业务处理必须幂等
                businessHandler.handle(record);
                nextOffset = record.offset() + 1;
            }

            offsetStore.commit(groupId, topic, partition, nextOffset);
        }
    }
}
```

### 10.5 幂等生产（ProducerId + Sequence）

```java
public class IdempotentProducerState {
    // key: producerId + partition
    private final ConcurrentHashMap<String, Long> lastSeqMap = new ConcurrentHashMap<>();

    public boolean shouldAccept(long producerId, int partition, long sequence) {
        String key = producerId + ":" + partition;
        Long last = lastSeqMap.get(key);
        if (last == null) {
            lastSeqMap.put(key, sequence);
            return true;
        }
        if (sequence <= last) {
            // 重复或乱序，拒绝
            return false;
        }
        lastSeqMap.put(key, sequence);
        return true;
    }
}
```

### 10.6 重试与死信转移

```java
public class RetryDispatcher {
    private final BrokerClient brokerClient;
    private final int maxRetry;

    public RetryDispatcher(BrokerClient brokerClient, int maxRetry) {
        this.brokerClient = brokerClient;
        this.maxRetry = maxRetry;
    }

    public void onConsumeFailed(MessageRecord record, Exception ex) {
        int retry = parseRetry(record.headers());
        if (retry >= maxRetry) {
            brokerClient.send("dlq." + record.topic(), record.key(), record.value(),
                    withError(record.headers(), ex.getMessage()));
            return;
        }

        int nextRetry = retry + 1;
        long delayMs = backoff(nextRetry);
        brokerClient.send("retry." + record.topic(), record.key(), record.value(),
                withRetry(record.headers(), nextRetry, delayMs));
    }

    private long backoff(int retry) {
        if (retry == 1) return 1000L;
        if (retry == 2) return 5000L;
        if (retry == 3) return 30000L;
        return 300000L;
    }

    private int parseRetry(Map<String, String> headers) {
        String v = headers.getOrDefault("x-retry", "0");
        return Integer.parseInt(v);
    }

    private Map<String, String> withRetry(Map<String, String> old, int retry, long delay) {
        Map<String, String> copy = new HashMap<>(old);
        copy.put("x-retry", String.valueOf(retry));
        copy.put("x-delay-ms", String.valueOf(delay));
        return copy;
    }

    private Map<String, String> withError(Map<String, String> old, String err) {
        Map<String, String> copy = new HashMap<>(old);
        copy.put("x-error", err == null ? "unknown" : err);
        return copy;
    }
}
```

### 10.7 回放任务

```java
public class ReplayJobRunner {
    private final BrokerClient brokerClient;

    public ReplayJobRunner(BrokerClient brokerClient) {
        this.brokerClient = brokerClient;
    }

    public long replayByOffsetRange(String sourceTopic, int partition, long fromOffset, long toOffset, String targetTopic) {
        long copied = 0;
        long cursor = fromOffset;

        while (cursor <= toOffset) {
            FetchResponse resp = brokerClient.fetch(sourceTopic, partition, cursor, 2 * 1024 * 1024, 300);
            if (resp.records().isEmpty()) {
                break;
            }
            for (MessageRecord record : resp.records()) {
                if (record.offset() > toOffset) {
                    return copied;
                }
                brokerClient.send(targetTopic, record.key(), record.value(), record.headers());
                copied++;
                cursor = record.offset() + 1;
            }
        }
        return copied;
    }
}
```

---

## 11. 分区与副本策略（面试高频）

### 11.1 分区数怎么定
经验做法：
1. 先按峰值吞吐倒推分区数。
2. 留 30% 扩容余量。
3. 每分区目标写入不超过单盘可承载上限的 70%。

示例：
1. 峰值 60 万 msg/s。
2. 单分区稳定 2 万 msg/s。
3. 至少 `30` 分区，建议起步 `48` 分区。

### 11.2 副本数怎么定
1. 副本 1：不具备容灾。
2. 副本 2：可用性提升，但仲裁能力弱。
3. 副本 3：主流折中，推荐。

### 11.3 ISR 管理
1. 副本落后过大剔除 ISR。
2. 恢复追平后再加入 ISR。
3. `min.insync.replicas` 必须与 `acks=all` 配套。

---

## 12. 消费组 Rebalance（必须讲清）

### 12.1 触发场景
1. 消费者实例增减。
2. topic 分区数变化。
3. 消费者心跳超时。

### 12.2 风险
1. 抖动频繁导致暂停消费。
2. 重分配期间重复消费增加。

### 12.3 优化
1. 使用 Sticky/Cooperative Rebalance。
2. 增大 session timeout 与 heartbeat 合理比值。
3. 控制滚动发布速度，避免全组同时重启。

---

## 13. 消息积压与背压治理

### 13.1 积压指标
1. `lag = logEndOffset - committedOffset`。
2. 关注 lag 增长速率，不只看绝对值。

### 13.2 常见原因
1. 下游处理慢。
2. 消费并发不足。
3. 分区热点不均。

### 13.3 治理手段
1. 提升消费并行度（增加实例，前提分区数足够）。
2. 调大 batch 拉取与批处理。
3. 优化业务处理耗时，隔离慢 handler。
4. 做分区重平衡或热点 key 拆分。

---

## 14. 数据保留与清理

### 14.1 Retention
1. 按时间清理（如 72h）。
2. 按大小清理（如每分区 500GB）。

### 14.2 Compaction（适合状态流）
1. 同 key 仅保留最新值。
2. 典型场景：用户状态、配置状态。

### 14.3 冷热分层
1. 热数据留 SSD。
2. 冷数据下沉对象存储。
3. 回放时按需回温。

---

## 15. 可观测性与告警

### 15.1 必监控指标
1. 生产吞吐（msg/s, MB/s）。
2. 生产延迟（P95/P99）。
3. 消费延迟与 lag。
4. ISR 数量变化、Under-replicated partitions。
5. Broker 磁盘水位、Page Cache 命中率。
6. 请求错误码分布（超时、重试、鉴权失败）。

### 15.2 告警阈值示例
1. URP > 0 持续 3 分钟告警。
2. 任一 broker 磁盘 > 85% 告警，>90% 紧急。
3. 关键 topic lag > 1,000,000 告警。
4. 生产超时率 > 0.5% 告警。

---

## 16. 故障演练（必须有实操感）

### 演练1：Leader Broker 宕机
1. 预期：分区自动选主，生产抖动后恢复。
2. 检查：恢复时长、丢消息情况、ISR 变化。
3. 目标：RTO < 1 分钟。

### 演练2：消费者雪崩
1. 预期：lag 上升但系统不崩。
2. 动作：限流 + 扩容消费者 + 降级非关键消费组。
3. 目标：lag 在 15 分钟内回落。

### 演练3：磁盘写满
1. 预期：写入被保护性拒绝而非整体崩溃。
2. 动作：触发自动清理策略、迁移分区、扩盘。
3. 目标：避免脏写与数据损坏。

### 演练4：网络分区（脑裂风险）
1. 预期：少数派 broker 不可写。
2. 动作：以 ISR 与仲裁策略保障一致性。
3. 目标：不发生双主。

---

## 17. 与相近题的边界（少题覆盖）

1. 与 `Event-driven Architecture`：
   1. MQ 题关注“通道可靠性与吞吐”。
   2. EDA 题关注“业务事件建模与编排”。

2. 与 `Outbox Pattern`：
   1. MQ 解决传输。
   2. Outbox 解决“业务库写入与消息发送一致性”。

3. 与 `Idempotency`：
   1. MQ at-least-once 必然引入重复消费风险。
   2. 幂等是消费侧必须补的能力。

4. 与 `Backpressure`：
   1. MQ 可承压但不是无限缓冲。
   2. 背压题会更深入流控协议和上游协商。

5. 与 `Distributed Log Aggregation`：
   1. 日志聚合是 MQ 的典型应用场景。
   2. 但日志系统还要解决检索与存储分层。

---

## 18. 面试高频追问

1. 为什么消息队列常用“顺序写磁盘”还能高吞吐？
   1. 顺序写命中磁盘与页缓存特性。
   2. 批量刷盘和零拷贝进一步提升吞吐。

2. `acks=all` 就一定不丢吗？
   1. 依赖 ISR 与最小同步副本配置正确。
   2. 配置不当仍可能在极端故障中丢数据。

3. 怎么保证订单消息按序？
   1. 同 `orderId` 固定路由到同一分区。
   2. 消费端单线程或同 key 串行处理。

4. 为什么会重复消费？
   1. 处理成功但 offset 未提交。
   2. Rebalance 期间重复拉取。

5. 如何做到“业务上 exactly-once”？
   1. 生产端幂等 + 事务。
   2. 消费端幂等（去重表/状态机）。
   3. 关键链路配 Outbox。

6. 回放会不会污染线上？
   1. 先回放到隔离 topic。
   2. 消费侧按 replay header 走隔离逻辑。
   3. 验证后再并入主链路。

---

## 19. 上场前 Checklist

1. 是否讲清了投递语义（at-most/at-least/exactly-once）。
2. 是否讲清了分区内有序而非全局有序。
3. 是否讲清了 producer ack 策略与副本关系。
4. 是否讲清了消费者位点提交流程。
5. 是否讲清了重试、死信、回放闭环。
6. 是否给出 lag、URP、延迟等关键监控指标。
7. 是否给出 leader 宕机、积压、磁盘写满的应急方案。

---

## 20. 30 秒总结

这题本质是“高吞吐、可恢复、可回放的分布式日志系统设计”。  
主线要答好三件事：  
1. 如何写得快（分区 + 顺序写 + 批处理）。  
2. 如何不丢（副本 + ack + 位点提交 + 幂等）。  
3. 如何可运维（重试死信、回放、监控告警、故障演练）。  

