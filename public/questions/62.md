# Q62：Design Container Orchestration System（容器编排系统）- 95分面试版

## 1. 三句话题目本质
1. 容器编排系统本质是“声明目标状态，然后持续把真实状态拉回目标状态”的控制系统。  
2. 难点不只是调度，而是调度 + 发布 + 扩缩容 + 自愈 + 隔离 + 可观测性协同工作。  
3. 面试高分关键：讲清 Control Plane / Data Plane、Reconciliation 模型、调度策略和故障恢复策略。  

## 2. 一个真实场景故事
你们平台上跑着支付、搜索、推荐三个核心服务。  
一次发版窗口发生了这些事：
- 推荐服务发布新版本，Readiness 探针异常，50% Pod 无法就绪。
- 同时某机房 30 台机器掉线，数千 Pod 需要重调度。
- 支付服务流量突增，HPA 开始扩容，但集群资源紧张。

如果系统设计不好，后果是：
- 发布卡死，服务不可用。
- 调度器把新 Pod 全压到少数节点，形成热点。
- 扩容抢占导致低优先级服务雪崩。

容器编排题要讲的就是：在复杂故障和资源竞争下，如何让平台自动稳住。

## 3. 术语白话表（至少 10 项）
| 术语 | 白话解释 | 面试可复述 |
|---|---|---|
| Control Plane | 控制面 | “负责决策和管理，不直接跑业务流量” |
| Data Plane | 数据面 | “真正承载业务 Pod 和流量转发” |
| Desired State | 期望状态 | “用户想要多少副本、什么版本” |
| Reconciliation | 对账循环 | “系统持续比较期望与现实并修正” |
| Pod | 最小调度单元 | “一个或多个强耦合容器的运行单位” |
| Deployment | 无状态工作负载控制器 | “负责副本管理和滚动发布” |
| StatefulSet | 有状态工作负载 | “带稳定身份和存储的副本管理” |
| Scheduler | 调度器 | “决定 Pod 放到哪台节点” |
| Service Discovery | 服务发现 | “让服务能找到彼此” |
| Rolling Update | 滚动发布 | “分批替换旧版本，控制风险” |
| HPA | 水平自动扩缩容 | “按指标自动增减副本” |
| PDB | PodDisruptionBudget | “限制一次能被驱逐的 Pod 数量” |
| Taint/Toleration | 污点/容忍 | “指定哪些 Pod 能上特殊节点” |
| Affinity | 亲和/反亲和 | “希望放一起或分开” |
| Liveness/Readiness | 存活/就绪探针 | “判断是否活着/是否可接流量” |
| CNI/CSI | 网络/存储插件接口 | “接入网络和持久化能力” |

## 4. 需求澄清（功能/非功能/不做范围）
### 4.1 功能需求
- 支持部署无状态和有状态应用。
- 支持服务发现、负载均衡、服务暴露（Ingress/Gateway）。
- 支持滚动发布、灰度、回滚。
- 支持自动扩缩容（CPU/内存/自定义指标）。
- 支持节点故障自动重调度与自愈。
- 支持多租户隔离、配额和权限。

### 4.2 非功能需求（SLO 示例）
- 调度时延：`P95 <= 3s`（从 Pod Pending 到 Bind）。
- 发布成功率：`>= 99.9%`。
- 控制面可用性：`>= 99.95%`。
- 节点故障检测与重调度启动：`<= 30s`。
- 集群 API 可用性：`>= 99.99%`。

### 4.3 Out of Scope（首版不做）
- 不做跨地域强一致多活控制面。
- 不做复杂 serverless runtime（先支持容器工作负载）。
- 不做细粒度 GPU 拓扑调度（可后续扩展）。

## 5. 容量估算（含数字推导）
### 5.1 假设
- 集群节点：`8,000`
- Pod 总数：`1,200,000`
- 峰值新增/迁移 Pod：`15,000 pods/min`
- 每秒心跳：节点每 10 秒一次

### 5.2 控制面吞吐
- 节点心跳 QPS：`8,000 / 10 = 800 qps`
- 资源变更事件（估算）：`5,000~20,000 events/s`（高峰）

### 5.3 etcd 存储与压力
- 若每 Pod 状态对象平均 2KB，`1.2M * 2KB ≈ 2.4GB`（仅对象体积，索引和版本开销更大）
- 高写入场景需要严格控制对象更新频率和 watch fanout

### 5.4 关键结论
- Scheduler 必须并行化 + 插件化，不能单线程全局扫。
- 控制面状态存储（如 etcd）是瓶颈与命门，更新要节流。
- 发布和扩容必须有速率控制，不然容易引发控制面风暴。

## 6. 架构（简版 + 完整版）
### 6.1 简版架构
```text
API Server -> etcd
          -> Scheduler
          -> Controllers
Nodes(Kubelet-like) -> Runtime(CRI) -> Pods
Service Proxy/Ingress -> Traffic
```

### 6.2 完整版架构
```text
[Control Plane]
  -> API Server (authn/authz/admission)
  -> State Store (etcd cluster)
  -> Scheduler (filter/score/bind)
  -> Controller Manager
       - Deployment Controller
       - StatefulSet Controller
       - Node Controller
       - HPA Controller
       - Endpoint/Service Controller
  -> Cluster Autoscaler

[Data Plane]
  -> Node Agent (kubelet-like)
  -> Container Runtime (containerd/CRI-O)
  -> CNI Plugin (pod network)
  -> CSI Plugin (persistent volume)
  -> Service Proxy (iptables/ipvs/eBPF)

[Traffic Plane]
  -> Ingress/Gateway
  -> Service Discovery + Load Balancing

[Ops Plane]
  -> Metrics/Logs/Trace
  -> Alert + Runbook
  -> UI Console (deploy/scale/rollback)
```

## 7. API 设计（含请求/响应样例）
### 7.1 创建 Deployment
`POST /api/v1/workloads/deployments`

Request:
```json
{
  "namespace": "payment",
  "name": "pay-api",
  "image": "registry/pay-api:2.1.0",
  "replicas": 12,
  "resources": {
    "cpuRequestMilli": 500,
    "cpuLimitMilli": 1000,
    "memRequestMb": 512,
    "memLimitMb": 1024
  },
  "strategy": {
    "type": "RollingUpdate",
    "maxUnavailable": 2,
    "maxSurge": 3
  }
}
```

Response:
```json
{
  "deploymentId": "dep_pay_api_001",
  "status": "CREATED"
}
```

### 7.2 扩缩容
`POST /api/v1/workloads/deployments/{id}/scale`

Request:
```json
{
  "replicas": 18,
  "reason": "traffic peak"
}
```

### 7.3 回滚
`POST /api/v1/workloads/deployments/{id}/rollback`

Request:
```json
{
  "revision": 12
}
```

### 7.4 查询节点健康
`GET /api/v1/nodes/health?zone=az-a`

### 7.5 错误码语义
- `409_ROLLOUT_IN_PROGRESS`：已有发布进行中，禁止重复发版。
- `422_INSUFFICIENT_CLUSTER_CAPACITY`：资源不足无法满足副本。
- `503_CONTROL_PLANE_DEGRADED`：控制面降级。

## 8. 数据模型（核心表/索引）
### 8.1 工作负载定义
```sql
CREATE TABLE workload_def (
  workload_id BIGINT PRIMARY KEY,
  namespace VARCHAR(64) NOT NULL,
  name VARCHAR(128) NOT NULL,
  kind VARCHAR(32) NOT NULL,           -- DEPLOYMENT/STATEFULSET/JOB
  desired_replicas INT NOT NULL,
  image VARCHAR(256) NOT NULL,
  strategy_json JSON NOT NULL,
  revision BIGINT NOT NULL,
  created_at TIMESTAMP NOT NULL,
  updated_at TIMESTAMP NOT NULL,
  UNIQUE(namespace, name)
);
```

### 8.2 Pod 实例状态
```sql
CREATE TABLE pod_instance (
  pod_id BIGINT PRIMARY KEY,
  workload_id BIGINT NOT NULL,
  namespace VARCHAR(64) NOT NULL,
  node_id BIGINT,
  phase VARCHAR(16) NOT NULL,         -- PENDING/RUNNING/SUCCEEDED/FAILED
  readiness BOOLEAN NOT NULL DEFAULT FALSE,
  liveness BOOLEAN NOT NULL DEFAULT TRUE,
  cpu_request_milli INT NOT NULL,
  mem_request_mb INT NOT NULL,
  revision BIGINT NOT NULL,
  created_at TIMESTAMP NOT NULL,
  updated_at TIMESTAMP NOT NULL
);
CREATE INDEX idx_pod_workload_phase ON pod_instance(workload_id, phase);
CREATE INDEX idx_pod_node_phase ON pod_instance(node_id, phase);
```

### 8.3 节点状态
```sql
CREATE TABLE node_status (
  node_id BIGINT PRIMARY KEY,
  zone VARCHAR(32) NOT NULL,
  cpu_total_milli INT NOT NULL,
  cpu_allocatable_milli INT NOT NULL,
  mem_total_mb INT NOT NULL,
  mem_allocatable_mb INT NOT NULL,
  ready BOOLEAN NOT NULL,
  taints JSON,
  last_heartbeat TIMESTAMP NOT NULL
);
CREATE INDEX idx_node_zone_ready ON node_status(zone, ready);
```

### 8.4 发布历史
```sql
CREATE TABLE rollout_history (
  rollout_id BIGINT PRIMARY KEY,
  workload_id BIGINT NOT NULL,
  from_revision BIGINT NOT NULL,
  to_revision BIGINT NOT NULL,
  status VARCHAR(16) NOT NULL,        -- RUNNING/SUCCEEDED/FAILED/ROLLED_BACK
  started_at TIMESTAMP NOT NULL,
  ended_at TIMESTAMP
);
```

## 9. 核心流程（至少 3 条）
### 9.1 正常部署流程
1. 用户提交 Deployment 声明（期望 12 副本）。
2. Deployment Controller 比对现状，创建缺失 Pod。
3. Scheduler 对每个 Pending Pod 做过滤和打分，绑定节点。
4. Node Agent 拉镜像并启动容器。
5. Readiness 通过后加入 Service Endpoints。

### 9.2 高峰自动扩容流程
1. HPA 读取 `CPU/请求延迟` 指标，判断需从 12 扩到 20。
2. 控制器更新期望副本数。
3. Scheduler 进行增量调度。
4. 如容量不足，触发 Cluster Autoscaler 扩节点。
5. 新节点 Ready 后继续放置剩余 Pod。

### 9.3 故障恢复流程（节点宕机）
1. Node Controller 检测心跳超时，节点标记 NotReady。
2. 该节点上的 Pod 标记失效，触发重建。
3. Scheduler 将替代 Pod 调度到健康节点。
4. Service Endpoints 自动剔除故障 Pod，流量不中断。

### 9.4 滚动发布流程
1. 用户提交新镜像 revision=13。
2. 控制器按 `maxSurge/maxUnavailable` 分批创建新 Pod。
3. 每批新 Pod Readiness 通过后，再下线旧 Pod。
4. 若错误率超阈值，触发自动回滚到 revision=12。

## 10. 一致性与事务边界
### 10.1 一致性模式
- 控制面采用最终一致控制循环，不追求强同步操作。
- 核心保证是“最终收敛到期望状态”。

### 10.2 事务边界
- 单次对象写入（如 Deployment spec）是原子。
- 多对象变更（Pod + Endpoint + Event）通过控制器异步收敛。

### 10.3 幂等要求
- Controller Reconcile 必须幂等，重复执行不能副作用放大。
- Scheduler Bind 操作需处理重复绑定冲突。

### 10.4 面试可复述
“容器编排不是事务型系统，而是控制循环系统，关键是幂等和最终收敛。”

## 11. 可用性与容错
### 11.1 常见故障
- API Server 过载导致控制请求排队。
- etcd 延迟升高导致控制面抖动。
- 大规模节点故障导致调度风暴。
- 发布脚本错误引发大面积 Pod CrashLoop。

### 11.2 容错策略
- 控制面多副本 + 负载均衡。
- etcd 三/五节点集群，定期快照。
- 调度限速与分批发布避免风暴。
- 自动回滚 + PDB 防止服务同时下线太多实例。

### 11.3 RTO / RPO
- 控制面单节点故障 RTO：`< 1 分钟`。
- 区域级节点故障 RTO：`5~10 分钟`（依赖容量）。
- 配置对象 RPO：秒级（取决于 etcd 写入保障）。

## 12. 可观测性（指标 + 告警阈值）
### 12.1 调度与控制面
- `scheduler_queue_depth`
- `pod_schedule_latency_p95`
- `apiserver_request_latency_p99`
- `controller_reconcile_fail_rate`

阈值示例：
- `pod_schedule_latency_p95 > 3000ms` 持续 10 分钟 -> P1
- `controller_reconcile_fail_rate > 2%` 持续 5 分钟 -> P1

### 12.2 节点与运行时
- `node_not_ready_count`
- `pod_restart_rate`
- `image_pull_fail_rate`

阈值示例：
- `node_not_ready_count > 50` 持续 3 分钟 -> P1
- `pod_restart_rate > baseline*3` 持续 10 分钟 -> P1

### 12.3 发布与扩缩容
- `rollout_success_rate`
- `rollback_trigger_count`
- `hpa_scale_events`

阈值示例：
- `rollout_success_rate < 98%` 日级 -> P2
- 单工作负载 `rollback_trigger_count >= 2/h` -> P1

## 13. 安全与隔离
- Namespace 隔离：团队/环境分离（prod/staging/dev）。
- RBAC：谁能部署、谁能回滚、谁能看 secrets。
- NetworkPolicy：默认拒绝，按需放通。
- Secret 管理：密文存储、短期 token、挂载权限最小化。
- Pod 安全策略：禁止特权容器、限制宿主机路径挂载。
- 资源配额：每命名空间 CPU/内存上限，防止“邻居噪声”。

## 14. 成本与取舍
### 14.1 成本构成
- 节点资源（CPU/内存/磁盘）。
- 控制面和存储成本（etcd/日志/指标）。
- 网络与负载均衡成本。

### 14.2 关键取舍
- requests 配太高：稳定但浪费。
- requests 配太低：利用率高但容易抖动。
- 发布批次太大：速度快但风险高。
- 扩容太激进：响应快但成本高、波动大。

### 14.3 降本策略
- 混部策略（低优先级任务填充空闲资源）。
- 自动缩容和闲时回收。
- 右 sizing（根据历史真实用量调整 requests/limits）。

## 15. 关键代码（Java 更细 + 前端功能代码）
### 15.1 Java：Scheduler 过滤+打分+绑定
```java
public class SchedulerService {
    public Optional<Long> schedule(PodSpec pod, List<NodeSnapshot> nodes) {
        List<NodeSnapshot> feasible = nodes.stream()
            .filter(n -> n.isReady())
            .filter(n -> n.getCpuFreeMilli() >= pod.getCpuRequestMilli())
            .filter(n -> n.getMemFreeMb() >= pod.getMemRequestMb())
            .filter(n -> taintTolerationMatch(pod, n))
            .toList();

        if (feasible.isEmpty()) return Optional.empty();

        NodeSnapshot best = feasible.stream()
            .max(Comparator.comparingInt(n -> scoreNode(pod, n)))
            .orElseThrow();

        boolean bound = podRepo.bindPodToNode(pod.getPodId(), best.getNodeId()); // CAS bind
        return bound ? Optional.of(best.getNodeId()) : Optional.empty();
    }

    private int scoreNode(PodSpec pod, NodeSnapshot node) {
        int resourceScore = node.getCpuFreeMilli() / 100 + node.getMemFreeMb() / 256;
        int zoneSpreadScore = spreadPluginScore(pod, node);
        return resourceScore + zoneSpreadScore;
    }

    private boolean taintTolerationMatch(PodSpec pod, NodeSnapshot node) {
        return node.getTaints().stream().allMatch(t -> pod.getTolerations().contains(t.getKey()));
    }
}
```

### 15.2 Java：Deployment Reconcile（滚动发布）
```java
public class DeploymentController {
    public void reconcile(Deployment dep) {
        PodStats stats = podRepo.stats(dep.getId(), dep.getRevision());
        int desired = dep.getDesiredReplicas();
        int maxSurge = dep.getStrategy().getMaxSurge();
        int maxUnavailable = dep.getStrategy().getMaxUnavailable();

        int currentTotal = stats.getOldPods() + stats.getNewPods();
        int canCreate = Math.max(0, desired + maxSurge - currentTotal);
        int canDeleteOld = Math.max(0, stats.getReadyNewPods() - (desired - maxUnavailable));

        for (int i = 0; i < canCreate; i++) {
            podRepo.createPendingPod(dep.getId(), dep.getRevision(), dep.getTemplate());
        }
        for (int i = 0; i < canDeleteOld; i++) {
            podRepo.deleteOneOldPod(dep.getId(), dep.getRevision() - 1);
        }
    }
}
```

### 15.3 Java：HPA 控制器（带稳定窗口，防抖）
```java
public class HpaController {
    public int desiredReplicas(HpaSpec spec, int currentReplicas, double currentCpuUtilization) {
        int raw = (int) Math.ceil(currentReplicas * currentCpuUtilization / spec.getTargetCpuUtilization());
        int bounded = Math.max(spec.getMinReplicas(), Math.min(spec.getMaxReplicas(), raw));
        return stabilizationWindow(currentReplicas, bounded, spec.getScaleUpStep(), spec.getScaleDownStep());
    }

    private int stabilizationWindow(int current, int target, int upStep, int downStep) {
        if (target > current) return Math.min(current + upStep, target);
        if (target < current) return Math.max(current - downStep, target);
        return current;
    }
}
```

### 15.4 Java：节点故障自愈控制器
```java
public class NodeController {
    public void reconcileNodes(List<NodeSnapshot> nodes, Instant now) {
        for (NodeSnapshot node : nodes) {
            long gapSec = Duration.between(node.getLastHeartbeat(), now).toSeconds();
            if (gapSec > 30 && node.isReady()) {
                nodeRepo.markNotReady(node.getNodeId());
                List<PodInstance> victims = podRepo.listRunningByNode(node.getNodeId());
                for (PodInstance p : victims) {
                    if (p.isManagedByController()) {
                        podRepo.markLostAndRecreate(p.getPodId());
                    }
                }
            }
        }
    }
}
```

### 15.5 Java：Service Endpoint Controller（Readiness 决定是否接流量）
```java
public class EndpointController {
    public void reconcileService(long serviceId) {
        List<PodInstance> pods = podRepo.listByServiceSelector(serviceId);
        List<String> readyEndpoints = pods.stream()
            .filter(PodInstance::isReadiness)
            .map(p -> p.getPodIp() + ":" + p.getContainerPort())
            .toList();
        endpointRepo.replaceEndpoints(serviceId, readyEndpoints);
    }
}
```

### 15.6 前端（React + TypeScript）：发布面板（发布/回滚）
```tsx
import { useState } from "react";

export function RolloutPanel({ deploymentId }: { deploymentId: string }) {
  const [image, setImage] = useState("");
  const [revision, setRevision] = useState("");
  const [msg, setMsg] = useState("");

  const rollout = async () => {
    const resp = await fetch(`/api/v1/workloads/deployments/${deploymentId}`, {
      method: "PATCH",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ image })
    });
    setMsg(resp.ok ? "发布已开始" : `发布失败: ${resp.status}`);
  };

  const rollback = async () => {
    const resp = await fetch(`/api/v1/workloads/deployments/${deploymentId}/rollback`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ revision: Number(revision) })
    });
    setMsg(resp.ok ? "回滚已触发" : `回滚失败: ${resp.status}`);
  };

  return (
    <div>
      <h3>发布控制</h3>
      <input value={image} onChange={(e) => setImage(e.target.value)} placeholder="新镜像，如 registry/app:v2" />
      <button onClick={rollout}>开始发布</button>
      <input value={revision} onChange={(e) => setRevision(e.target.value)} placeholder="回滚版本号" />
      <button onClick={rollback}>回滚</button>
      {msg && <p>{msg}</p>}
    </div>
  );
}
```

### 15.7 前端（React + TypeScript）：扩缩容与节点健康面板
```tsx
import { useEffect, useState } from "react";

type NodeHealth = { nodeId: number; zone: string; ready: boolean; cpuFreeMilli: number; memFreeMb: number };

export function ScaleAndHealthPanel({ deploymentId }: { deploymentId: string }) {
  const [replicas, setReplicas] = useState("3");
  const [nodes, setNodes] = useState<NodeHealth[]>([]);
  const [msg, setMsg] = useState("");

  const loadNodes = async () => {
    const resp = await fetch("/api/v1/nodes/health");
    if (resp.ok) {
      const data = await resp.json();
      setNodes(data.items || []);
    }
  };

  useEffect(() => {
    loadNodes();
    const t = window.setInterval(loadNodes, 5000);
    return () => window.clearInterval(t);
  }, []);

  const scale = async () => {
    const resp = await fetch(`/api/v1/workloads/deployments/${deploymentId}/scale`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ replicas: Number(replicas), reason: "manual scale" })
    });
    setMsg(resp.ok ? "扩缩容请求已提交" : `扩缩容失败: ${resp.status}`);
  };

  return (
    <div>
      <h3>扩缩容与节点健康</h3>
      <input value={replicas} onChange={(e) => setReplicas(e.target.value)} />
      <button onClick={scale}>设置副本数</button>
      {msg && <p>{msg}</p>}
      <ul>
        {nodes.map((n) => (
          <li key={n.nodeId}>
            node={n.nodeId} zone={n.zone} ready={String(n.ready)} cpuFree={n.cpuFreeMilli} memFree={n.memFreeMb}
          </li>
        ))}
      </ul>
    </div>
  );
}
```

## 16. 测试策略
### 16.1 单元测试
- 调度过滤规则（资源、污点、亲和）。
- 打分插件稳定性与可重复性。
- HPA 防抖与上下限约束。

### 16.2 集成测试
- 部署 -> 调度 -> 启动 -> 就绪 -> 接流量全链路。
- 滚动发布异常自动回滚。
- 节点掉线触发重调度。

### 16.3 压测
- 1.5 万 Pod/min 创建压力。
- 控制面高频 watch 和写入压力测试。
- 大规模滚动发布并发测试。

### 16.4 故障注入
- API Server 单实例故障。
- etcd 延迟和 leader 切换。
- AZ 故障（批量节点 NotReady）。

## 17. 丰富例子（至少 10 个）
1. 支付服务发布新版本，Readiness 失败时自动停止继续发布。  
2. 节点突然断网，30 秒内被标记 NotReady 并开始重调度。  
3. 高峰期 CPU>70%，HPA 把副本从 10 扩到 16。  
4. 资源不足时触发 Cluster Autoscaler 增加节点。  
5. 灰度发布 10% 流量观察错误率，异常立即回滚。  
6. 某工作负载只允许跑在带 GPU 污点节点。  
7. 反亲和策略把同服务副本分散到不同可用区。  
8. 镜像仓库故障导致拉镜像失败，系统重试并告警。  
9. Namespace 配额防止测试团队占满生产资源。  
10. Service Endpoints 仅包含 Readiness 通过的 Pod。  
11. 扩容后冷启动慢，预热探针优化上线。  
12. etcd 压力高时降低状态更新频率避免雪崩。  

## 18. 面试追问 + 可复述回答
### Q1：调度器如何避免热点节点？
可复述：  
“先过滤资源，再通过打分把流量打散，结合 zone spread 和反亲和避免集中在少数节点。”

### Q2：滚动发布如何保证零停机？
可复述：  
“通过 maxUnavailable/maxSurge 控制批次，只有新 Pod Readiness 通过才下旧 Pod，并配合自动回滚。”

### Q3：节点宕机会发生什么？
可复述：  
“Node Controller 检测心跳超时后标记 NotReady，控制器重建 Pod，服务端点剔除故障实例。”

### Q4：为什么要声明式 API？
可复述：  
“声明式让系统通过 Reconcile 自动收敛，避免人工每步操作，可靠性更高。”

### Q5：HPA 会不会抖动？
可复述：  
“会，所以要加稳定窗口、扩缩容步长和冷却时间，避免频繁震荡。”

### Q6：etcd 挂了会怎样？
可复述：  
“控制面新变更会受影响，但已运行 Pod 还会继续跑；重点是快速恢复控制面可写能力。”

## 19. 新手学习路线
### 第 1 周：核心模型
- 学 Pod/Deployment/Service 三个核心对象。
- 理解声明式与 Reconcile。

### 第 2 周：调度与发布
- 手写简化 scheduler（过滤+打分）。
- 实现简化滚动发布控制器。

### 第 3 周：扩缩容与自愈
- 做 HPA 逻辑和节点故障重调度。
- 加就绪探针与服务端点控制。

### 第 4 周：运维与安全
- 做告警阈值、回滚策略、RBAC/配额。
- 写前端运维面板（发布、扩缩容、健康查看）。

## 20. 上场前 Checklist
- [ ] 我能解释 Control Plane 与 Data Plane 的职责。  
- [ ] 我能讲清调度器 filter/score/bind 三阶段。  
- [ ] 我能说清滚动发布为何不会一次性替换全部 Pod。  
- [ ] 我能说明节点故障后的自动重调度流程。  
- [ ] 我能给出 3 个以上可执行告警阈值。  
- [ ] 我能解释 HPA 防抖策略。  
- [ ] 我能讲清 Namespace/RBAC/配额如何实现多租户隔离。  
- [ ] 我能展示前端控制台如何发版、回滚、扩缩容。  

## 21. 30 秒总结
容器编排系统的高分答案是：  
“通过声明式 API 和控制循环，把期望状态持续收敛为真实状态；调度负责放置，控制器负责发布和自愈，扩缩容负责弹性，隔离与观测负责长期稳定。”  
把这句话展开到具体流程、阈值、失败边界和代码实现，就能达到面试高分水位。  
