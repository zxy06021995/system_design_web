# Q57 Time Series Database (时序数据库)

> 来源校验（questions.ts）  
> `title`: Time Series Database (时序数据库)  
> `tags`: 时序存储, 压缩, 下采样, 写入优化, 查询聚合  
> `keyPoints`: 时间分区, 列式压缩, 降采样策略, 保留策略, 多维标签索引  
> `learningCoreId`: 15（母题：Database Sharding）

## 1. 三句话题目本质
1. 这题本质是“高吞吐写入时间序列数据并支持快速聚合查询”。
2. 难点在写入放大、标签基数爆炸、冷热数据管理和查询成本。
3. 高分要讲清：写入路径、分区索引、压缩下采样、保留策略与查询 API。

## 2. 一个真实场景故事
监控平台每秒采集 500 万指标点，初期直接写通用关系库，1 周后查询超时和存储成本失控。改成 TSDB 后：按时间分区写入、标签索引分层、冷热保留+下采样，查询性能提升，成本降低。

## 3. 术语白话表（>=10）
1. Metric：指标名，如 CPU 使用率。
2. Tag：标签维度，如 host=web01。
3. Series：同 metric+tag 组合的时间序列。
4. Point：一个时间点的数据。
5. Ingest：写入采集过程。
6. Compaction：压缩合并存储块。
7. Retention：保留策略。
8. Downsampling：下采样降精度。
9. Cardinality：标签组合数量。
10. Chunk：时间块存储单元。
11. Rollup：聚合结果表。
12. Hot/Warm/Cold：热温冷分层存储。

## 4. 需求澄清（功能/非功能/不做范围/SLO）
### 4.1 功能需求
1. 支持批量写入时间点。
2. 支持区间查询和聚合（avg/sum/max）。
3. 支持标签过滤与分组。
4. 支持下采样和保留策略。
5. 支持告警查询低延迟。

### 4.2 非功能需求
1. 高写入吞吐。
2. 低延迟聚合查询。
3. 数据可靠和可恢复。
4. 成本可控。

### 4.3 不做范围
1. 不做复杂事务型更新。
2. 不做全文检索。
3. 不做跨云全局强一致。

### 4.4 SLO
1. 写入成功率 >= 99.95%。
2. 热数据查询 P95 <= 1s。
3. 下采样任务成功率 >= 99.9%。

## 5. 容量估算（数字推导）
1. 写入峰值 500 万点/s。
2. 每点约 30B（时间戳+值+压缩标签引用），约 150MB/s。
3. 每日原始数据约 12.9TB。
4. 热存 7 天约 90TB，温存 30 天下采样后约 60TB。
5. 冷存 180 天对象存储归档。
6. 结论：必须压缩+下采样+冷热分层。

## 6. 架构（简版+完整版）
### 6.1 简版
`Collector -> Ingest API -> TSDB Write Path -> Query API`

### 6.2 完整版
1. Ingest Gateway：批量接收指标点。
2. Write Buffer：内存缓冲 + WAL。
3. Shard by Series Hash：按序列分片写入。
4. Storage Engine：Chunk 存储 + 压缩。
5. Index Service：标签倒排索引。
6. Query Engine：区间扫描+聚合。
7. Downsample Worker：离线汇总低精度数据。
8. Lifecycle Manager：冷热迁移和清理。

## 7. API 设计（请求/响应/错误码/幂等）
### 7.1 批量写入
`POST /api/tsdb/v1/write`

请求：
```json
{
  "points": [
    {"metric":"cpu.usage","tags":{"host":"web01","env":"prod"},"ts":1708768800,"value":0.72}
  ]
}
```

### 7.2 区间查询
`POST /api/tsdb/v1/query`

响应：
```json
{
  "series": [{"tags":{"host":"web01"},"points":[[1708768800,0.72],[1708768860,0.69]]}]
}
```

### 7.3 聚合查询
`POST /api/tsdb/v1/query/aggregate`

错误码：
1. `422_TOO_HIGH_CARDINALITY`
2. `413_BATCH_TOO_LARGE`
3. `429_INGEST_THROTTLED`
4. `503_QUERY_TIMEOUT`

幂等规则：
1. 可选 `pointId` 去重（采集重试时防重复点）。
2. 下采样任务按 `taskId` 幂等。

## 8. 数据模型（实体/索引/分片）
1. `series_meta(series_id, metric, tags_hash, tags_json)`。
2. `chunk(series_id, start_ts, end_ts, compressed_blob)`。
3. `tag_index(tag_key, tag_val, series_id)`。
4. `rollup_5m(metric, tags_hash, bucket_ts, agg_val)`。
5. `retention_policy(metric, hot_days, warm_days, cold_days)`。

冷热与分片：
1. 分片按 `series_id hash`。
2. 热 chunk 在 SSD，温数据在 HDD，冷数据对象存储。
3. 旧 chunk 按 retention 自动迁移。

## 9. 核心流程（正常/高峰/故障恢复）
### 9.1 正常流程
1. 采集端批量上报。
2. 网关校验后写 WAL+内存缓冲。
3. Flush 成 chunk 并更新索引。

### 9.2 高峰流程
1. 写入端批量合并和压缩降低 IO。
2. 查询端优先热 rollup，避免扫原始点。
3. 高基数标签触发限流与告警。

### 9.3 故障恢复流程
1. 节点重启从 WAL 重放未 flush 数据。
2. 索引损坏可从 chunk 重建。
3. 下采样失败任务进入重试队列。

## 10. 一致性与事务边界
1. 写入以 WAL 成功为可靠边界。
2. 索引更新与 chunk 写入最终一致。
3. 查询可容忍短暂索引滞后。
4. 下采样结果可重算，非主数据。
5. 删除策略分软删与硬删（延迟执行）。

## 11. 可用性与容错（含 RTO/RPO）
1. 写入分片多副本。
2. WAL 防止宕机丢点。
3. 查询服务无状态水平扩展。
4. RTO：写入节点故障 10 分钟恢复。
5. RPO：WAL 同步后点数据不丢。

## 12. 可观测性（指标+阈值+处置动作）
1. `ingest_qps` 和 `ingest_error_rate`（>1% 告警）。
2. `wal_replay_lag`（>60s 扩容恢复）。
3. `query_p95_ms`（>1000ms 启用聚合缓存）。
4. `cardinality_growth_rate`（突增触发限维度策略）。
5. `downsample_backlog`（积压触发 worker 扩容）。
6. `storage_tier_usage`（热层>80% 触发迁移）。

## 13. 安全与合规
1. 写入 token 按项目隔离。
2. 指标标签敏感信息过滤。
3. 查询权限按租户和 metric 维度限制。
4. 审计导出和策略变更。
5. 数据保留策略合规配置。

## 14. 成本与取舍
1. 原始点保留越久成本越高。
2. 下采样节省存储但损失精度。
3. 高基数标签查询灵活但代价高。
4. 取舍：热数据高精度，温冷数据低精度。

## 15. Java 关键代码（>=5段）
### 15.1 核心算法：series key 计算
```java
public String seriesKey(String metric, Map<String, String> tags) {
    return metric + "|" + tags.entrySet().stream()
        .sorted(Map.Entry.comparingByKey())
        .map(e -> e.getKey() + "=" + e.getValue())
        .reduce((a, b) -> a + "," + b).orElse("");
}
```

### 15.2 幂等去重：pointId
```java
public boolean acceptPoint(Point p) {
    if (p.pointId() == null) return true;
    if (dedupRepo.exists(p.pointId())) return false;
    dedupRepo.save(p.pointId(), Duration.ofHours(24));
    return true;
}
```

### 15.3 重试退避/失败处理：flush chunk
```java
public void flushWithRetry(Chunk c) {
    long wait = 50;
    for (int i = 0; i < 4; i++) {
        try { chunkRepo.save(c); return; }
        catch (RuntimeException ex) {
            if (i == 3) { dlqRepo.save(c.seriesId(), ex.getMessage()); return; }
            sleep(wait); wait = Math.min(wait * 2, 400);
        }
    }
}
```

### 15.4 一致性边界：WAL + memory buffer
```java
@Transactional
public void appendWalAndBuffer(Point p) {
    walRepo.append(p);
    memBuffer.add(p);
}
```

### 15.5 观测触发/回滚判定
```java
public void guardCardinality() {
    double growth = metrics.gauge("cardinality_growth_rate").value();
    if (growth > 1.5) {
        featureSwitch.enable("drop_unapproved_tags");
        alerting.fire("CARDINALITY_SPIKE", "growth=" + growth);
    }
}
```

## 16. 前端功能代码（React JS，仅 API 协作）
### 16.1 指标查询（loading/error/done）
```javascript
import { useState } from "react";

export function useTsdbQuery() {
  const [state, setState] = useState({ phase: "idle", series: [], error: "" });
  async function query(payload) {
    setState({ phase: "loading", series: [], error: "" });
    try {
      const res = await fetch("/api/tsdb/v1/query", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify(payload)
      });
      if (!res.ok) throw new Error(`HTTP_${res.status}`);
      const data = await res.json();
      setState({ phase: "done", series: data.series || [], error: "" });
    } catch (e) {
      setState({ phase: "error", series: [], error: String(e.message || e) });
    }
  }
  return { state, query };
}
```

### 16.2 聚合查询重试（高峰退避）
```javascript
export async function queryAgg(payload) {
  let delay = 200;
  for (let i = 0; i < 3; i++) {
    try {
      const res = await fetch("/api/tsdb/v1/query/aggregate", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify(payload)
      });
      if (!res.ok) throw new Error(`HTTP_${res.status}`);
      return { ok: true, data: await res.json() };
    } catch (err) {
      if (i === 2) return { ok: false, error: String(err.message || err) };
      await new Promise((r) => setTimeout(r, delay));
      delay = Math.min(delay * 2, 1200);
    }
  }
}
```

## 17. 测试策略
1. 单元测试：series key、压缩编码、聚合函数。
2. 集成测试：写入->flush->索引->查询。
3. 压测：500 万点/s 写入。
4. 故障测试：WAL 回放、索引重建、分片失效。
5. 回归测试：下采样口径一致性。

## 18. 丰富例子（>=10）
1. 服务器 CPU 每 10s 采样写入。
2. 近 1 小时曲线查询。
3. 按 region 分组求平均。
4. 5 分钟下采样生成趋势图。
5. 热点 metric 用 rollup 加速。
6. 高基数标签被限写。
7. 节点重启 WAL 重放恢复。
8. 冷数据归档仍可离线查询。
9. 告警查询走热存保障时延。
10. 保留策略到期自动清理。
11. 查询超时触发降维建议。
12. 变更 retention 有审计追踪。

## 19. 面试追问 + 可复述回答
1. TSDB 和普通 DB 最大差别？
回答：写多读聚合、时间分区、压缩和保留策略优先。
2. 为什么要下采样？
回答：降低存储和查询成本，保留趋势信息。
3. 高基数标签怎么治理？
回答：白名单、限流、拆分指标和预聚合。
4. 如何避免写入丢失？
回答：WAL 先行，flush 后确认。
5. 与母题 Q15 差异？
回答：Q15 偏分库分片通用方法，Q57 偏时序存储专用优化。

## 20. 新手学习路线
1. 先做时间分区写入。
2. 加标签索引与查询聚合。
3. 做压缩与下采样。
4. 增加 retention 和冷热迁移。
5. 做高峰与故障演练。

## 21. 上场前 Checklist
1. 能讲清写入路径和 WAL。
2. 能讲明标签索引与基数问题。
3. 能说下采样与冷热分层策略。
4. 能给出查询阈值和处置动作。
5. 能区分 Q57 与 Q15。

## 22. 与母题差异（对应母题/共性/差异/新增知识/话术）
### 22.1 对应母题
Q15 Database Sharding & Consistent Hashing。

### 22.2 共性能力
1. 都涉及分片和扩容。
2. 都涉及路由与数据分布。
3. 都需要一致性与容错。

### 22.3 关键差异
1. Q15 是通用分片；Q57 是时序专用存储。
2. Q57 强调时间分区和写优化。
3. Q57 强调下采样与保留策略。
4. Q57 更依赖标签索引和聚合查询。
5. Q57 更强调冷热分层和压缩。

### 22.4 本题新增知识点（>=5）
1. 时间分区 chunk 存储。
2. 标签高基数治理。
3. 下采样 rollup 机制。
4. WAL+flush 一致性边界。
5. 热温冷分层保留策略。
6. 时序聚合查询加速。

### 22.5 面试差异话术
“Q15 讲分片框架，Q57 讲时序场景特化：写入优化、压缩下采样、标签索引和冷热治理。”

---

## 单题自审（Q57）
### A. 完整性检查
1. 22 节完整：通过。
2. Java 代码段 5 段：通过。
3. React JS API 代码 2 段：通过。

### B. 易懂性检查
1. 术语白话 >=10：通过。
2. 正常/高峰/故障流程完整：通过。

### C. 专属性检查
1. 聚焦 TSDB：通过。
2. API、压缩、下采样、冷热策略清晰：通过。

### D. 工程落地检查
1. 阈值+动作绑定：通过。
2. RTO/RPO 与恢复路径明确：通过。

### E. 代码相关性检查
1. Java 五类点位覆盖：通过。
2. 前端 API 调用与高峰重试体现：通过。

### F. 母题差异检查
1. 与 Q15 差异具体：通过。

### 自评分（100）
1. 完整性：20/20
2. 易懂性：19/20
3. 面试可讲性：19/20
4. 技术深度：19/20
5. 工程落地性：19/20
总分：96/100（通过）
